{"cells": [{"cell_type": "markdown", "id": "6d7f5a2c", "metadata": {"tags": ["learner", "md"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 14: An Example With LHC Data</h1>\n"]}, {"cell_type": "markdown", "id": "ff95d3bc", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_14_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L14.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "91dd7386", "metadata": {"tags": ["learner", "md"]}, "source": ["<table style=\"width:100%\">\n", "    <colgroup>\n", "       <col span=\"1\" style=\"width: 40%;\">\n", "       <col span=\"1\" style=\"width: 15%;\">\n", "       <col span=\"1\" style=\"width: 45%;\">\n", "    </colgroup>\n", "    <tr>\n", "        <th style=\"text-align: left; font-size: 13pt;\">Section</th>\n", "        <th style=\"text-align: left; font-size: 13pt;\">Exercises</th>\n", "        <th style=\"text-align: left; font-size: 13pt;\">Summary</th>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_14_1\">L14.1 Large Hadron Collider Data</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_14_1\">L14.1 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_14_2\">L14.2 Working with LHC Data</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_14_2\">L14.2 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_14_3\">L14.3 Training and Testing the Network</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_14_3\">L14.3 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_14_4\">L14.4 Add a Hidden Layer</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_14_4\">L14.4 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "</table>\n", "\n"]}, {"cell_type": "markdown", "id": "d0a32547", "metadata": {"tags": ["learner", "catsoop_00", "md"]}, "source": ["<h3>Learning Objectives</h3>\n", "\n", "Text needed\n"]}, {"cell_type": "markdown", "id": "a651180b", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Importing Libraries</h3>\n", "\n", "Before beginning, run the cell below to import the relevant libraries for this notebook. \n", "Optionally, set the plot resolution and default figure size.\n"]}, {"cell_type": "code", "execution_count": 7, "id": "9dab2f77", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "import numpy as np\n", "\n", "#set plot resolution\n", "#%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure size\n", "#plt.rcParams['figure.figsize'] = (9,6)\n"]}, {"cell_type": "markdown", "id": "05535af7", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_14_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L14.1 Large Hadron Collider Data</h2>  \n", "\n", "| [Top](#section_14_0) | [Previous Section](#section_14_0) | [Exercises](#exercises_14_1) | [Next Section](#section_14_2) |\n"]}, {"cell_type": "markdown", "id": "d2c69377", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section."]}, {"cell_type": "code", "execution_count": 1, "id": "73dfe0cc", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"data": {"text/html": ["\n", "        <iframe\n", "            width=\"970\"\n", "            height=\"550\"\n", "            src=\"images/slides_L14_01.html\"\n", "            frameborder=\"0\"\n", "            allowfullscreen\n", "            \n", "        ></iframe>\n", "        "], "text/plain": ["<IPython.lib.display.IFrame at 0x10f6ef190>"]}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": ["#>>>RUN\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L14/slides_L14_01.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "71d5e72a", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_14_1'></a>     \n", "\n", "| [Top](#section_14_0) | [Restart Section](#section_14_1) | [Next Section](#section_14_2) |\n"]}, {"cell_type": "markdown", "id": "2c6d1ed0", "metadata": {"tags": ["learner", "md", "catsoop_01"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 14.1.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 13, "id": "f07b1ac7", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "98f9d057", "metadata": {"tags": ["learner", "catsoop_01", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "26607e2f", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_14_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L14.2 Working with LHC Data</h2>  \n", "\n", "| [Top](#section_14_0) | [Previous Section](#section_14_1) | [Exercises](#exercises_14_2) | [Next Section](#section_14_3) |\n"]}, {"cell_type": "markdown", "id": "a0b910c7", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "How do we train? How do we handle more complicated data?"]}, {"cell_type": "code", "execution_count": 20, "id": "466f917d", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "import h5py\n", "import pandas as pd\n", "\n", "treename = 'l1pf_egm_reg'\n", "\n", "VARS = ['pt', 'eta', 'phi', 'energy',\n", "  'e2x2', 'e2x5', 'e3x5', 'e5x5', 'e2x2_div_e2x5', 'e2x2_div_e5x5', 'e2x5_div_e5x5',#7\n", "  'hoE', 'bremStrength', 'ecalIso', 'crystalCount',#4\n", "  'lowerSideLobePt','upperSideLobePt',#2\n", "  'phiStripContiguous0', 'phiStripOneHole0', 'phiStripContiguous3p', 'phiStripOneHole3p',#4\n", "  'sihih','sipip','sigetaeta','sigphiphi','sigetaphi',#5\n", "  'e_m2_m2','e_m2_m1','e_m2_p0','e_m2_p1','e_m2_p2',\n", "  'e_m1_m2','e_m1_m1','e_m1_p0','e_m1_p1','e_m1_p2',\n", "  'e_p0_m2','e_p0_m1','e_p0_p0','e_p0_p1','e_p0_p2',\n", "  'e_p1_m2','e_p1_m1','e_p1_p0','e_p1_p1','e_p1_p2',\n", "  'e_p2_m2','e_p2_m1','e_p2_p0','e_p2_p1','e_p2_p2',#^25\n", "  'h_m1_m1','h_m1_p0','h_m1_p1',\n", "  'h_p0_m1','h_p0_p0','h_p0_p1',\n", "  'h_p1_m1','h_p1_p0','h_p1_p1',#^9\n", "  'gen_match']\n", "\n", "filename = 'xtalTuple_TTbar_PU0.z'\n", "\n", "h5file = h5py.File(filename, 'r') # open read-only\n", "params = h5file[treename][()]\n", "\n", "df = pd.DataFrame(params,columns=VARS)\n", "\n", "TODROP = [\n", "  'e2x2_div_e2x5', 'e2x2_div_e5x5', 'e2x5_div_e5x5',#7\n", "  'e_m2_m2','e_m2_m1','e_m2_p0','e_m2_p1','e_m2_p2',\n", "  'e_m1_m2','e_m1_m1','e_m1_p0','e_m1_p1','e_m1_p2',\n", "  'e_p0_m2','e_p0_m1','e_p0_p0','e_p0_p1','e_p0_p2',\n", "  'e_p1_m2','e_p1_m1','e_p1_p0','e_p1_p1','e_p1_p2',\n", "  'e_p2_m2','e_p2_m1','e_p2_p0','e_p2_p1','e_p2_p2',#^25\n", "  'h_m1_m1','h_m1_p0','h_m1_p1',\n", "  'h_p0_m1','h_p0_p0','h_p0_p1',\n", "  'h_p1_m1','h_p1_p0','h_p1_p1',#^9\n", "]\n", "\n", "df = df.drop(TODROP, axis=1) #remove custom variables\n", "\n", "for ie in ['e2x2', 'e2x5', 'e3x5', 'e5x5']:\n", "    df[ie] /= df['energy']\n", "\n", "\n", "df['isPU'] = pd.Series(df['gen_match']==0, index=df.index, dtype='i4')\n", "df['isEG'] = pd.Series(df['gen_match']==1, index=df.index, dtype='i4')\n", "\n", "MINPT = 0.5\n", "MAXPT = 100.\n", "df = df.loc[(df['pt']>MINPT) & (df['pt']<MAXPT) & (abs(df['eta'])<1.3)]\n", "df.fillna(0., inplace=True)\n", "\n", "df0 = df[df['gen_match']==0].head(100000)\n", "df1 = df[df['gen_match']==1].head(10000)\n", "\n", "df = pd.concat([df0, df1], ignore_index=True)\n", "df = df.sample(frac=1).reset_index(drop=True)\n", "\n", "print(df)\n", "print(sum(df['gen_match']==0))\n", "print(sum(df['gen_match']==1))\n"]}, {"cell_type": "code", "execution_count": 20, "id": "233f2c5d", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["col_names = list(df.columns)\n", "print(col_names)\n", "\n", "fig, axs = plt.subplots(len(col_names),1,figsize=(4,4*len(col_names)))\n", "for ix,ax in enumerate(axs):\n", "    ax.hist(df[col_names[ix]][df['gen_match']==0],bins=np.linspace(np.min(df[col_names[ix]]),np.max(df[col_names[ix]]),20),histtype='step',color='r',density=True)\n", "    ax.hist(df[col_names[ix]][df['gen_match']==1],bins=np.linspace(np.min(df[col_names[ix]]),np.max(df[col_names[ix]]),20),histtype='step',color='b',density=True)\n", "    ax.set_xlabel(col_names[ix])\n", "\n", "plt.show()"]}, {"cell_type": "code", "execution_count": 20, "id": "257e484c", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["fig, axs = plt.subplots(len(col_names)-4,len(col_names)-4,figsize=(8*len(col_names),8*len(col_names)))\n", "for ix in range(len(col_names)-3):\n", "    for iy in range(ix):\n", "        axs[ix-1,iy].plot(df[col_names[ix]][df['gen_match']==0],df[col_names[iy]][df['gen_match']==0],'r+')\n", "        axs[ix-1,iy].plot(df[col_names[ix]][df['gen_match']==1],df[col_names[iy]][df['gen_match']==1],'b.')\n", "        axs[ix-1,iy].set_xlabel(col_names[ix])\n", "        axs[ix-1,iy].set_ylabel(col_names[iy])\n", "\n", "plt.show()"]}, {"cell_type": "markdown", "id": "22c8d077", "metadata": {"tags": ["learner", "md"]}, "source": ["Lets make a simple logistic regression network on this data to differentiate between pileup data (PU) and electron and photons (EG).\n", "\n", "An important component of training a neural network is preparing the input. It is typical to split the data you have into different sets. Three are common: \"training\", \"testing\", and \"validation\".\n", "Here we use 30% of the data for testing and 70% for training and validation, with that data split 80%/20% for training/validation.\n", "PyTorch uses `dataloaders` which help handle batching, etc."]}, {"cell_type": "code", "execution_count": 20, "id": "0fb25db6", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["import torch \n", "\n", "dataset = df.values\n", "\n", "X = dataset[:,4:-3]\n", "#last 3 columns are labels\n", "ninputs = len(list(df.columns))-3-4\n", "\n", "Y = dataset[:,-1:]\n", "#last column will be used for the label\n", "\n", "test_frac = 0.3\n", "val_frac = 0.2\n", "\n", "torch.random.manual_seed(42) # fix a random seed for reproducibility\n", "alldataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(Y, dtype=torch.float32))\n", "\n", "testdataset, trainvaldataset = torch.utils.data.random_split(\n", "    alldataset, [int(len(Y)*test_frac),\n", "              int(len(Y)*(1-test_frac))])\n", "\n", "traindataset, valdataset = torch.utils.data.random_split(\n", "    trainvaldataset, [int(len(Y)*(1.-test_frac)*(1.-val_frac)),\n", "              int(len(Y)*(1.-test_frac)*val_frac)])\n", "\n", "testloader = torch.utils.data.DataLoader(testdataset,\n", "                                          num_workers=6,\n", "                                          batch_size=500,\n", "                                          shuffle=False)\n", "trainloader = torch.utils.data.DataLoader(traindataset,\n", "                                          num_workers=6,\n", "                                          batch_size=500,\n", "                                          shuffle=True)\n", "valloader = torch.utils.data.DataLoader(valdataset,\n", "                                        num_workers=6,\n", "                                        batch_size=500,\n", "                                        shuffle=False)\n"]}, {"cell_type": "markdown", "id": "5af8514f", "metadata": {"tags": ["learner", "md"]}, "source": ["Now we define our network architecture and the connections. Lets start with the kind of logistic regression network we saw already.\n", "\n", "PyTorch requires that we first define the layers we want to use in `__init__()` (here we build using standard library layers), and then we define the connection in `forward()`. This setup will allow PyTorch to construct the backward pass automatically, although for more complex or specialized networks it is possible to define the backward pass manually."]}, {"cell_type": "code", "execution_count": 20, "id": "54a15188", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["class LR_net(torch.nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.fc1 = torch.nn.Linear(ninputs,1)\n", "        self.output = torch.nn.Sigmoid()\n", "\n", "    def forward(self, x):\n", "        x = self.fc1(x)\n", "        x = self.output(x)\n", "        return x\n", "    \n", "\n", "model_lr = LR_net()\n", "print(model_lr)\n", "print('----------')\n", "print(model_lr.state_dict())"]}, {"cell_type": "markdown", "id": "ac90e900", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_14_2'></a>     \n", "\n", "| [Top](#section_14_0) | [Restart Section](#section_14_2) | [Next Section](#section_14_3) |\n"]}, {"cell_type": "markdown", "id": "69f30ae9", "metadata": {"tags": ["learner", "md", "catsoop_02"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 14.2.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 23, "id": "acc683a6", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "0bf363af", "metadata": {"tags": ["learner", "catsoop_02", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "1d1630a3", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_14_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L14.3 Training and Testing the Network</h2>  \n", "\n", "| [Top](#section_14_0) | [Previous Section](#section_14_2) | [Exercises](#exercises_14_3) | [Next Section](#section_14_4) |\n"]}, {"cell_type": "markdown", "id": "89852a6f", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "Now lets train! We do this using the `Adam` optimizer and binary cross entropy loss (as before)."]}, {"cell_type": "code", "execution_count": 30, "id": "eaa7ff3f", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "criterion = torch.nn.BCELoss()\n", "optimizer_lr = torch.optim.Adam(model_lr.parameters(), lr=0.003) \n", "\n", "history_lr = {'loss':[], 'val_loss':[]}\n", "\n", "for epoch in range(20):\n", "\n", "    current_loss = 0.0 #rezero loss\n", "    \n", "    for i, data in enumerate(trainloader):\n", "\n", "        inputs, labels = data\n", "        \n", "        # zero the parameter gradients\n", "        optimizer_lr.zero_grad()\n", "\n", "        # forward + backward + optimize (training magic)\n", "        # This will use the pytorch autograd feature to adjust the\n", "        ## parameters of our function to minimize the loss\n", "        outputs = model_lr(inputs)\n", "        loss = criterion(outputs, labels)\n", "        loss.backward()\n", "        optimizer_lr.step()\n", "        \n", "        # add loss statistics\n", "        current_loss += loss.item()\n", "        \n", "        if i == len(trainloader)-1:\n", "            current_val_loss = 0.0\n", "            with torch.no_grad():#disable updating gradient\n", "                for iv, vdata in enumerate(valloader):\n", "                    val_inputs, val_labels = vdata\n", "                    val_loss = criterion(model_lr(val_inputs), val_labels)\n", "                    current_val_loss += val_loss.item()\n", "            print('[%d, %4d] loss: %.4f  val loss: %.4f' % \n", "                  (epoch + 1, i + 1, current_loss/float(i+1) , current_val_loss/float(len(valloader))))\n", "\n", "            history_lr['loss'].append(current_loss/float(i+1))\n", "            history_lr['val_loss'].append(current_val_loss/float(len(valloader)))\n", "            \n", "print('Finished Training')\n", "torch.save(model_lr.state_dict(), 'lr_model.pt')\n", "print(model_lr.state_dict())"]}, {"cell_type": "markdown", "id": "389636d9", "metadata": {"tags": ["learner", "md"]}, "source": ["Ok, how is the training doing? Lets visualize the evolution of the loss by epoch."]}, {"cell_type": "code", "execution_count": 30, "id": "f2504550", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["plt.semilogy(history_lr['loss'], label='loss')\n", "plt.semilogy(history_lr['val_loss'], label='val_loss')\n", "plt.legend(loc=\"upper right\")\n", "plt.xlabel('epoch')\n", "plt.ylabel('loss (binary crossentropy)')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "f6256331", "metadata": {"tags": ["learner", "md"]}, "source": ["You may have heard of overtraining. Lets define a \"stopping criteria\" by using the validation loss. We will stop the training if the validation loss appears to have hit its minimum (but we will give it a few epochs to allow for local minimum or single-epoch spikes. There are other ways to define an early stopping criteria but this will do for now."]}, {"cell_type": "code", "execution_count": 30, "id": "94538aef", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["def train(model,trainloader,valloader,nepochs=100,lr=0.003,l2reg=0.,patience=5,name=None):\n", "\n", "    criterion = torch.nn.BCELoss()\n", "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2reg) \n", "\n", "    history = {'loss':[], 'val_loss':[]}\n", "\n", "    min_loss = 999999.\n", "    min_epoch = 0\n", "    min_model = model.state_dict()\n", "    should_stop = False\n", "    \n", "    for epoch in range(nepochs):\n", "\n", "        current_loss = 0.0 #rezero loss\n", "\n", "        for i, data in enumerate(trainloader):\n", "\n", "            inputs, labels = data\n", "\n", "            # zero the parameter gradients\n", "            optimizer.zero_grad()\n", "\n", "            # forward + backward + optimize\n", "            # This will use the pytorch autograd feature to adjust the\n", "            ## parameters of our function to minimize the loss\n", "            outputs = model(inputs)\n", "            loss = criterion(outputs, labels)\n", "            loss.backward()\n", "            optimizer.step()\n", "\n", "            # print statistics\n", "            current_loss += loss.item()\n", "\n", "            if i == len(trainloader)-1:\n", "                current_val_loss = 0.0\n", "                with torch.no_grad():#disable updating gradient\n", "                    model.eval() #place model in evaluation state\n", "                                ## necessary for some layer types (like dropout)\n", "                    for iv, vdata in enumerate(valloader):\n", "                        val_inputs, val_labels = vdata\n", "                        val_loss = criterion(model(val_inputs), val_labels)\n", "                        current_val_loss += val_loss.item()\n", "                    model.train() #return to training state\n", "                current_loss = current_loss/float(i+1)\n", "                current_val_loss = current_val_loss/float(len(valloader))\n", "                print('[%d, %4d] loss: %.4f  val loss: %.4f' % \n", "                      (epoch + 1, i + 1, current_loss , current_val_loss))\n", "\n", "                if current_val_loss < min_loss:\n", "                    min_loss = current_val_loss\n", "                    min_model = model.state_dict()\n", "                    min_epoch = epoch\n", "                elif epoch-min_epoch==5:\n", "                    model.load_state_dict(min_model)\n", "                    should_stop = True\n", "                    break\n", "\n", "                history['loss'].append(current_loss)\n", "                history['val_loss'].append(current_val_loss)\n", "                \n", "            if should_stop:\n", "                break\n", "\n", "    print('Finished Training')\n", "    if name is not None:\n", "        torch.save(model.state_dict(), '%s.pt'%name)\n", "    return history"]}, {"cell_type": "code", "execution_count": 30, "id": "bd675e99", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["history_lr = train(model_lr,trainloader,valloader,name='lr_model')"]}, {"cell_type": "code", "execution_count": 30, "id": "701bc6af", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["plt.semilogy(history_lr['loss'], label='loss')\n", "plt.semilogy(history_lr['val_loss'], label='val_loss')\n", "plt.legend(loc=\"upper right\")\n", "plt.xlabel('epoch')\n", "plt.ylabel('loss (binary crossentropy)')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "a6c1bf3f", "metadata": {"tags": ["learner", "md"]}, "source": ["Now lets see how to apply this network to our test data. This is essentially the same setup we used for the validation data."]}, {"cell_type": "code", "execution_count": 30, "id": "e588d00b", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["def apply(model, testloader):\n", "    with torch.no_grad():\n", "        model.eval()\n", "        outputs = []\n", "        labels = []\n", "        for data in testloader:\n", "            test_inputs, test_labels = data\n", "            outputs.append(model(test_inputs).numpy())\n", "            labels.append(test_labels.numpy())\n", "        model.train()\n", "\n", "        Y_test_predict = outputs\n", "        Y_test = labels\n", "\n", "    Y_test_predict = np.concatenate(Y_test_predict)\n", "    Y_test = np.concatenate(Y_test)\n", "    \n", "    return Y_test_predict,Y_test\n", "\n", "Y_test_predict_lr, Y_test = apply(model_lr, testloader)\n", "\n", "print(Y_test_predict_lr.shape)\n", "print(Y_test.shape)"]}, {"cell_type": "markdown", "id": "cc1767c7", "metadata": {"tags": ["learner", "md"]}, "source": ["And now lets plot the distribution of the output of the network. As we wanted, the PU (red) is peaked at 0, while the EG (blue) is closer to 1. "]}, {"cell_type": "code", "execution_count": 30, "id": "de486bc7", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["plt.hist(Y_test_predict_lr[Y_test==0],histtype='step',color='r',density=True)\n", "plt.hist(Y_test_predict_lr[Y_test==1],histtype='step',color='b',density=True)\n", "plt.xlabel('Logistic Regression Discriminant')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "a16ea19d", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_14_3'></a>     \n", "\n", "| [Top](#section_14_0) | [Restart Section](#section_14_3) | [Next Section](#section_14_4) |\n"]}, {"cell_type": "markdown", "id": "87e047a0", "metadata": {"tags": ["learner", "md", "catsoop_03"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 14.3.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 33, "id": "c59b2ab7", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "53b695f9", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "b32c3d56", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_14_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L14.4 Add a Hidden Layer</h2>     \n", "\n", "| [Top](#section_14_0) | [Previous Section](#section_14_3) | [Exercises](#exercises_14_4) |\n"]}, {"cell_type": "markdown", "id": "59cccf2d", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "Ok, but a logistic regression is the simplest thing. What about hidden layers?"]}, {"cell_type": "code", "execution_count": 40, "id": "252a28eb", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "class MLP2_net(torch.nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.fc1 = torch.nn.Linear(ninputs,30)\n", "        self.act1 = torch.nn.ReLU()\n", "        self.fc2 = torch.nn.Linear(30,10)\n", "        self.act2 = torch.nn.ReLU()\n", "        self.fc3 = torch.nn.Linear(10,1)\n", "        self.output = torch.nn.Sigmoid()\n", "\n", "    def forward(self, x):\n", "        x = self.fc1(x)\n", "        x = self.act1(x)\n", "        x = self.fc2(x)\n", "        x = self.act2(x)\n", "        x = self.fc3(x)\n", "        x = self.output(x)\n", "        return x\n", "    \n", "\n", "model_mlp_2layer = MLP2_net()\n", "print(model_mlp_2layer)"]}, {"cell_type": "code", "execution_count": 40, "id": "e526d8b1", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["history_mlp_2layer = train(model_mlp_2layer,trainloader,valloader,name='mlp_2layer_model')\n", "Y_test_predict_mlp_2layer, Y_test = apply(model_mlp_2layer, testloader)"]}, {"cell_type": "code", "execution_count": 40, "id": "aba3384f", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["plt.semilogy(history_mlp_2layer['loss'], label='loss')\n", "plt.semilogy(history_mlp_2layer['val_loss'], label='val_loss')\n", "plt.legend(loc=\"upper right\")\n", "plt.xlabel('epoch')\n", "plt.ylabel('loss (binary crossentropy)')\n", "plt.show()\n", "\n", "plt.hist(Y_test_predict_mlp_2layer[Y_test==0],histtype='step',color='r',density=True)\n", "plt.hist(Y_test_predict_mlp_2layer[Y_test==1],histtype='step',color='b',density=True)\n", "plt.xlabel('MLP (2 hidden layers) Discriminant')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "11877d39", "metadata": {"tags": ["learner", "md"]}, "source": ["This looks better than the logistic regression. But how would we establish that? A Receiver Operating Characteristic (ROC) curve is a typical way to compare multiple algorithms. Basically we are going to make a requirement on the NN output: if it is below a given value we will call it PU, and if it is above that value its EG. Then we compute how good this is at predicting the true labels. We can scan this cutoff value between 0 and 1 and then plot each point."]}, {"cell_type": "code", "execution_count": 40, "id": "e9460713", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["def compute_ROC(labels, predicts, npts=101):\n", "    cutvals = np.linspace(0.,1.,num=npts)\n", "    tot0 = float(len(labels[labels==0]))\n", "    tot1 = float(len(labels[labels==1]))\n", "    tpr = []\n", "    fpr = []\n", "    for c in cutvals:\n", "        fpr.append(float(len(predicts[(labels==0) & (predicts>c)]))/tot0)\n", "        tpr.append(float(len(predicts[(labels==1) & (predicts>c)]))/tot1)\n", "    \n", "    return np.array(fpr),np.array(tpr)\n", "\n", "mlp_2layer_rocpts = compute_ROC(Y_test,Y_test_predict_mlp_2layer)\n", "lr_rocpts = compute_ROC(Y_test,Y_test_predict_lr)\n", "\n", "plt.plot(mlp_2layer_rocpts[0],mlp_2layer_rocpts[1],'g-',label=\"MLP (2 hidden layers)\")\n", "plt.plot(lr_rocpts[0],lr_rocpts[1],'m--',label=\"Logistic Regression\")\n", "plt.title(\"ROC (Receiver Operating Characteristic) Curve\")\n", "plt.xlabel(\"False Positive Rate (FPR) aka Background Efficiency\")\n", "plt.ylabel(\"True Positive Rate (TPR) aka Signal Efficiency\")\n", "plt.legend(loc=\"lower right\")\n", "plt.show()"]}, {"cell_type": "markdown", "id": "26379b16", "metadata": {"tags": ["learner", "md"]}, "source": ["So the MLP we made is indeed a bit better, especially if we want to classify the background very well.\n", "\n", "Lets try a larger network. "]}, {"cell_type": "code", "execution_count": 40, "id": "b60a0696", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["class MLP3_net(torch.nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.fc1 = torch.nn.Linear(ninputs,50)\n", "        self.act1 = torch.nn.ReLU()\n", "        self.fc2 = torch.nn.Linear(50,30)\n", "        self.act2 = torch.nn.ReLU()\n", "        self.fc3 = torch.nn.Linear(30,10)\n", "        self.act3 = torch.nn.ReLU()\n", "        self.fc4 = torch.nn.Linear(10,1)\n", "        self.output = torch.nn.Sigmoid()\n", "\n", "    def forward(self, x):\n", "        x = self.fc1(x)\n", "        x = self.act1(x)\n", "        x = self.fc2(x)\n", "        x = self.act2(x)\n", "        x = self.fc3(x)\n", "        x = self.act3(x)\n", "        x = self.fc4(x)\n", "        x = self.output(x)\n", "        return x\n", "    \n", "\n", "model_mlp_3layer = MLP3_net()\n", "print(model_mlp_3layer)"]}, {"cell_type": "markdown", "id": "0d0abb78", "metadata": {"tags": ["learner", "md"]}, "source": ["Lets also try to use a form of regularization, in this case L2. If left unchecked, larger networks especially can begin to find and abuse certain subtle features that we perhaps don't want them to. The obvious case is if the feature is only present in the training set then we may be hurting ourselves by focusing on that. L2 regularization adds a \"penalty term\" to the loss function which is a function of the magnitude squared of the weight values. \n", "\n", "$\\mathcal{L} = \\mathcal{L}_\\textrm{BCE} + \\lambda\\sum |W|^2$\n", "\n", "We can control the relative importance of this term via the $\\lambda$ parameter. By encouraging the network to keep the weights small, it is less able to magnify the importance of one particular feature/node."]}, {"cell_type": "code", "execution_count": 40, "id": "b0ca1ba8", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["history_mlp_3layer = train(model_mlp_3layer,trainloader,valloader,l2reg=0.0001,name='mlp_3layer_model')\n", "Y_test_predict_mlp_3layer, Y_test = apply(model_mlp_3layer, testloader)"]}, {"cell_type": "code", "execution_count": 40, "id": "df6a39b3", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["mlp_2layer_rocpts = compute_ROC(Y_test,Y_test_predict_mlp_2layer)\n", "mlp_3layer_rocpts = compute_ROC(Y_test,Y_test_predict_mlp_3layer)\n", "lr_rocpts = compute_ROC(Y_test,Y_test_predict_lr)\n", "\n", "plt.plot(mlp_2layer_rocpts[0],mlp_2layer_rocpts[1],'g-',label=\"MLP (2 hidden layers)\")\n", "plt.plot(mlp_3layer_rocpts[0],mlp_3layer_rocpts[1],'--',color='orange',label=\"MLP (3 hidden layers)\")\n", "plt.plot(lr_rocpts[0],lr_rocpts[1],'m--',label=\"Logistic Regression\")\n", "plt.title(\"ROC (Receiver Operating Characteristic) Curve\")\n", "plt.xlabel(\"Bkg Eff\")\n", "plt.ylabel(\"Sig Eff\")\n", "plt.legend(loc=\"lower right\")\n", "plt.show()"]}, {"cell_type": "markdown", "id": "fe1a092c", "metadata": {"tags": ["learner", "md"]}, "source": ["Its hard to tell... Luckily we are not required to plot $\\epsilon_{s}$ and $\\epsilon_{b}$. Lets use $1/\\epsilon_{b}$ instead of $\\epsilon_{b}$ to really bring out the behavior for low $\\epsilon_{b}$."]}, {"cell_type": "code", "execution_count": 40, "id": "4556e191", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["mlp_2layer_rocpts = compute_ROC(Y_test,Y_test_predict_mlp_2layer,101)\n", "mlp_3layer_rocpts = compute_ROC(Y_test,Y_test_predict_mlp_3layer,101)\n", "lr_rocpts = compute_ROC(Y_test,Y_test_predict_lr,101)\n", "\n", "plt.plot(1./mlp_2layer_rocpts[0],mlp_2layer_rocpts[1],'g-',label=\"MLP (2 hidden layers)\")\n", "plt.plot(1./mlp_3layer_rocpts[0],mlp_3layer_rocpts[1],'--',color='orange',label=\"MLP (3 hidden layers)\")\n", "plt.plot(1./lr_rocpts[0],lr_rocpts[1],'m--',label=\"Logistic Regression\")\n", "plt.title(\"ROC (Receiver Operating Characteristic) Curve\")\n", "plt.xlabel(\"1/Bkg Eff\")\n", "plt.xlim([-1, 2500])\n", "plt.ylabel(\"Sig Eff\")\n", "plt.legend(loc=\"upper right\")\n", "plt.show()"]}, {"cell_type": "markdown", "id": "c6ae13f1", "metadata": {"tags": ["learner", "md"]}, "source": ["Lets try two other types of regularizer to finish: *batch normalization* and *dropout*.\n", "\n", "Batch normalization works by rescaling each input such that the mean and standard deviation are 0 and 1, respectively. This helps make sure that each node has similar values when it is passed to the following layer.\n", "\n", "Dropout works by randomly removing a given fraction of the nodes in a layer each training pass. This helps ensure that no one node becomes crucially important to the final result."]}, {"cell_type": "code", "execution_count": 40, "id": "668597db", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["class MLP3_BN_net(torch.nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.bn0 = torch.nn.BatchNorm1d(ninputs)\n", "        self.fc1 = torch.nn.Linear(ninputs,50)\n", "        self.act1 = torch.nn.ReLU()\n", "        self.bn1 = torch.nn.BatchNorm1d(50)\n", "        self.fc2 = torch.nn.Linear(50,30)\n", "        self.act2 = torch.nn.ReLU()\n", "        self.bn2 = torch.nn.BatchNorm1d(30)\n", "        self.fc3 = torch.nn.Linear(30,10)\n", "        self.act3 = torch.nn.ReLU()\n", "        self.bn3 = torch.nn.BatchNorm1d(10)\n", "        self.fc4 = torch.nn.Linear(10,1)\n", "        self.output = torch.nn.Sigmoid()\n", "\n", "    def forward(self, x):\n", "        x = self.bn0(x)\n", "        x = self.fc1(x)\n", "        x = self.act1(x)\n", "        x = self.bn1(x)\n", "        x = self.fc2(x)\n", "        x = self.act2(x)\n", "        x = self.bn2(x)\n", "        x = self.fc3(x)\n", "        x = self.act3(x)\n", "        x = self.bn3(x)\n", "        x = self.fc4(x)\n", "        x = self.output(x)\n", "        return x\n", "    \n", "class MLP3_Drop_net(torch.nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.fc1 = torch.nn.Linear(ninputs,50)\n", "        self.act1 = torch.nn.ReLU()\n", "        self.drop1 = torch.nn.Dropout(0.1)\n", "        self.fc2 = torch.nn.Linear(50,30)\n", "        self.act2 = torch.nn.ReLU()\n", "        self.drop2 = torch.nn.Dropout(0.1)\n", "        self.fc3 = torch.nn.Linear(30,10)\n", "        self.act3 = torch.nn.ReLU()\n", "        self.drop3 = torch.nn.Dropout(0.1)\n", "        self.fc4 = torch.nn.Linear(10,1)\n", "        self.output = torch.nn.Sigmoid()\n", "\n", "    def forward(self, x):\n", "        x = self.fc1(x)\n", "        x = self.act1(x)\n", "        x = self.drop1(x)\n", "        x = self.fc2(x)\n", "        x = self.act2(x)\n", "        x = self.drop2(x)\n", "        x = self.fc3(x)\n", "        x = self.act3(x)\n", "        x = self.drop3(x)\n", "        x = self.fc4(x)\n", "        x = self.output(x)\n", "        return x\n", "\n", "model_mlp_3layer_bn = MLP3_BN_net()\n", "print(model_mlp_3layer_bn)\n", "\n", "model_mlp_3layer_drop = MLP3_Drop_net()\n", "print(model_mlp_3layer_drop)"]}, {"cell_type": "code", "execution_count": 40, "id": "9a280529", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["history_mlp_3layer_bn = train(model_mlp_3layer_bn,trainloader,valloader,name='mlp_3layer_bn_model')\n", "Y_test_predict_mlp_3layer_bn, Y_test = apply(model_mlp_3layer_bn, testloader)"]}, {"cell_type": "code", "execution_count": 40, "id": "be525e1d", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["history_mlp_3layer_drop = train(model_mlp_3layer_drop,trainloader,valloader,name='mlp_3layer_drop_model')\n", "Y_test_predict_mlp_3layer_drop, Y_test = apply(model_mlp_3layer_drop, testloader)"]}, {"cell_type": "code", "execution_count": 40, "id": "9bfb384c", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["mlp_3layer_rocpts = compute_ROC(Y_test,Y_test_predict_mlp_3layer,101)\n", "mlp_3layer_bn_rocpts = compute_ROC(Y_test,Y_test_predict_mlp_3layer_bn,101)\n", "mlp_3layer_drop_rocpts = compute_ROC(Y_test,Y_test_predict_mlp_3layer_drop,101)\n", "\n", "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12,4))\n", "\n", "ax1.plot(mlp_3layer_rocpts[0],mlp_3layer_rocpts[1],'--',color='orange',label=\"MLP (3 hidden layers)\")\n", "ax1.plot(mlp_3layer_bn_rocpts[0],mlp_3layer_bn_rocpts[1],'--',color='brown',label=\"MLP (3 hidden layers w/ BN)\")\n", "ax1.plot(mlp_3layer_drop_rocpts[0],mlp_3layer_drop_rocpts[1],'--',color='cyan',label=\"MLP (3 hidden layers w/ Dropout)\")\n", "ax1.set_title(\"ROC (Receiver Operating Characteristic) Curve\")\n", "ax1.set_xlabel(\"Bkg Eff\")\n", "ax1.set_ylabel(\"Sig Eff\")\n", "ax1.legend(loc=\"lower right\")\n", "\n", "ax2.plot(1./mlp_3layer_rocpts[0],mlp_3layer_rocpts[1],'--',color='orange',label=\"MLP (3 hidden layers)\")\n", "ax2.plot(1./mlp_3layer_bn_rocpts[0],mlp_3layer_bn_rocpts[1],'--',color='brown',label=\"MLP (3 hidden layers w/ BN)\")\n", "ax2.plot(1./mlp_3layer_drop_rocpts[0],mlp_3layer_drop_rocpts[1],'--',color='cyan',label=\"MLP (3 hidden layers w/ Dropout)\")\n", "ax2.set_title(\"ROC (Receiver Operating Characteristic) Curve\")\n", "ax2.set_xlabel(\"1/Bkg Eff\")\n", "ax2.set_xlim([-1, 2500])\n", "ax2.set_ylabel(\"Sig Eff\")\n", "ax2.legend(loc=\"upper right\")\n", "\n", "plt.show()"]}, {"cell_type": "markdown", "id": "a767f533", "metadata": {"tags": ["learner", "md"]}, "source": ["You should now have all the tools you need to start developing, training, and testing your own own neural networks!"]}, {"cell_type": "markdown", "id": "96231368", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_14_4'></a>   \n", "\n", "| [Top](#section_14_0) | [Restart Section](#section_14_4) |\n"]}, {"cell_type": "markdown", "id": "36a59ef1", "metadata": {"tags": ["learner", "md", "catsoop_04"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 14.4.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 43, "id": "4d246dc8", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "ac37a8cc", "metadata": {"tags": ["learner", "catsoop_04", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "f57ce9ed", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Extra Slides</h3>\n", "\n", "Run the code below to view the slides for this section."]}, {"cell_type": "code", "execution_count": 1, "id": "31e1e2f9", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"data": {"text/html": ["\n", "        <iframe\n", "            width=\"970\"\n", "            height=\"550\"\n", "            src=\"https://mitx-8s50.github.io/slides/L14/slides_L14_04.html\"\n", "            frameborder=\"0\"\n", "            allowfullscreen\n", "            \n", "        ></iframe>\n", "        "], "text/plain": ["<IPython.lib.display.IFrame at 0x10f7f5130>"]}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": ["#>>>RUN\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L14/slides_L14_04.html', width=970, height=550)"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}