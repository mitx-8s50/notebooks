{"cells": [{"cell_type": "markdown", "id": "648c5691", "metadata": {"tags": ["learner", "md"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 10: Bayesian Statistics and Likelihood</h1>\n"]}, {"cell_type": "markdown", "id": "11639ba2", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_10_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L10.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "02859d6d", "metadata": {"tags": ["learner", "md"]}, "source": ["<table style=\"width:100%\">\n", "    <colgroup>\n", "       <col span=\"1\" style=\"width: 40%;\">\n", "       <col span=\"1\" style=\"width: 15%;\">\n", "       <col span=\"1\" style=\"width: 45%;\">\n", "    </colgroup>\n", "    <tr>\n", "        <th style=\"text-align: left; font-size: 13pt;\">Section</th>\n", "        <th style=\"text-align: left; font-size: 13pt;\">Exercises</th>\n", "        <th style=\"text-align: left; font-size: 13pt;\">Summary</th>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_10_1\">L10.1 Definition of Convolution</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_10_1\">L10.1 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_10_2\">L10.2 Example of Convolutions with Different Functions</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_10_2\">L10.2 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_10_3\">L10.3 Prior and Posterior Probabilities and Bayes Theorem</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_10_3\">L10.3 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_10_4\">L10.4 Bayesian vs. Frequentist and Likelihood</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_10_4\">L10.4 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_10_5\">L10.5 Questions About Likelihood</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_10_5\">L10.5 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_10_6\">L10.6 Likelihood Ratio</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_10_6\">L10.6 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "</table>\n", "\n"]}, {"cell_type": "markdown", "id": "7f1e40d3", "metadata": {"tags": ["learner", "catsoop_00", "md"]}, "source": ["<h3>Learning Objectives</h3>\n", "\n", "Text needed\n"]}, {"cell_type": "markdown", "id": "10e7f86a", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Importing Libraries</h3>\n", "\n", "Before beginning, run the cell below to import the relevant libraries for this notebook. \n", "Optionally, set the plot resolution and default figure size.\n"]}, {"cell_type": "code", "execution_count": 7, "id": "af1805bb", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "import numpy as np\n", "\n", "#set plot resolution\n", "#%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure size\n", "#plt.rcParams['figure.figsize'] = (9,6)\n"]}, {"cell_type": "markdown", "id": "0856101b", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_10_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L10.1 Definition of Convolution</h2>  \n", "\n", "| [Top](#section_10_0) | [Previous Section](#section_10_0) | [Exercises](#exercises_10_1) | [Next Section](#section_10_2) |\n"]}, {"cell_type": "markdown", "id": "aca067bd", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "Convlutions are a critical component of evey good statistical analysis. Its a way to multiply distributions together. \n", "Lets build convlutions up by scratch and then go from there. \n", "\n", "The core concept of a convolution is that you are effectively multiplying distributions. Given two functions $f(x)$ and $g(x)$, we can define convolutions by \n", "\\begin{eqnarray}\n", "(f*g)(z) &=& \\int^{\\infty}_{-\\infty} f(z-t)g(t)dt\n", "\\end{eqnarray}\n", "For data analysis, we usually think about this in the context of probability distribuitons $g$ and $f$. From here, we construct a new probability distribution $(f*g)$. Anyway, lets take a look at how it works. \n", "\n", "To do this, lets first define some functions to convolve. "]}, {"cell_type": "code", "execution_count": 10, "id": "a82359e2", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "#First lets define a triangular distribution\n", "def triangle(x,mean=5):\n", "    Norm=mean*mean\n", "    val=np.where(x <= mean,np.maximum(x,np.zeros(len(x))), np.maximum(2*mean-x,np.zeros(len(x))))\n", "    return val/Norm\n", "\n", "#Now define the gaussian\n", "def gaussian(x,mean=0,sigma=1):\n", "    return 1./(sigma * np.sqrt(2 * np.pi)) * np.exp( - (x - mean)**2 / (2 * sigma**2)) \n"]}, {"cell_type": "code", "execution_count": 10, "id": "a70e216d", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "#Now lets do a convolution by hand\n", "def convolve(f1,f2,x,sigma=1,iMin=-10,iMax=10,iN=2000):\n", "    step=(iMax-iMin)/iN\n", "    pInt=0\n", "    for i0 in range(iN):\n", "            pX   = np.repeat(i0*step+iMin,len(x))\n", "            pVal = f1(x-pX,sigma=sigma)*f2(pX)\n", "            pInt += pVal*step\n", "    return pInt"]}, {"cell_type": "code", "execution_count": 10, "id": "5e8e08b8", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from scipy import stats\n", "\n", "#now lets plot\n", "fig, ax = plt.subplots()\n", "x_in=np.linspace(-10, 15, 100)\n", "tri_out=triangle(x_in)\n", "gaus_out=gaussian(x_in)\n", "conv_out=convolve(gaussian,triangle,x_in)\n", "conv2_out=convolve(gaussian,triangle,x_in,sigma=2)\n", "\n", "ax.plot(x_in,tri_out,label='triangle')\n", "ax.plot(x_in,gaus_out,label='gaussian')\n", "ax.plot(x_in,conv_out,label='convolved')\n", "ax.plot(x_in,conv2_out,label='convolved($\\sigma=5$)')\n", "ax.set(xlabel='x(t)', ylabel='y(t)',title='Convolutions')\n", "ax.grid()\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "a766a4c3", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_10_1'></a>     \n", "\n", "| [Top](#section_10_0) | [Restart Section](#section_10_1) | [Next Section](#section_10_2) |\n"]}, {"cell_type": "markdown", "id": "52cd47a6", "metadata": {"tags": ["learner", "md", "catsoop_01"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 10.1.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 13, "id": "f2b5bf2c", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "84ce8b99", "metadata": {"tags": ["learner", "catsoop_01", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "a2fe3a84", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_10_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L10.2 Example of Convolutions with Different Functions</h2>  \n", "\n", "| [Top](#section_10_0) | [Previous Section](#section_10_1) | [Exercises](#exercises_10_2) | [Next Section](#section_10_3) |\n"]}, {"cell_type": "markdown", "id": "33c37f93", "metadata": {"tags": ["learner", "md"]}, "source": ["### Challenge question\n", "\n", "What does $f(x)=\\sin(x)$ convoled with a gaussian look like? How does it change by the resolution of the gaussian?"]}, {"cell_type": "code", "execution_count": 20, "id": "ed4c9fa2", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "def func(x):\n", "    return np.sin(x)*x\n", "\n", "#now lets plot\n", "fig, ax = plt.subplots()\n", "x_in=np.linspace(-10, 15, 100)\n", "func_out=func(x_in)\n", "gaus_out=gaussian(x_in)\n", "conv_out=convolve(gaussian,func,x_in)\n", "conv2_out=convolve(gaussian,func,x_in,sigma=2)\n", "\n", "ax.plot(x_in,func_out,label='sin(x)')\n", "ax.plot(x_in,gaus_out,label='gaussian')\n", "ax.plot(x_in,conv_out,label='convolved')\n", "ax.plot(x_in,conv2_out,label='convolved($\\sigma=2$)')\n", "ax.set(xlabel='x(t)', ylabel='y(t)',title='Convolutions')\n", "ax.grid()\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": 20, "id": "88ec3946", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "#answer\n", "def func(x):\n", "    return np.sin(x)*x\n", "\n", "#now lets plot\n", "fig, ax = plt.subplots()\n", "x_in=np.linspace(-10, 15, 100)\n", "sin_out=sin(x_in)\n", "gaus_out=gaussian(x_in)\n", "conv_out=convolve(gaussian,sin,x_in,sigma=2)\n", "\n", "\n", "ax.plot(x_in,sin_out,label='triangle')\n", "ax.plot(x_in,gaus_out,label='gaussian')\n", "ax.plot(x_in,conv_out,label='convolved')\n", "ax.set(xlabel='x(t)', ylabel='y(t)',title='Convolutions')\n", "ax.grid()\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "e2496576", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_10_2'></a>     \n", "\n", "| [Top](#section_10_0) | [Restart Section](#section_10_2) | [Next Section](#section_10_3) |\n"]}, {"cell_type": "markdown", "id": "e19fd655", "metadata": {"tags": ["learner", "md", "catsoop_02"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 10.2.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 23, "id": "1d171774", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "3ebe11a5", "metadata": {"tags": ["learner", "catsoop_02", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "81192f8b", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_10_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L10.3 Prior and Posterior Probabilities and Bayes Theorem</h2>  \n", "\n", "| [Top](#section_10_0) | [Previous Section](#section_10_2) | [Exercises](#exercises_10_3) | [Next Section](#section_10_4) |\n"]}, {"cell_type": "markdown", "id": "99b4a9e8", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section."]}, {"cell_type": "code", "execution_count": 1, "id": "a7f30871", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"data": {"text/html": ["\n", "        <iframe\n", "            width=\"970\"\n", "            height=\"550\"\n", "            src=\"https://mitx-8s50.github.io/slides/L10/slides_L10_03.html\"\n", "            frameborder=\"0\"\n", "            allowfullscreen\n", "            \n", "        ></iframe>\n", "        "], "text/plain": ["<IPython.lib.display.IFrame at 0x10d49af10>"]}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": ["#>>>RUN\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L10/slides_L10_03.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "cfa149d2", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>What is a measurment?</h3>\n", "\n", "Lets say we have a measurement of some parameter $x$, and this meausurement behaves like a gaussian about some point with width $\\sigma=1$. Lets write out this meausrement.  "]}, {"cell_type": "code", "execution_count": 20, "id": "685db9e5", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from scipy import stats\n", "\n", "#Our measurement probability\n", "def gaus(mu=0,sigma=1): \n", "    x = np.arange(-10, 10, 0.001)\n", "    y = stats.norm.pdf(x,mu,sigma)\n", "    return x,y\n", "\n", "#a quick plot of what we expect the measurement to be\n", "def plotgaus():\n", "    x,y=gaus(0,1)\n", "    fig, ax = plt.subplots(figsize=(9,6))\n", "    plt.style.use('fast')\n", "    ax.plot(x,y)\n", "    ax.fill_between(x,y,0, alpha=0.1, color='b')\n", "    ax.set_xlim([-6,6])\n", "    ax.set_xlabel('x')\n", "    ax.set_ylabel('p')\n", "    plt.show()\n", "\n", "plotgaus()"]}, {"cell_type": "markdown", "id": "6175f8d8", "metadata": {"tags": ["learner", "md"]}, "source": ["This preconceived distribution about how our measurement will behave is known as a prior. Now lets say we perform this measurement, but our observed measurement is not actually at the point we expect it to be. What if, for example our measurement is at $x=2$ what woud be the likelihood of this occuring or not occuring. "]}, {"cell_type": "code", "execution_count": 20, "id": "002c21ec", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "def gaus(mu=0,sigma=1,meas=2): \n", "    x = np.arange(-10, 10, 0.001)\n", "    xmeas = np.arange(meas, 10, 0.001)\n", "    y = stats.norm.pdf(x,mu,sigma)\n", "    ymeas = stats.norm.pdf(xmeas,mu,sigma)\n", "    return x,y,xmeas,ymeas\n", "\n", "def plotgaus():\n", "    x,y,xmeas,ymeas=gaus(0,1,2)\n", "    fig, ax = plt.subplots(figsize=(9,6))\n", "    plt.style.use('fast')\n", "    ax.plot(x,y)\n", "    ax.fill_between(x,y,0, alpha=0.1, color='b')\n", "    ax.fill_between(xmeas,ymeas,0, alpha=0.3, color='b')\n", "    ax.set_xlim([-6,6])\n", "    ax.set_xlabel('x')\n", "    ax.set_ylabel('p')\n", "    plt.show()\n", "plotgaus()"]}, {"cell_type": "markdown", "id": "a8f35efc", "metadata": {"tags": ["learner", "md"]}, "source": ["From the above, observed measurement. It is clear that this distribution has a large deviation from what we actually expected to observe. The key question here that we would like to understand: Is this a statistical fluctation, or is this something we did not expect?  \n", "\n", "Understanding if this is something we do not expect is the focus of this lecture. Lets now imagine that we perform this measurement a number of time, say 10 times, and it all comes up away from our expectation, or perhaps 1000 times, what do we thing?"]}, {"cell_type": "code", "execution_count": 20, "id": "e99b1e21", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "def gaus(mu=0,sigma=1): \n", "    x = np.arange(-10, 10, 0.001)\n", "    y = stats.norm.pdf(x,mu,sigma)\n", "    return x,y\n", "\n", "def plotGausSample(iZ,iSample):\n", "    plt.style.use('fast')\n", "    fig, ax = plt.subplots(figsize=(9,6))\n", "    #sample\n", "    samples = np.random.normal(iZ,1,iSample)\n", "    x,y=gaus(iZ,1)\n", "    #\n", "    xs,ys=gaus(0,1)\n", "    ax.plot(xs,ys,label='prior')\n", "    ax.plot(x,y,label='posterior')\n", "    count, bins, ignored = plt.hist(samples, 30, density=True)\n", "    ax.fill_between(xs,ys,0, alpha=0.1)\n", "    ax.set_xlim([-6,6])\n", "    ax.set_xlabel('x')\n", "    ax.set_ylabel('p')\n", "    ax.legend()\n", "    plt.show()\n", "\n", "plotGausSample(2,10)\n", "plotGausSample(2,1000)"]}, {"cell_type": "markdown", "id": "40ad7544", "metadata": {"tags": ["learner", "md"]}, "source": ["Now we have two results, we have a prior (aka a guess) of what we though the data would look like and a posterior for what we actually observe the data. These two observations yield two different ways to interpret the data. \n", "\n", "**Bayesian** It is possible to create a model of everything and within your model you can explain all random phenomena. As we take more data, we can fine tune our model to be ever more predictive. \n", "\n", "**Frequentist** The data guides our model. We can use what we observe in the data to explain how we will observe these phenomena in the future. "]}, {"cell_type": "markdown", "id": "1f233f68", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Bayes Thereom</h3>\n", "\n", "To understand Bayes theorem, lets define a bunch of terms, first we would like the define the probability of a hypothesis happening. We can write this probability as $P\\left(\\mathcal{H}\\right)$, this is the probability of the hypothesis happening given a prior for how we expect the distribution to behave. To put a concrete label to this, lets consider that you are on a game show similar to the \"Monty Hall\" game show of the 1960s. You have 3 doors and there is a car behind one of the doors. The car is placed randomly, what is your original $P\\left(\\mathcal{H}\\right)$?\n", "\n", "\\begin{eqnarray}\n", "P(\\mathcal{H}=\\rm{door~1}) & = & \\frac{1}{3} \\\\\n", "P(\\mathcal{H}=\\rm{door~2}) & = & \\frac{1}{3} \\\\\n", "P(\\mathcal{H}=\\rm{door~3}) & = & \\frac{1}{3} \n", "\\end{eqnarray}\n", "\n", "Now everything changes when you open one of the doors. In that instance your knowledge of what happens changes completely since you now have knowledge of what is behind which door. In the case of this, there are two results. The first result is that there is a car behind the door, great! In the second case, you know that the door that was opened is empty. So the question is what do you.  However, in the Monty Hall problem. Monty will mix things up by opening a door that does not have a car, and then asking you to stay with you original choice, or to switch. What do you do? \n", "\n", "Let's define $P\\left(\\mathcal{D}\\right)$ as the probability of an instance of the data happening, and furthemore defining\n", "\n", "\\begin{eqnarray}\n", " P\\left(\\mathcal{H} | \\mathcal{D} \\right) & = & \\rm{probability~of~a~hypothesis~given~data} \\\\\n", " P\\left(\\mathcal{D} | \\mathcal{H} \\right) & = & \\rm{probability~of~data~given~a~hypothesis} \\\\\n", "\\end{eqnarray}\n", "\n", "Let's say you choose door 1, and Monty opens door two. We can write down the probability that Monty will open door 2\n", "\\begin{eqnarray}\n", "P(\\rm{open~2}|\\mathcal{H}=\\rm{car~at~door~1}) & = & \\frac{1}{2} \\\\\n", "P(\\rm{open~2}|\\mathcal{H}=\\rm{car~at~door~2}) & = & 0 \\\\\n", "P(\\rm{open~2}|\\mathcal{H}=\\rm{car~at~door~3}) & = & 1 \n", "\\end{eqnarray}\n", "\n", "If you chose correctly, the first time, then Monty has a 50% chance of choosing door 2 or 3, and both doors will not contain a car. If you did not choose correctly the first time, then Monty will for sure open a specific door, and in this scenario you know for sure that behind the other door that Monty did not choose is the car. So given this perspective do you switch or stay? Lets write out the probabilities again, remember when you start you have a $\\frac{1}{3}$ chance of choosing correctly, and a $\\frac{2}{3}$ chance of choosing incorrectly. \n", "\n", "\\begin{eqnarray}\n", "P(\\mathcal{H}=\\rm{car~at~door~1}|\\mathcal{D}=\\rm{open~2}) & = & \\frac{1}{3} \\\\\n", "P(\\mathcal{H}=\\rm{car~at~door~2}|\\mathcal{D}=\\rm{open~2}) & = & 0 \\\\\n", "P(\\mathcal{H}=\\rm{car~at~door~3}|\\mathcal{D}=\\rm{open~2}) & = & \\frac{2}{3}  \n", "\\end{eqnarray}\n", "\n", "or more generically we can write out:\n", "\\begin{equation}\n", "P(\\mathcal{H}=\\rm{car~at~door~1}|\\mathcal{D}=\\rm{open~2}) = \\frac{P(\\mathcal{D}=\\rm{open~2}|\\mathcal{H}=\\rm{car~at~door~1})P(\\mathcal{H}=\\rm{door~1})}{\\rm{all~combinations}}\n", "\\end{equation}\n", "or in other words\n", "\n", "\\begin{equation}\n", "P(\\mathcal{H}=\\rm{car~at~door~1}|\\mathcal{D}=\\rm{open~2}) = \\frac{P(\\mathcal{D}=\\rm{open~2}|\\mathcal{H}=\\rm{car~at~door~1})P(\\mathcal{H}=\\rm{door~1})}{P\\left(\\mathcal{D}=\\rm{car~at~door~1}\\right)+P\\left(\\mathcal{D}=\\rm{car~at~door~2}\\right)+P\\left(\\mathcal{D}=\\rm{car~at~door~3}\\right)}\n", "\\end{equation}\n", "\n", "More generically, we can write this as what is known as Bayes theorem\n", "\\begin{equation}\n", " P\\left(\\mathcal{H} | \\mathcal{D} \\right) =  \\frac{P\\left(\\mathcal{D} | \\mathcal{H} \\right)P(\\mathcal{H})}{P(\\mathcal{D})} \n", "\\end{equation}\n", "\n", "We can also relabel these terms as: \n", "\\begin{eqnarray}\n", " P\\left(\\mathcal{H} | \\mathcal{D} \\right) & = & \\rm{Posterior} \\\\\n", " P\\left(\\mathcal{D} | \\mathcal{H} \\right) & = & \\rm{Likelihood} \\\\\n", " P\\left(\\mathcal{H} \\right) & = & \\rm{Prior} \\\\\n", " P\\left(\\mathcal{D} \\right) & = & \\rm{Normalizer(all~possibilities)} \n", "\\end{eqnarray} \n", "\n", "The Posterior is our observed result, the Prior is our initial guess, the likelihood is what actually we observe, and finally the bottom term, the normalizer, is to ensure that our probabilities integrate to $1$ (aka, we have covered, and only covered, all possibilities).  As a general rule of thumb, the way to remember this is\n", "\n", "\\begin{equation}\n", " \\rm{posterior} \\propto \\rm{likelihood} \\times \\rm{prior}\n", "\\end{equation}\n", "\n", "Given Bayes theorem, lets go back to our original measurement. In that case, we had two normal distributions, a posterior about two, and a prior about zero. How do we connect these two? \n", "\n", "\\begin{equation}\n", " P\\left(\\mathcal{H} | \\mathcal{D} \\right) =  \\frac{P\\left(\\mathcal{D} | \\mathcal{H} \\right)P(\\mathcal{H})}{P(\\mathcal{D})} \\\\\n", "\\mathcal{N}(x,\\mu=2,1) = \\frac{P\\left(\\mathcal{D} | \\mathcal{H} \\right)}{P(\\mathcal{D})}\\mathcal{N}(x,\\mu=0,1) \\\\\n", "P\\left(\\mathcal{D} | \\mathcal{H} \\right) = \\frac{\\mathcal{N}(x,\\mu=2,1)}{\\mathcal{N}(x,\\mu=0,1)} P(\\mathcal{D}) \n", "\\end{equation}\n", "\n", "Since $P(\\mathcal{D})$ is just a constant to ensure that our resulting probability is normalized, we can pretty easily compute the likelihood in our first example. Lets plot it. "]}, {"cell_type": "code", "execution_count": 20, "id": "b8ecc3a4", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "def plotGausSampleLike(iZ,iSample):\n", "    plt.style.use('fast')\n", "    fig, ax = plt.subplots(figsize=(9,6))\n", "    #Sample\n", "    samples = np.random.normal(iZ,1,iSample)\n", "    x,y=gaus(iZ,1)\n", "    #prior\n", "    xs,ys=gaus(0,1)\n", "    #likelihood\n", "    yratio=np.minimum(y/ys,1.)\n", "    #plot\n", "    ax.plot(xs,ys,label='prior')\n", "    ax.plot(x,y,label='posterior')\n", "    ax.plot(x,yratio,label='Likelihood/20')\n", "    count, bins, ignored = plt.hist(samples, 30, density=True)\n", "    ax.fill_between(xs,ys,0, alpha=0.1)\n", "    ax.set_xlim([-6,6])\n", "    ax.set_xlabel('x')\n", "    ax.set_ylabel('p')\n", "    ax.legend()\n", "    plt.show()\n", "\n", "plotGausSampleLike(2,100)"]}, {"cell_type": "markdown", "id": "d032666c", "metadata": {"tags": ["learner", "md"]}, "source": ["As you can see the likelihood shoots up, and in fact shoots up above 1, this means that our liklelihood is unphysical, and we are not capturing our physics, or more imporantly, this means that **our prior is wrong**. Whats the right prior in this scenario? \n", "\n", "Lets tweak our prior to have a large sigma $\\mathcal{N}(\\mu=0,\\sigma=2)$"]}, {"cell_type": "code", "execution_count": 20, "id": "24f97001", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "def plotGausSampleLike(iZ,iSample,iSigMax):\n", "    plt.style.use('fast')\n", "    fig, ax = plt.subplots(figsize=(9,6))\n", "    #ample our posterior\n", "    samples = np.random.normal(iZ,1,iSample)\n", "    x,y=gaus(iZ,1)\n", "    #Sample our prior\n", "    xs,ys=gaus(0,iSigMax) #######<<<<< This is our tweak\n", "    #now compute the likelihood\n", "    yratio=np.minimum(y/ys,20.)\n", "    #pot this stuff\n", "    ax.plot(xs,ys,label='prior')\n", "    ax.plot(x,y,label='posterior')\n", "    ax.plot(x,yratio,label='Likelihood/20')\n", "    count, bins, ignored = plt.hist(samples, 30, density=True)\n", "    ax.fill_between(xs,ys,0, alpha=0.1)\n", "    ax.set_xlim([-6,6])\n", "    ax.set_xlabel('x')\n", "    ax.set_ylabel('p')\n", "    ax.legend()\n", "    plt.show()\n", "    #now return our sampled normal distribution\n", "    return samples\n", "    \n", "samples=plotGausSampleLike(2,100,2)"]}, {"cell_type": "markdown", "id": "7bce363d", "metadata": {"tags": ["learner", "md"]}, "source": ["What have we done?  Essentially, we had previously claimed that our measurement was at zero with uncertainty (expected $\\sigma$=1). Now what we have done is made the claim that our uncertainty is way larger. What this has done is lowered our likelihood for something observed to happen to some number that is now not insanely large. "]}, {"cell_type": "markdown", "id": "34857805", "metadata": {"tags": ["learner", "md"]}, "source": ["### Challenge question\n", "\n", "Consider that we have two gaussians one with mean of 0 and one with mean of 0.1, and both have width 1. In one experiment, we sample it 1000 samples, and in another we sample it 1,000,000 times. What is the max likelihood value of all the observed points? What would the case be if we sampled even more (10M...)? What does this mean about the distributions? "]}, {"cell_type": "code", "execution_count": 20, "id": "8913e51c", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "#solution\n", "def maxlikelihood(isamples,mu1=0.2,sig1=1,mu2=0,sig2=1):\n", "    val=np.max(isamples)#compute the highest sampled gaussian\n", "    #now compute the liklihood of these two\n", "    like=stats.norm.pdf(val,mu1,sig1)/stats.norm.pdf(val,mu2,sig2)\n", "    return like\n", "\n", "def maxlike(iN):\n", "    samples=plotGausSampleLike(0.2,iN,1)\n", "    like=maxlikelihood(samples)\n", "    print(\"Max:\",iN,\" is \",like)\n", "\n", "maxlike(1000)\n", "maxlike(1000000)"]}, {"cell_type": "markdown", "id": "c9db157d", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_10_3'></a>     \n", "\n", "| [Top](#section_10_0) | [Restart Section](#section_10_3) | [Next Section](#section_10_4) |\n"]}, {"cell_type": "markdown", "id": "fd89002b", "metadata": {"tags": ["learner", "md", "catsoop_03"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 10.3.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 33, "id": "d8a0c882", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "762a5e83", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "251835bf", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_10_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L10.4 Bayesian vs. Frequentist and Likelihood</h2>  \n", "\n", "| [Top](#section_10_0) | [Previous Section](#section_10_3) | [Exercises](#exercises_10_4) | [Next Section](#section_10_5) |\n"]}, {"cell_type": "markdown", "id": "352dd78f", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section."]}, {"cell_type": "code", "execution_count": 2, "id": "be50f51c", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"data": {"text/html": ["\n", "        <iframe\n", "            width=\"970\"\n", "            height=\"550\"\n", "            src=\"https://mitx-8s50.github.io/slides/L10/slides_L10_04.html\"\n", "            frameborder=\"0\"\n", "            allowfullscreen\n", "            \n", "        ></iframe>\n", "        "], "text/plain": ["<IPython.lib.display.IFrame at 0x10d4c2b80>"]}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": ["#>>>RUN\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L10/slides_L10_04.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "8c708ebe", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "Now, this interpretation all comes to a head when we start to think about how we wish to quote our results. If our priors are incorrect, and we continue to progressively take data, at some point our priors are going to be wrong. Dealing with how this is wrong depends on what sort of statistician you are.  See the diagram in the first slide above.\n", "\n", "**Frequentist** If I see a data distribution, I can come up with a way to fit it. By getting a good fit, I can explain the next result. \n", "\n", "**Bayesian** I start with a model, by taking more and more measurements, I can improve my model. \n", "\n", "To try to capture these ideas, lets fit this guassian with two separate approaches. "]}, {"cell_type": "code", "execution_count": 20, "id": "d4e3413a", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "#with Bayesian, we hypothesize a guassian and fit it\n", "from lmfit.models import GaussianModel\n", "#randomly sample 100 events froma. gaussian\n", "lN=100\n", "samples = np.random.normal(0,1,lN)\n", "#make a histogram\n", "count, bins, ignored = plt.hist(samples,30)\n", "binscenters = np.array([0.5 * (bins[i] + bins[i+1]) for i in range(len(bins)-1)])\n", "#poisson unc.\n", "weight=1./np.sqrt(count)\n", "weight[weight==float('+inf')] = 0\n", "plt.show()\n", "\n", "#Now we can consider two ways to interpret the data\n", "def frequentist(iBins,iCount,weight): #fit a gaussian float all parameters\n", "    model = GaussianModel()\n", "    params = model.make_params(center=2, amplitude=1, sigma=1) \n", "    result = model.fit(iCount, params, x=iBins,weights=weight)\n", "    result.plot()\n", "    print(result.fit_report())\n", "    \n", "def bayesianBad(iBins,iCount,weight):#fit a gaussian fix the mean and sgima\n", "    model = GaussianModel()\n", "    params = model.make_params(center=2, amplitude=1, sigma=1) \n", "    params['center'].vary=False\n", "    params['sigma'].vary=False\n", "    result = model.fit(iCount, params, x=iBins,weights=weight)\n", "    result.plot()\n", "    print(result.fit_report())\n", "\n", "frequentist(binscenters,count,weight)\n", "bayesianBad(binscenters,count,weight)"]}, {"cell_type": "markdown", "id": "05f0f112", "metadata": {"tags": ["learner", "md"]}, "source": ["From above, what you see is that if we have a sample data, in the frequentist scenario, we just fit this distribution, and extract the parameters. In the frequentist approach, the data is key, and so if our $\\chi^{2}$ is good for our fitted model, we can declare success. \n", "\n", "In the Bayesian approach, we need to reocncile our prior with our fitted data. If our prior is that our data should behave as a gaussian about two, and we try to fit it to the data, you see there is not a very good $\\chi^{2}$ value, and the fit is clearly off. \n", "\n", "To reconcile our Bayesian fit, what we need to do is modify our model so that we can actually go from our prior to our fitted function. To do this, we need to insert a new prior. In this case, our prior will be that the mean of the Gaussian can vary. Let's write this down. \n", "\n", "\\begin{eqnarray}\n", " P\\left(\\mathcal{H}=x\\right|\\mu,\\sigma) & = & \\mathcal{N}(x,\\mu=2,\\sigma=1) \\\\\n", " P\\left(\\mathcal{H}=\\mu\\right|\\sigma) & = & \\frac{1}{b-a}~\\forall~\\mu~\\in~[a,b] \\\\ \n", "                               & = & 0~~~~~~~~~\\forall~\\mu~\\notin~[a,b]\n", "\\end{eqnarray}\n", "All that we are saying above is that mu can now vary between a and b. Lets now fit the data with this statement. To do this, we are going to use a new feature in `lmfit`. What we are aroing to do is call `lmfit.minimize` and feed it a modified loss, which we define as `resid` (aka the residual function).\n", "\n", "Additionally, we will add another constraint and put it into the fit. In particular, we will add a paramater $\\Delta_{\\mu}$, such that the loss and $\\mu$ will be written as: \n", "\\begin{eqnarray}\n", "\\mu_{\\rm new} & = & \\mu -\\Delta_{\\mu} \\\\\n", "\\mathcal{L}_{\\rm new} & = & \\mathcal{L} + 0~\\forall~\\mu~\\in~[a,b] \\\\ \n", "                      & = & \\mathcal{L} + \\inf~\\forall~\\mu~\\notin~[a,b] \n", "\\end{eqnarray}\n", "In this case, we will just approximate a really large number as infinity. "]}, {"cell_type": "code", "execution_count": 20, "id": "d0d411e4", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "import lmfit\n", "#here is our modified function\n", "def gauss(x, amp, mu, sigma,dmu):\n", "    return amp * np.exp(-(x-mu+dmu)**2 / (2.*sigma**2))\n", "\n", "#now we define our loss we want to minimize\n", "def resid(params, x, ydata,weights):\n", "    mu    = params['center'].value\n", "    sigma = params['sigma'].value\n", "    amp   = params['amplitude'].value\n", "    dmu   = params['deltamu'].value\n", "    lossshift=0\n", "    if abs(dmu) > 3:\n", "        lossshift=1e32\n", "    y_model= gauss(x,amp,mu,sigma,dmu)\n", "    residarr = (y_model - ydata)*weights\n", "    #now append our constraint to the loss\n", "    residarr = np.append(residarr,lossshift)\n", "    return residarr\n", "    \n", "def bayesianGood(iBins,iCount,weights):\n", "    model = GaussianModel()\n", "    params = model.make_params(center=2, amplitude=1, sigma=1) \n", "    params['center'].vary=False\n", "    params['sigma'].vary=False\n", "    params.add(\"deltamu\", value=0.0, min=-10, max=10) #Our new line of code\n", "    result = lmfit.minimize(resid, params, args=(iBins, iCount,weights))\n", "    lmfit.report_fit(result)\n", "    #Now we plot it. \n", "    plt.errorbar(iBins, iCount,np.sqrt(iCount), lw=2,fmt=\".k\", capsize=0)\n", "    plt.plot(binscenters,gauss(binscenters,result.params['amplitude'].value,result.params['center'].value,result.params['sigma'].value,result.params['deltamu'].value))\n", "    plt.xlabel(\"x\")\n", "    plt.ylabel(\"p\")\n", "    plt.show()\n", "\n", "bayesianGood(binscenters,count,weight)\n", "\n"]}, {"cell_type": "markdown", "id": "d04556aa", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_10_4'></a>     \n", "\n", "| [Top](#section_10_0) | [Restart Section](#section_10_4) | [Next Section](#section_10_5) |\n"]}, {"cell_type": "markdown", "id": "729ba31a", "metadata": {"tags": ["learner", "md", "catsoop_04"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 10.4.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 43, "id": "e36a1e8d", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "3f8d63bd", "metadata": {"tags": ["learner", "catsoop_04", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "3a14e017", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_10_5'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L10.5 Questions About Likelihood</h2>  \n", "\n", "| [Top](#section_10_0) | [Previous Section](#section_10_4) | [Exercises](#exercises_10_5) | [Next Section](#section_10_6) |\n"]}, {"cell_type": "markdown", "id": "c2f8c9da", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "TEXT\n"]}, {"cell_type": "code", "execution_count": 50, "id": "19489b7b", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "#code\n"]}, {"cell_type": "markdown", "id": "92ce12f3", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_10_5'></a>     \n", "\n", "| [Top](#section_10_0) | [Restart Section](#section_10_5) | [Next Section](#section_10_6) |\n"]}, {"cell_type": "markdown", "id": "77b83f34", "metadata": {"tags": ["learner", "md", "catsoop_05"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 10.5.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 53, "id": "26e25076", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "e2d708cc", "metadata": {"tags": ["learner", "catsoop_05", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "88302269", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_10_6'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L10.6 Likelihood Ratio</h2>     \n", "\n", "| [Top](#section_10_0) | [Previous Section](#section_10_5) | [Exercises](#exercises_10_6) |\n"]}, {"cell_type": "markdown", "id": "cd61cf55", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section."]}, {"cell_type": "code", "execution_count": 3, "id": "34ca2799", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"data": {"text/html": ["\n", "        <iframe\n", "            width=\"970\"\n", "            height=\"550\"\n", "            src=\"https://mitx-8s50.github.io/slides/L10/slides_L10_06.html\"\n", "            frameborder=\"0\"\n", "            allowfullscreen\n", "            \n", "        ></iframe>\n", "        "], "text/plain": ["<IPython.lib.display.IFrame at 0x10d4c25b0>"]}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": ["#>>>RUN\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L10/slides_L10_06.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "567a1ffa", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_10_6'></a>   \n", "\n", "| [Top](#section_10_0) | [Restart Section](#section_10_6) |\n"]}, {"cell_type": "markdown", "id": "db83a929", "metadata": {"tags": ["learner", "md", "catsoop_06"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 10.6.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 63, "id": "2bce8b57", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "8709c99c", "metadata": {"tags": ["learner", "catsoop_06", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}