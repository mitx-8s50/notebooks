{"cells": [{"cell_type": "markdown", "id": "406ac73c", "metadata": {"tags": ["learner", "md"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 20: An Introduction to Probabilistic Programming</h1>\n"]}, {"cell_type": "markdown", "id": "347f9e61", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_20_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L20.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "4e6485e6", "metadata": {"tags": ["learner", "md"]}, "source": ["<table style=\"width:100%\">\n", "    <colgroup>\n", "       <col span=\"1\" style=\"width: 40%;\">\n", "       <col span=\"1\" style=\"width: 15%;\">\n", "       <col span=\"1\" style=\"width: 45%;\">\n", "    </colgroup>\n", "    <tr>\n", "        <th style=\"text-align: left; font-size: 13pt;\">Section</th>\n", "        <th style=\"text-align: left; font-size: 13pt;\">Exercises</th>\n", "        <th style=\"text-align: left; font-size: 13pt;\">Summary</th>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_20_1\">L20.1 Overview of Probabilistic Programming</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_20_1\">L20.1 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_20_2\">L20.2 A Familiar Example - The Hubble Constant</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_20_2\">L20.2 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_20_3\">L20.3 Inference with HMC</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_20_3\">L20.3 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_20_4\">L20.4 Inference with SVI</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_20_4\">L20.4 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_20_5\">L20.5 Modeling More Complex Distributions</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_20_5\">L20.5 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_20_6\">L20.6 Some Real-World Applications</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_20_6\">L20.6 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_20_7\">L20.7 Discussion Part I</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_20_7\">L20.7 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_20_8\">L20.8 Discussion Part II</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_20_8\">L20.8 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "</table>\n", "\n"]}, {"cell_type": "markdown", "id": "e528aab2", "metadata": {"tags": ["learner", "catsoop_00", "md"]}, "source": ["<h3>Learning Objectives</h3>\n", "\n", "Text needed\n"]}, {"cell_type": "markdown", "id": "96983a89", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Importing Libraries</h3>\n", "\n", "Before beginning, run the cell below to import the relevant libraries for this notebook. \n", "Optionally, set the plot resolution and default figure size.\n"]}, {"cell_type": "code", "execution_count": 7, "id": "c816effe", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import matplotlib.pylab as pylab\n", "\n", "%matplotlib inline\n", "%load_ext autoreload\n", "%autoreload 2\n", "\n", "#color_default = 'firebrick'\n", "\n", "#set plot resolution\n", "#%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure size\n", "#plt.rcParams['figure.figsize'] = (9,6)\n"]}, {"cell_type": "code", "execution_count": 7, "id": "6d515f9f", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "! pip install emcee corner pyro-ppl tqdm seaborn"]}, {"cell_type": "code", "execution_count": 7, "id": "10792f9a", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "! pip install --upgrade daft"]}, {"cell_type": "code", "execution_count": 7, "id": "fe4573d8", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "# Fix seed for reproducibility\n", "\n", "import random\n", "import torch\n", "import pyro\n", "\n", "SEED = 192\n", "\n", "random.seed(SEED)\n", "np.random.seed(SEED)\n", "torch.manual_seed(SEED)\n", "pyro.set_rng_seed(SEED)"]}, {"cell_type": "markdown", "id": "51fc4ddb", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_20_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L20.1 Overview of Probabilistic Programming</h2>  \n", "\n", "| [Top](#section_20_0) | [Previous Section](#section_20_0) | [Exercises](#exercises_20_1) | [Next Section](#section_20_2) |\n"]}, {"cell_type": "markdown", "id": "a24ba62d", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>What is probabilistic programming?</h3>\n", "\n", "<h4>Probabilistic modeling as storytelling</h4>\n", "\n", "- One way to look at defining probabilistic models: **telling a story about how you think your data came about**\n", "- Many scientific observations of interest can be described in terms of simulators\n", "- Simulators execute a series of stochastic steps to end up at a realization of the dataset of interest---**simulators are probabilistic programs**"]}, {"cell_type": "markdown", "id": "bbc645bd", "metadata": {"tags": ["learner", "md"]}, "source": ["<img src=\"https://raw.githubusercontent.com/smsharma/smsharma/master/lhc.png#\" alt=\"LHC Schematic\" width=\"800\"/>"]}, {"cell_type": "markdown", "id": "80304e50", "metadata": {"tags": ["learner", "md"]}, "source": ["$$\\theta \\sim p(\\theta)$$\n", "$$\\mathrm{parton} \\sim p(\\mathrm{parton}\\mid\\theta)$$\n", "$$\\mathrm{shower} \\sim p(\\mathrm{shower}\\mid\\mathrm{parton})$$\n", "$$\\mathrm{detector} \\sim p(\\mathrm{detector}\\mid\\mathrm{shower})$$\n", "$$x \\sim p(x\\mid \\mathrm{detector})$$"]}, {"cell_type": "markdown", "id": "59e7fd89", "metadata": {"tags": ["learner", "md"]}, "source": ["- We want to _marginalize_ over the nuisance parameters {parton, shower, detector} and work with the likelihood $p(x\\mid\\theta)$, which is the central object necessary for statistical inference\n", "- By writing our forward model as a probabilistic program, we can then use inference algorithms to 'invert' the process and get the Bayesian posterior $p(\\theta\\mid x)$ for a given observation"]}, {"cell_type": "markdown", "id": "2a396bb2", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Probabilistic programming languages (PPLs)</h3>\n", "\n", "- Probabilistic programming languages (PPLs) realize this paradigm by treating (random) variables and distributions as first-class objects, allowing us to\n", "    - Compose probabilistic models in terms of distributions of random variables\n", "    - Perform inference on models by them conditioning on observations"]}, {"cell_type": "markdown", "id": "2525c6d0", "metadata": {"tags": ["learner", "md"]}, "source": ["A biased sample of PPLs:\n", "- [Pyro](https://pyro.ai/)/[NumPyro](https://num.pyro.ai/en/stable/): based on PyTorch/JAX with strengths in variational inference and MCMC respectively\n", "- [PyMC3](https://docs.pymc.io/en/v3/): a solid general-purpose PPL in Python\n", "- [Turing.jl](https://turing.ml/stable/): based on Julia\n", "- [Tensorflow Probability](https://www.tensorflow.org/probability): based on Tensorflow\n", "\n", "Being based on differentiable framework, they make it easy to use gradient-based optimization and more efficient inference algorithms."]}, {"cell_type": "code", "execution_count": 10, "id": "23538537", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "#code\n"]}, {"cell_type": "markdown", "id": "6551d81d", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_20_1'></a>     \n", "\n", "| [Top](#section_20_0) | [Restart Section](#section_20_1) | [Next Section](#section_20_2) |\n"]}, {"cell_type": "markdown", "id": "7a76a160", "metadata": {"tags": ["learner", "md", "catsoop_01"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 20.1.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 13, "id": "28116848", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "67aa16bf", "metadata": {"tags": ["learner", "catsoop_01", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "379edc4f", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_20_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L20.2 A Familiar Example - The Hubble Constant</h2>  \n", "\n", "| [Top](#section_20_0) | [Previous Section](#section_20_1) | [Exercises](#exercises_20_2) | [Next Section](#section_20_3) |\n"]}, {"cell_type": "markdown", "id": "8237b5df", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "We will start by playing around with the familiar supernova dataset that we've seen a few time before. Let's download, load and plot it."]}, {"cell_type": "code", "execution_count": 20, "id": "5acc8957", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "import urllib.request\n", "\n", "# Download the SN data file\n", "url = 'https://stash.osgconnect.net/public/dleon/CosmoMC/data/sn_z_mu_dmu_plow_union2.1.txt'\n", "urllib.request.urlretrieve(url, './sn_z_mu_dmu_plow_union2.1.txt')"]}, {"cell_type": "code", "execution_count": 20, "id": "60dc4352", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "data = np.genfromtxt('sn_z_mu_dmu_plow_union2.1.txt')\n", "\n", "zs = data.T[1]  # Redshift\n", "mm = data.T[2]  # Difference between apparent and absolute magnitude\n", "dm = data.T[3]  # Its uncertainty\n", "\n", "# Get distance modulus and its uncertainty\n", "d = 10. ** (mm / 5. + 1.) / 1e6  \n", "derr = 10. ** ((mm + dm) / 5. + 1. - 6.) - d\n", "\n", "c = 3e5  # Speed of light"]}, {"cell_type": "code", "execution_count": 20, "id": "b26758c7", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "print(\"Dataset consists of {} data points\".format(len(d)))"]}, {"cell_type": "code", "execution_count": 20, "id": "c8cf4fe2", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "plt.figure(figsize=(8,6))\n", "\n", "plt.errorbar(zs, d, yerr=derr, marker='.', ls='None', color='k')\n", "plt.xlabel(r\"$z$\")\n", "plt.ylabel(r\"Distance\\,[Mpc]\")"]}, {"cell_type": "markdown", "id": "5a97a54a", "metadata": {"tags": ["learner", "md"]}, "source": ["As before, we'll use this dataset to infer the hubble parameter $H_0$, as well as an additional parameter $q$, historically called the deceleration parameter. $q < 0$ corresponds to an accelerating universe, $q > 0$ a decelerating universe (see https://en.wikipedia.org/wiki/Deceleration_parameter for further details).\n", "\n", "The theoretical distance modulus prediction can be computed as \n", "$$ d = \\frac{c}{H_0}\\left(z + \\frac{1}{2}(1 - q) \\, z^2\\right)$$"]}, {"cell_type": "code", "execution_count": 20, "id": "487e0849", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "z_max = 0.5  # Maximum redshift up to which to use data"]}, {"cell_type": "markdown", "id": "2f64a71f", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Weighted least squares</h3>\n", "\n", "We'll start with a simple weighted least squares approach using `scipy.optimize.minimize`, which should be familiar. We write down a likelihood, and use the optimizer to obtain the maximum-likelihood estimate (MLE) by minimizing the (negative) log-likelihood."]}, {"cell_type": "code", "execution_count": 20, "id": "ed6e397c", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "from scipy.optimize import minimize"]}, {"cell_type": "code", "execution_count": 20, "id": "1295fc2e", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "def d_pred(z, H0, q):\n", "    \"\"\" Predicted distance modulus\n", "    \"\"\"\n", "    return c / H0 * (z + 0.5 * (1 - q) * z ** 2)"]}, {"cell_type": "code", "execution_count": 20, "id": "a74d7a81", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "def log_likelihood(theta, x, y, yerr):\n", "    \"\"\" Gaussian log-likelihood of dist. mod. given H0 and deceleration parameter q\n", "    \"\"\"\n", "    H0, q = theta\n", "    y_pred = d_pred(x, H0, q)\n", "    ll = -0.5 * np.sum((y - y_pred) ** 2 / yerr ** 2)\n", "    return ll"]}, {"cell_type": "code", "execution_count": 20, "id": "07552092", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "nll = lambda *args: -log_likelihood(*args)\n", "soln = minimize(nll, [70, 0.5], bounds=[[1e-5,100],[-2., 2.]], args=(zs[zs < z_max], d[zs < z_max], derr[zs < z_max]), )\n", "H0_mle, q_mle = soln.x"]}, {"cell_type": "code", "execution_count": 20, "id": "b0310a4f", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["print(\"MLE estimates are H0 = {}, q0 = {}\".format(H0_mle, q_mle))"]}, {"cell_type": "markdown", "id": "c0745c10", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>MCMC with `emcee`</h3>\n", "\n", "Next, we'll do traditional MCMC with `emcee`, which should again be familiar. Here, we draw samples from a log-probability distribution, with the samples then being representative of the joint posterior on the two parameters of interest."]}, {"cell_type": "code", "execution_count": 20, "id": "8ec89679", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["import emcee\n", "import corner"]}, {"cell_type": "code", "execution_count": 20, "id": "b8ada6ba", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["def log_prob(theta, x, y, y_err):\n", "        \n", "    H0, q = theta\n", "        \n", "    # Prior contribution\n", "    \n", "    lp = 0\n", "    if not 10. < H0 < 100. and not -5. < q < 5.:\n", "        lp = -np.inf\n", "\n", "    return lp + log_likelihood(theta, x, y, y_err)"]}, {"cell_type": "code", "execution_count": 20, "id": "5bbf0423", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["sampler = emcee.EnsembleSampler(nwalkers=32, ndim=2, log_prob_fn=log_prob, args=(zs[zs < z_max], d[zs < z_max], derr[zs < z_max]))\n", "pos = soln.x + 1e-4 * np.random.randn(32, 2)\n", "sampler.run_mcmc(pos, 5000, progress=True);"]}, {"cell_type": "code", "execution_count": 20, "id": "34ea7f10", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["samples_emcee = sampler.get_chain(discard=1000, thin=15, flat=True)"]}, {"cell_type": "code", "execution_count": 20, "id": "2c92ab48", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["corner.corner(samples_emcee, truths=soln.x, labels=[r\"$H_0$\", r\"$q$\"]);"]}, {"cell_type": "markdown", "id": "b047e511", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Differentiable probabilistic programming approaches with `Pyro`</h3>"]}, {"cell_type": "code", "execution_count": 20, "id": "3910ab50", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["import torch\n", "import pyro\n", "import pyro.distributions as dist\n", "from pyro.infer import autoguide\n", "from pyro.optim import Adam\n", "from pyro.infer import SVI, Trace_ELBO\n", "from tqdm.notebook import tqdm"]}, {"cell_type": "markdown", "id": "e58975fe", "metadata": {"tags": ["learner", "md"]}, "source": ["Now let's try our hand at the probabilistic programming approach. We will use a probabilistic programming language (PPL) framework called `Pyro`, which derives from `PyTorch`. There are two parts to this whole thing:\n", "1. _Define the model_\n", "2. _Perform inference on it_"]}, {"cell_type": "markdown", "id": "12781c91", "metadata": {"tags": ["learner", "md"]}, "source": ["<h4>2.3.1 Defining the model: probabilistic graphical models (PGMs)</h4>\n", "\n", "In the probabilistic programming spirit, we will define our model as a probabilistic program which encodes the dependencies between random variables. A nice way to visualize complex models are _probabilistic graphical models_ (PGMs) which specify the conditional dependencies between variables, both modeled and observed. \n", "\n", "We will use [`daft`](https://docs.daft-pgm.org/en/latest/api/#the-pgm-object) to visualize our model using a PGM, below. The various probability distributions are represented in **red rectangles**, stochastic random variables in **black circles**, and _observed_ parameters in **double circles**. The conditional independence between a set of variables (here, $N$ different supernova observations) are indicated by placing these inside a large rectangle (called a _plate_)."]}, {"cell_type": "code", "execution_count": 20, "id": "61174dfd", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["import daft\n", "\n", "pgm = daft.PGM(observed_style=\"outer\")\n", "\n", "pgm.add_node(\"P(H_0)\", r\"$P(H_0)$\", -1, 4, shape=\"rectangle\", aspect=1.5, plot_params={\"ec\":color_default})\n", "pgm.add_node(\"H_0\", r\"$H_0$\", 0, 4)\n", "pgm.add_node(\"obs\", r\"$d^{\\mathrm{obs}}_n$\", 4, 3.5, observed=True)\n", "pgm.add_node(\"err\", r\"$\\sigma_{d_n}^\\mathrm{obs}$\", 2.5, 4.5, observed=True)\n", "pgm.add_node(\"P(d_n|d)\", r\"$P\\left(d^{\\mathrm{obs}}_n\\mid d_n, \\sigma_{d_n}^\\mathrm{obs}\\right)$\", 2.5, 3.5, scale=1, aspect=3.0, shape=\"rectangle\", plot_params={\"ec\":color_default})\n", "pgm.add_node(\"d\", r\"$d_n$\", 1, 3.5)\n", "\n", "pgm.add_node(\"P(q)\", r\"$P(q)$\", -1, 3, shape=\"rectangle\", aspect=1.5, plot_params={\"ec\":color_default})\n", "pgm.add_node(\"q\", r\"$q$\", 0, 3)\n", "\n", "pgm.add_plate([0.5, 2.5, 4, 2.5], label=r\"$n=1\\ldots N$ Supernovae\")\n", "\n", "pgm.add_edge(\"P(H_0)\", \"H_0\")\n", "pgm.add_edge(\"H_0\", \"d\")\n", "pgm.add_edge(\"q\", \"d\")\n", "pgm.add_edge(\"P(q)\", \"q\")\n", "pgm.add_edge(\"P(d_n|d)\", \"obs\")\n", "pgm.add_edge(\"err\", \"P(d_n|d)\")\n", "pgm.add_edge(\"d\", \"P(d_n|d)\")\n", "\n", "pgm.render(dpi=150);"]}, {"cell_type": "markdown", "id": "a54b9b3d", "metadata": {"tags": ["learner", "md"]}, "source": ["The relevant distributions are asssumed to be as follows:\n", "$$P(H_0) = \\mathrm{Unif}(10., 100.)$$\n", "$$P(q) = \\mathrm{Unif}(-5, 5)$$\n", "$$P\\left(d^{\\mathrm{obs}}_n\\mid d\\right) = \\mathcal N\\left(d^{\\mathrm{obs}}_n\\mid d_n, \\sigma_{d_n}^\\mathrm{obs}\\right)$$"]}, {"cell_type": "markdown", "id": "d25dcd92", "metadata": {"tags": ["learner", "md"]}, "source": ["Defining the model in `Pyro` follows the above PGM closely\n", "- Stochastic random variables are defined with `pyro.sample(name, distribution)`\n", "- Deterministic variables are specified with `pyro.deterministic(name, expression)`\n", "- Conditioning on observations is done using `pyro.sample(name, distribution, obs=observation)`\n", "- Independence between random variables (plating) is specified using `pyro.plate`"]}, {"cell_type": "code", "execution_count": 20, "id": "f45438a1", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["def model(x, y, yerr):\n", "    \"\"\" Model definition in Pyro\n", "    \"\"\"\n", "    \n", "    # Stochastic nodes\n", "    H0 = 100 * pyro.sample(\"h0\", dist.Uniform(0.1, 1.0))\n", "    q = pyro.sample(\"q\", dist.Uniform(-5, 5))\n", "    \n", "    # Deterministic node\n", "    y_pred = pyro.deterministic(\"y_pred\", c / H0 * (x + 0.5 * (1 - q) * x ** 2))\n", "    \n", "    # N different independent observations\n", "    with pyro.plate('observe_data'):\n", "        return pyro.sample(\"obs\", dist.Normal(y_pred, yerr), obs=y)"]}, {"cell_type": "markdown", "id": "03a12d9e", "metadata": {"tags": ["learner", "md"]}, "source": ["Having defined the model, we can go on to perform inference on it. In an ideal world, we wouldn't have to think about this part at all---all of the \"science\" happens in the model definition (arguably). \n", "\n", "This is (unfortunately) not the casel however---choosing how to do inference on complex, high-dimensional models can be hard! Here are some examples using what's built in in `Pyro`:\n", "- Hamiltonian Monte Carlo (HMC)\n", "- Stochastic Variational Inference (SVI)"]}, {"cell_type": "markdown", "id": "956acc3a", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_20_2'></a>     \n", "\n", "| [Top](#section_20_0) | [Restart Section](#section_20_2) | [Next Section](#section_20_3) |\n"]}, {"cell_type": "markdown", "id": "3b7cadcf", "metadata": {"tags": ["learner", "md", "catsoop_02"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 20.2.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 23, "id": "f358e049", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "ffb65fe5", "metadata": {"tags": ["learner", "catsoop_02", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "d6323523", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_20_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L20.3 Inference with HMC</h2>  \n", "\n", "| [Top](#section_20_0) | [Previous Section](#section_20_2) | [Exercises](#exercises_20_3) | [Next Section](#section_20_4) |\n"]}, {"cell_type": "markdown", "id": "3cb01094", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "- Markov Chain Monte Carlo (MCMC) algorithms let us sample from probability distributions---you've seen examples already and implemented the Metroplis-Hastings-[Rosenbluth](https://www.nytimes.com/2021/02/09/science/arianna-wright-dead.html) algorithm. Traditional MCMC algorithms can becomes increasingly inefficient when the parameter space of interest becomes high dimensional. \n", "\n", "- Hamiltonian Monte Carlo (HMC) is a variant of MCMC which uses likelihood _gradient_ information $\\nabla_\\theta\\mathcal L$ in addition to the likelihood $\\mathcal L$ and can scale much better to sampling from higher-dimensional distributions. Intuitively, it leverages the _momentum_ of the likelihood function in addition to the position.\n", "\n", "- See [this page](https://chi-feng.github.io/mcmc-demo/app.html) for a cool demo and comparison of some sampling algorithms, including HMC and vanilla random walk Metropolis-Hastings.\n"]}, {"cell_type": "code", "execution_count": null, "id": "4ff6d69f", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#>>>RUN\n", "\n", "from IPython.display import IFrame\n", "IFrame('https://chi-feng.github.io/mcmc-demo/app.html?algorithm=RandomWalkMH&target=donut', width=750, height=500)"]}, {"cell_type": "markdown", "id": "0b37682a", "metadata": {"tags": ["learner", "md"]}, "source": ["Since we've written down our model in a differentiable framework, we can easily run HMC on it. Now we're getting into _differentiable_ probabilistic programming."]}, {"cell_type": "code", "execution_count": null, "id": "c55528a6", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["from pyro.infer import MCMC, NUTS\n", "\n", "nuts_kernel = NUTS(model)\n", "mcmc = MCMC(nuts_kernel, num_samples=2000, warmup_steps=300)\n", "\n", "mcmc.run(torch.Tensor(zs[zs < z_max]), torch.Tensor(d[zs < z_max]), torch.Tensor(derr[zs < z_max]))"]}, {"cell_type": "code", "execution_count": null, "id": "11302ea6", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["samples_hmc = {k: v.detach().cpu().numpy() for k, v in mcmc.get_samples().items()}\n", "samples_hmc = np.array([samples_hmc['h0'] * 100, samples_hmc['q']]).T"]}, {"cell_type": "code", "execution_count": null, "id": "e6ce809b", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["import matplotlib.lines\n", "\n", "figure = corner.corner(samples_hmc, labels=[r\"$H_0$\", r\"$q$\"])\n", "corner.corner(samples_emcee, fig=figure,weights=np.ones(len(samples_emcee)) / (len(samples_emcee) / len(samples_hmc)), color=color_default);\n", "\n", "red_line = matplotlib.lines.Line2D([], [], color='firebrick', label='MCMC')\n", "black_line = matplotlib.lines.Line2D([], [], color='k', label='HMC')\n", "\n", "plt.legend(handles=[black_line, red_line], bbox_to_anchor=(0., 1.0, 1., .0), loc=4)"]}, {"cell_type": "markdown", "id": "bd19c4c1", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_20_3'></a>     \n", "\n", "| [Top](#section_20_0) | [Restart Section](#section_20_3) | [Next Section](#section_20_4) |\n"]}, {"cell_type": "markdown", "id": "14f5caee", "metadata": {"tags": ["learner", "md", "catsoop_03"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 20.3.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 33, "id": "16a41a0c", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "8b7af707", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "389a7c73", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_20_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L20.4 Inference with SVI</h2>  \n", "\n", "| [Top](#section_20_0) | [Previous Section](#section_20_3) | [Exercises](#exercises_20_4) | [Next Section](#section_20_5) |\n"]}, {"cell_type": "markdown", "id": "d3897a48", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "Other than obtaining samples from the posterior of interest, another method for approximating posterior distributions is to **optimize over a family of functions (called the _variational family_) such that the inferred distribution closely matches the actual posterior.**\n", "\n", "The name comes from the _calculus of variations_, where infinitesimal changes to functionals (functions of functions, colloquially) are studied.\n", "\n", "- In contrast to the MCMC-style approaches, this turns posterior inference into an optimization problem, rather than a sampling problem. \n", "- In contrast to the MLE approach (and the deep learning regression approach), rather than finding a single optimal set of parameter values (optimizing over possible numbers), we are now finding a single optimal _distribution_ (optimizing over possible functions).\n", "\n", "Let's demonstrate this with plots, arbitrarily showing the \"true\" posterior as a t-distribution (just for schematic purposes).\n"]}, {"cell_type": "code", "execution_count": 40, "id": "68317900", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "from scipy.stats import t, norm\n", "\n", "# \"True\" distribution, just for schematic purposes\n", "x = np.linspace(66, 74, 200)\n", "post_fake = t.pdf(x, df=2, loc=np.mean(samples_emcee[:, 0]), scale=np.std(samples_emcee[:, 0]))\n"]}, {"cell_type": "code", "execution_count": 40, "id": "67bfb879", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "fig, ax = plt.subplots(ncols=3, nrows=1, figsize=(16,4))\n", "\n", "ax[0].axvline(soln.x[0], label=\"MLE\")\n", "ax[0].plot(x, post_fake, color=color_default, label=\"Log-probability\")\n", "ax[0].set_title(\"MLE/MAP estimate\")\n", "\n", "ax[1].hist(samples_emcee[:, 0], histtype='step', density=True, label=\"Sampled posterior\")\n", "ax[1].plot(x, post_fake, color=color_default, label=\"Log-probability\")\n", "ax[1].set_title(\"MCMC\")\n", "\n", "# Arbitrarily-defined Gaussian approximation for the posterior\n", "post_approx = norm.pdf(x, loc=np.mean(samples_emcee[:, 0]), scale=np.std(samples_emcee[:, 0]))\n", "ax[2].plot(x, post_approx, label=\"Variational distribution\")\n", "ax[2].plot(x, post_fake, color=color_default, label=\"Log-probability\")\n", "ax[2].set_title(\"Variational inference\")\n", "\n", "[ax[i].set_xlabel(r\"$\\theta$\") for i in range(3)]\n", "[ax[i].set_ylabel(r\"$\\log p$\") for i in range(3)]\n", "[ax[i].set_ylim(0, 1) for i in range(3)]\n", "[ax[i].legend(loc='upper left') for i in range(3)]"]}, {"cell_type": "markdown", "id": "e149c0cc", "metadata": {"tags": ["learner", "md"]}, "source": ["Here, we can borrow many of the optimization tricks from deep learning and efficiently scale to much higher dimensions and more complex likelihoods than is possible with sampling algorithms. In _stochastic_ variational inference, we use stochastic gradient descent to optimize over the variational family and target our desired posterior distribution.\n", "\n", "We minimize the KL-divergence (a particular measure of closeness between distributions, also known as the \"relative entropy\") to arrive at the distribution that approximates our posterior well:\n", "\n", "$$D_{\\mathrm{KL}}(P \\| Q)=\\int_{-\\infty}^{\\infty} p(x) \\log \\left(\\frac{p(x)}{q(x)}\\right) d x$$\n", "\n", "Let's compute $D_{\\mathrm{KL}}(P \\| Q)$ for a few different Gaussians, again `PyTorch` is doing all of the heavy lifting:"]}, {"cell_type": "code", "execution_count": 40, "id": "d87ca1e0", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["p = torch.distributions.Normal(loc=1,scale=1)\n", "q = torch.distributions.Normal(loc=1.2,scale=0.8)\n", "\n", "kl = torch.distributions.kl.kl_divergence(p, q)\n", "\n", "print(\"KL-divergence is {}\".format(kl))"]}, {"cell_type": "code", "execution_count": 40, "id": "e45114a6", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["p = torch.distributions.Normal(loc=1,scale=1)\n", "q = torch.distributions.Normal(loc=2.2,scale=0.8)\n", "\n", "kl = torch.distributions.kl.kl_divergence(p, q)\n", "\n", "print(\"KL-divergence is {}\".format(kl))"]}, {"cell_type": "markdown", "id": "3afc60a6", "metadata": {"tags": ["learner", "md"]}, "source": ["How do we do this without already knowing the posterior? While we can't minimize the KL-divergence directly, we can optimize another function that's equal to it up to a constant, called the Evidence Lower BOund (ELBO). This is abstracted away in `Pyro` and we won't go into details, but see [here](https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf) or [here](https://mpatacchiola.github.io/blog/2021/01/25/intro-variational-inference.html) for details and a derivation of this objective function."]}, {"cell_type": "markdown", "id": "0d6c310a", "metadata": {"tags": ["learner", "md"]}, "source": ["Now in addition to the model we need to define the variational family, in `Pyro` parlance known as a _guide_. Let's start with something simple---two different Gaussian distributions modeling the posteriors of the two parameters $H_0$ and $q$."]}, {"cell_type": "code", "execution_count": 40, "id": "01c00b36", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["def guide(x, y, yerr):\n", "    \"\"\" A variational distribution defined by two uncorrelated Gaussians\n", "    \"\"\"\n", "    \n", "    # Means and standard deviations of Gaussians\n", "    locs = pyro.param(\"loc\", 0.5 * torch.ones(2))\n", "    scales = pyro.param(\"scales\", 0.001 * torch.ones(2), constraint=torch.distributions.constraints.positive)\n", "    \n", "    # Parameter samples from variational distribution\n", "    h0 = pyro.sample(\"h0\", dist.Normal(locs[0], scales[0]))\n", "    q = pyro.sample(\"q\", dist.Normal(locs[1], scales[1]))"]}, {"cell_type": "markdown", "id": "ff5b8961", "metadata": {"tags": ["learner", "md"]}, "source": ["Now we can use the `Pyro` API to perform gradient descent on the guide parameters, in this case optimizing the means and stds of the two Gaussians."]}, {"cell_type": "code", "execution_count": 40, "id": "17db8a8e", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["pyro.clear_param_store()\n", "pyro.set_rng_seed(SEED)\n", "\n", "# Set up the optimizer\n", "opt = Adam({'lr':0.01})\n", "\n", "# Set up the inference algorithm\n", "svi = SVI(model, guide, opt, loss=Trace_ELBO())\n", "\n", "# Number of stochastic gradient descent steps\n", "n_steps = 5000\n", "\n", "# Do gradient steps\n", "for step in tqdm(range(n_steps)):\n", "    loss = svi.step(torch.Tensor(zs[zs < z_max]), torch.Tensor(d[zs < z_max]), torch.Tensor(derr[zs < z_max]))"]}, {"cell_type": "markdown", "id": "46a07eb9", "metadata": {"tags": ["learner", "md"]}, "source": ["After optimization, we can draw samples values from the guide to get a representative posterior. Note that since the modeled posterior in this case is quite simple (just two Gaussians) this could be done analytically. However in general for complex posteriors and high-dimensional distributions we need to sample."]}, {"cell_type": "code", "execution_count": 40, "id": "e8b8afa1", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["from pyro.infer import Predictive\n", "\n", "predictive = Predictive(model, guide=guide, num_samples=2000, return_sites=(\"h0\", \"q\"))\n", "\n", "samples_svi = predictive(torch.Tensor(zs[zs < z_max]), torch.Tensor(d[zs < z_max]), torch.Tensor(derr[zs < z_max]))\n", "samples_svi = np.array([samples_svi['h0'].flatten().numpy() * 100, samples_svi['q'].flatten().numpy()]).T"]}, {"cell_type": "markdown", "id": "8c7823b6", "metadata": {"tags": ["learner", "md"]}, "source": ["Let's plot the samples and compare with what we got from `emcee`:"]}, {"cell_type": "code", "execution_count": 40, "id": "bb1e8e4a", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["import matplotlib.lines\n", "\n", "figure = corner.corner(samples_svi, labels=[r\"$H_0$\", r\"$q$\"])\n", "corner.corner(samples_emcee, fig=figure,weights=np.ones(len(samples_emcee)) / (len(samples_emcee) / len(samples_svi)), color=color_default);\n", "\n", "red_line = matplotlib.lines.Line2D([], [], color='firebrick', label='MCMC')\n", "black_line = matplotlib.lines.Line2D([], [], color='k', label='SVI')\n", "\n", "plt.legend(handles=[black_line, red_line], bbox_to_anchor=(0., 1.0, 1., .0), loc=4)"]}, {"cell_type": "markdown", "id": "ac48752e", "metadata": {"tags": ["learner", "md"]}, "source": ["**It looks like the SVI approach doesn't do a great job at modeling the posterior, assuming the MCMC samples to be representative!** Not too surprising perhaps---our variational distribution totally ignored correlations between the two parameters, which clearly exist (the joint posterior is a diagonal ellipse, meaning there are degeneracies between parameters)."]}, {"cell_type": "markdown", "id": "4a9274c9", "metadata": {"tags": ["learner", "md"]}, "source": ["Fortunately, we can do better. `Pyro` provides a nice interface for automatically generate variational distributions, called \"autoguides\". For example, we can use a multivariate normal distribution which additionally fits for the covariances between different parameters, as follows: "]}, {"cell_type": "code", "execution_count": 40, "id": "d3ca9546", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["# Define a guide\n", "guide = autoguide.AutoMultivariateNormal(model)"]}, {"cell_type": "markdown", "id": "2234d8e9", "metadata": {"tags": ["learner", "md"]}, "source": ["Let's do the optimization again with this guide, draw some samples and plot them."]}, {"cell_type": "code", "execution_count": 40, "id": "020d8fdf", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["pyro.clear_param_store()\n", "pyro.set_rng_seed(SEED)\n", "\n", "# Set up the optimizer\n", "opt = Adam({'lr':0.01})\n", "\n", "# Set up the inference algorithm\n", "svi = SVI(model, guide, opt, loss=Trace_ELBO())\n", "\n", "# Number of stochastic gradient descent steps\n", "n_steps = 5000\n", "\n", "# Do gradient steps\n", "for step in tqdm(range(n_steps)):\n", "    loss = svi.step(torch.Tensor(zs[zs < z_max]), torch.Tensor(d[zs < z_max]), torch.Tensor(derr[zs < z_max]))"]}, {"cell_type": "code", "execution_count": 40, "id": "7612ef9f", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["predictive = Predictive(model, guide=guide, num_samples=5000, return_sites=(\"h0\", \"q\"))\n", "\n", "samples_svi = predictive(torch.Tensor(zs[zs < z_max]), torch.Tensor(d[zs < z_max]), torch.Tensor(derr[zs < z_max]))\n", "samples_svi = np.array([samples_svi['h0'].flatten().numpy() * 100, samples_svi['q'].flatten().numpy()]).T"]}, {"cell_type": "code", "execution_count": 40, "id": "f730928f", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["figure = corner.corner(samples_svi, labels=[r\"$H_0$\", r\"$q$\"])\n", "corner.corner(samples_emcee, fig=figure,weights=np.ones(len(samples_emcee)) / (len(samples_emcee) / len(samples_svi)), color=color_default);\n", "\n", "red_line = matplotlib.lines.Line2D([], [], color='firebrick', label='MCMC')\n", "black_line = matplotlib.lines.Line2D([], [], color='k', label='SVI')\n", "\n", "plt.legend(handles=[black_line, red_line], bbox_to_anchor=(0., 1.0, 1., .0), loc=4)"]}, {"cell_type": "markdown", "id": "a9baf78e", "metadata": {"tags": ["learner", "md"]}, "source": ["This time the joint posterior is much closer to the MCMC ones, and we can see the covariances between the two components in action."]}, {"cell_type": "markdown", "id": "3029991b", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_20_4'></a>     \n", "\n", "| [Top](#section_20_0) | [Restart Section](#section_20_4) | [Next Section](#section_20_5) |\n"]}, {"cell_type": "markdown", "id": "063bddfc", "metadata": {"tags": ["learner", "md", "catsoop_04"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 20.4.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 43, "id": "9714b251", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "14a36708", "metadata": {"tags": ["learner", "catsoop_04", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "b5b1275d", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_20_5'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L20.5 Modeling More Complex Distributions</h2>  \n", "\n", "| [Top](#section_20_0) | [Previous Section](#section_20_4) | [Exercises](#exercises_20_5) | [Next Section](#section_20_6) |\n"]}, {"cell_type": "markdown", "id": "c71e394d", "metadata": {"tags": ["learner", "md"]}, "source": ["<img src=\"https://lilianweng.github.io/lil-log/assets/images/normalizing-flow.png\" width=800/>"]}, {"cell_type": "markdown", "id": "160ac231", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>An Introduction to Normalizing Flows</h3>\n", "\n", "- Gaussians are nice and all, but we often want more flexibility to model complex distributions.\n", "- **Normalizing flows** are a class of models for modeling complicted distributions\n", "- They are based on having a sequence of **invertible, differentiable transformations with tractable Jacobians** (usually parameterized by neural networks) that transform (or deform) a simple distribution, usually a diagonal Gaussian to the complex distribution."]}, {"cell_type": "markdown", "id": "2f1d84f1", "metadata": {"tags": ["learner", "md"]}, "source": ["<img src=\"https://raw.githubusercontent.com/smsharma/smsharma/master/transformation.png\" alt=\"Transform\" width=\"640\"/>"]}, {"cell_type": "markdown", "id": "5aaf4d02", "metadata": {"tags": ["learner", "md"]}, "source": ["Suppose we have \n", "- Two sets of random variables $z, \\theta \\in \\mathbb R^d$ with corresponding probability densities $z\\sim p_z(z)$ and $\\theta\\sim p(\\theta)$ \n", "- A transformation $\\theta = T(z)$.\n", "\n", "The **change of variables theorem** relates the two probability densities:\n", "\n", "$$p(\\theta )=p_z\\left(T^{-1}(\\theta )\\right)\\left|\\operatorname{det} {J}_{T^{-1}}(\\theta )\\right|$$\n", "\n", "where ${J}_{T^{-1}}(\\theta)$ is the determinant of the Jacobian of the inverse transformation and quantifies local changes in \"volume\" during the transformation.\n", "\n", "$${J}_{T^{-1}}(\\theta)=\\left[\\begin{array}{ccc}\n", "\\frac{\\partial z_{1}(\\theta)}{\\partial \\theta_{1}} & \\cdots & \\frac{\\partial z_{1}(\\theta)}{\\partial \\theta_{d}} \\\\\n", "\\vdots & \\ddots & \\vdots \\\\\n", "\\frac{\\partial z_{d}(\\theta)}{\\partial \\theta_{1}} & \\cdots & \\frac{\\partial z_{d}(\\theta)}{\\partial \\theta_{d}}\n", "\\end{array}\\right]$$"]}, {"cell_type": "markdown", "id": "ce2b4e9a", "metadata": {"tags": ["learner", "md"]}, "source": ["Several flow transformation can be chained together.\n", "\n", "See, e.g., [this blog post](https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html) (where the schematic above is from) for details on normalizing flows, as well as different ways flows are constructed and parameterized."]}, {"cell_type": "markdown", "id": "70f7fd90", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Defining, sampling, and evaluating a flow</h3>"]}, {"cell_type": "code", "execution_count": 50, "id": "f77a5fe8", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "input_dim = 2 \n", "\n", "# Specifies the base distribution z ~ N(0,I) as a diagonal Gaussian\n", "dist_z = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))\n", "\n", "# Defines the Transformation \n", "T = pyro.distributions.transforms.affine_autoregressive(input_dim=input_dim, hidden_dims=[32, 32])\n", "\n", "# Creates a new distribution where the based distribution is transformed using the flow transformation `T`. \n", "dist_theta = dist.TransformedDistribution(dist_z, [T])\n"]}, {"cell_type": "markdown", "id": "1ef08370", "metadata": {"tags": ["learner", "md"]}, "source": ["We can compute the log-probability $\\log p(\\theta_0)$ of a new set of points $\\theta_0$ . Think of this as transforming the new set of points into the base distribution (the Gaussian) and computing the log-probability there."]}, {"cell_type": "code", "execution_count": 50, "id": "99cfd31c", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "theta = torch.Tensor([1.,1.])\n", "dist_theta.log_prob(theta)"]}, {"cell_type": "code", "execution_count": 50, "id": "f223e932", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "dist_theta.sample((8, input_dim))  # Draw 8 samples from the flow"]}, {"cell_type": "markdown", "id": "d934b913", "metadata": {"tags": ["learner", "md"]}, "source": ["The flow transformation, being parameterized by neural networks, has more parameters than our previous distributions:"]}, {"cell_type": "code", "execution_count": 50, "id": "2801e4c5", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "param_count = 0 \n", "for param in T.parameters():\n", "    param_count += len(param)\n", "    \n", "print(\"The flow transformation has {} parameters\".format(param_count))"]}, {"cell_type": "markdown", "id": "9f13e7e4", "metadata": {"tags": ["learner", "md"]}, "source": ["Let's try and train the normalizing flow we have defined to model a simple toy distribution---a 2-component Gaussian mixture in 2 dimensions."]}, {"cell_type": "code", "execution_count": 50, "id": "87a08819", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "from scipy.stats import multivariate_normal\n", "import seaborn as sns"]}, {"cell_type": "code", "execution_count": 50, "id": "e9c7e0d4", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "n_components = 2  # Number of mixture components\n", "weights_mixture = [0.3, 0.7]  # Relative weights of mixture components\n", "locs = [[1,1],[-1,-1]]  # Means of mixture components\n", "scales = [[0.1, 0.5] for _ in range(n_components)]  # stds of mixture components \n", "\n", "def simulator_gmm():\n", "    \"\"\" Draw samples from Gaussian mixture model\n", "    \"\"\"\n", "    idx_comp = np.random.choice(np.arange(n_components), p=weights_mixture)\n", "    return multivariate_normal(mean=locs[idx_comp], cov=scales[idx_comp]).rvs()"]}, {"cell_type": "code", "execution_count": 50, "id": "5255894a", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "# Sample from the GMM\n", "samples = np.array([simulator_gmm() for _ in range(10000)])"]}, {"cell_type": "code", "execution_count": 50, "id": "4f5a4aea", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(12,5))\n", "\n", "ax[0].scatter(*samples.T, alpha=0.3)\n", "ax[0].set_title(\"True samples\")\n", "\n", "sns.kdeplot(x=samples.T[0], y=samples.T[1], fill=True, levels=100, cmap='viridis', thresh=0, ax=ax[1])\n", "ax[1].set_title(\"True (target) density\")"]}, {"cell_type": "markdown", "id": "4b6c878a", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Training a normalizing flow</h3>\n", "\n", "Let's write a convenience function to train the parameter of the neural network"]}, {"cell_type": "code", "execution_count": 50, "id": "1e6167bb", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "def train_flow(dataset, params, batch_size=64, n_steps=5001, lr = 1e-3):\n", "    \"\"\" Convenience function for training the normalizing flow transformation\n", "    \"\"\"\n", "    dataset = torch.tensor(dataset)\n", "    optimizer = torch.optim.Adam(params, lr=lr)\n", "    \n", "    # Run specified number of optimization steps\n", "    for i_step in tqdm(range(n_steps)):\n", "        \n", "        optimizer.zero_grad()\n", "        \n", "        # Draw random batches and compute their log-probability\n", "        idxs = np.random.choice(dataset.shape[0], batch_size) \n", "        loss = -dist_theta.log_prob(dataset[idxs,:]).mean()\n", "        \n", "        loss.backward()\n", "        optimizer.step()\n", "        dist_theta.clear_cache()\n", "\n", "        if i_step % 200 == 0: print('Step: {}; Loss: {}'.format(i_step, loss.item()))"]}, {"cell_type": "code", "execution_count": 50, "id": "dff861cc", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "# Train the normalizing flow\n", "train_flow(torch.Tensor(samples), params = T.parameters(), n_steps=5000, batch_size=64)   "]}, {"cell_type": "code", "execution_count": 50, "id": "4318861e", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "# Draw 5000 samples from the base distribution as well as the normalizing flow\n", "samples_base = np.array([dist_z.sample().detach().numpy() for _ in range(5000)])\n", "samples_test = np.array([dist_theta.sample().detach().numpy() for _ in range(5000)])"]}, {"cell_type": "markdown", "id": "30bd6d3e", "metadata": {"tags": ["learner", "md"]}, "source": ["Let's plot the density of samples from our learned flow, as well as the base distribution and true target distribution."]}, {"cell_type": "code", "execution_count": 50, "id": "8806b65c", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "fig, ax = plt.subplots(ncols=3, nrows=1, figsize=(14,4))\n", "\n", "sns.kdeplot(x=samples_base.T[0], y=samples_base.T[1], fill=True, levels=100, cmap='viridis', thresh=0, ax=ax[0])\n", "ax[0].set_title(\"Base density\")\n", "\n", "sns.kdeplot(x=samples_test.T[0], y=samples_test.T[1], fill=True, levels=100, cmap='viridis', thresh=0, ax=ax[1])\n", "ax[1].set_title(\"Transformed density\")\n", "\n", "sns.kdeplot(x=samples.T[0], y=samples.T[1], fill=True, levels=100, cmap='viridis', thresh=0, ax=ax[2])\n", "ax[2].set_title(\"True (target) density\")\n", "\n", "[ax[i].set_xlim(-2.75, 2.75) for i in range(3)]\n", "[ax[i].set_ylim(-2.75, 2.75) for i in range(3)]"]}, {"cell_type": "markdown", "id": "921e573f", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Stacking together several flow</h3>"]}, {"cell_type": "code", "execution_count": 50, "id": "74b6f9e3", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "input_dim = 2 \n", "n_flows = 6  # Number of flow transformations to be chained\n", "\n", "# Base distribution\n", "dist_z = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))\n", "\n", "# Defines the Transformation \n", "T = [pyro.distributions.transforms.affine_autoregressive(input_dim=input_dim, hidden_dims=[32, 32]) for _ in range(n_flows)]\n", "\n", "# Creates a distribution object which is Z transformed with the transformation T \n", "dist_theta = dist.TransformedDistribution(dist_z, T)"]}, {"cell_type": "code", "execution_count": 50, "id": "9205e5e5", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "params = []\n", "param_count = 0\n", "\n", "for T_i in T:\n", "    params += list(T_i.parameters())\n", "    \n", "    for param in T_i.parameters():\n", "        param_count += len(param)\n", "    \n", "print(\"The flow transformation has {} parameters\".format(param_count))"]}, {"cell_type": "markdown", "id": "fe3d4110", "metadata": {"tags": ["learner", "md"]}, "source": ["This time we have still more learnable parameters."]}, {"cell_type": "code", "execution_count": 50, "id": "d93f0998", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "train_flow(torch.Tensor(samples), params=params, n_steps=5000, batch_size=64)   "]}, {"cell_type": "code", "execution_count": 50, "id": "cab319aa", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "samples_test = np.array([dist_theta.sample().detach().numpy() for _ in range(5000)])"]}, {"cell_type": "code", "execution_count": 50, "id": "88b9a377", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "fig, ax = plt.subplots(ncols=3, nrows=1, figsize=(14,4))\n", "\n", "sns.kdeplot(x=samples_base.T[0], y=samples_base.T[1], fill=True, levels=100, cmap='viridis', thresh=0, ax=ax[0])\n", "ax[0].set_title(\"Base density\")\n", "\n", "sns.kdeplot(x=samples_test.T[0], y=samples_test.T[1], fill=True, levels=100, cmap='viridis', thresh=0, ax=ax[1])\n", "ax[1].set_title(\"Transformed density\")\n", "\n", "sns.kdeplot(x=samples.T[0], y=samples.T[1], fill=True, levels=100, cmap='viridis', thresh=0, ax=ax[2])\n", "ax[2].set_title(\"True (target) density\")\n", "\n", "[ax[i].set_xlim(-2.75, 2.75) for i in range(3)]\n", "[ax[i].set_ylim(-2.75, 2.75) for i in range(3)]"]}, {"cell_type": "markdown", "id": "2f840a8e", "metadata": {"tags": ["learner", "md"]}, "source": ["The transformed distribution matches the target density much better now!\n", "\n", "For fun, we can plot the density returned by the 6 individual transformation in order to see how the probability density _flows_ from the base distribution to the target distribution."]}, {"cell_type": "code", "execution_count": 50, "id": "639934fd", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "fig, ax = plt.subplots(nrows=1, ncols=n_flows + 1, figsize=(26,4))\n", "\n", "for i in range(n_flows + 1):\n", "    \n", "    dist_test = dist.TransformedDistribution(dist_z, [T[j] for j in range(i)])\n", "\n", "    samples_test = np.array([dist_test.sample().detach().numpy() for _ in range(5000)])\n", "    sns.kdeplot(x=samples_test.T[0], y=samples_test.T[1], fill=True, levels=100, cmap='viridis', thresh=0, ax=ax[i])\n", "    \n", "    if i == 0:\n", "        ax[i].set_title(\"Base density\")\n", "    else:\n", "        ax[i].set_title(r\"$T_{} \\circ\\ldots$\".format(i))\n", "\n", "[ax[i].set_xlim(-2.75, 2.75) for i in range(n_flows + 1)]\n", "[ax[i].set_ylim(-2.75, 2.75) for i in range(n_flows + 1)]"]}, {"cell_type": "markdown", "id": "08c0a0e0", "metadata": {"tags": ["learner", "md"]}, "source": ["In `Pyro`, the variational distribution can be set to a particular type of normalizing flow (an _inverse autoregressive flow_, IAF) by leveraging the Autoguide functionality:"]}, {"cell_type": "code", "execution_count": 50, "id": "7eef733f", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "guide = autoguide.AutoIAFNormal(model)"]}, {"cell_type": "markdown", "id": "a34eba50", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_20_5'></a>     \n", "\n", "| [Top](#section_20_0) | [Restart Section](#section_20_5) | [Next Section](#section_20_6) |\n"]}, {"cell_type": "markdown", "id": "ba1bff4f", "metadata": {"tags": ["learner", "md", "catsoop_05"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 20.5.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 53, "id": "bb2446d0", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "bc2166d9", "metadata": {"tags": ["learner", "catsoop_05", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "5c2fc0be", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_20_6'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L20.6 Some Real-World Applications</h2>  \n", "\n", "| [Top](#section_20_0) | [Previous Section](#section_20_5) | [Exercises](#exercises_20_6) | [Next Section](#section_20_7) |\n"]}, {"cell_type": "markdown", "id": "43a9bbf0", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "The example we've been working with (Hubble inference) is fairly straightforward---there isn't much of an advantage to taking a probabilistic programming approach, other than exposing the underlying philosophy."]}, {"cell_type": "markdown", "id": "c761761f", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>A hierarchical approach to Hubble constant inference</h3>\n", "\n", "- Real-work scientific analyses and forward models can be quite a bit more complex---take a look at the PGM below, from this nice paper (https://arxiv.org/abs/1707.00007) which also aims to measure the Hubble constant (you can see it in the top right part of the plot) using supernova and other observations, but in a much move involved manner. \n", "- In a case like this, a probabilistic description of the model can make analyses significantly more pleasant."]}, {"cell_type": "markdown", "id": "fc38145a", "metadata": {"tags": ["learner", "md"]}, "source": ["<img src=\"https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/mnras/476/3/10.1093_mnras_sty418/1/sty418fig3.jpeg?Expires=1645818239&Signature=rdscjqUw36B1HYnu7IvpqBgUdJYnkeZIkiUikbjxqb9JlQOB28aPE-dJN6-KC7T7Qo3WCwsfPuF0CkB90b5CVfcXkDj2kqEwJjMzee5BxYC-ma2T9x4GZuxx1u3MeXp8dJGChLuwji3yS937UbJhdgTyZ~rkzl4cSwYcvHL4r~Zb33Xe1D3s3ptvNiaIzLIDfYThOpDmZxQmwhpV7Ad9EwOsA-EqvFTYU1hx5WSzsdLG1H0fL4ksOABcn3MAEpjlVXOSAbq4-Z-Gy8iC3Jg98SqkfStcNWp2BPTQcGq-wznn9EVEdRirl5ougdI0nOVUG6~GjZ2YkrlGzeZGC2Oyiw__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA\" width=1024/>"]}, {"cell_type": "markdown", "id": "2cda7a55", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Optimizing detector design</h3>\n", "\n", "- We have focused on using probabilistic programming for parameter estimation. However, by being able to define a scientific workflow in terms in a composable way, we can optimize many other parts of the chain.\n", "- A particular application of interest is **optimizing parameters of a detector to maximized performance (for a given scientific goal) subject to constraints (for example, a fixed budget)**\n", "- Example below via the MODE collaboration (https://mode-collaboration.github.io/docs/mode_npni_preprint.pdf)"]}, {"cell_type": "markdown", "id": "a4f66d15", "metadata": {"tags": ["learner", "md"]}, "source": ["<img src=\"https://gilesstrong.github.io/website/images/posts/2021-09-10-Differentiable-Programming-and-MODE/media/image2.png\" width=640/>"]}, {"cell_type": "markdown", "id": "fcde3f03", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_20_6'></a>     \n", "\n", "| [Top](#section_20_0) | [Restart Section](#section_20_6) | [Next Section](#section_20_7) |\n"]}, {"cell_type": "markdown", "id": "9c236f1c", "metadata": {"tags": ["learner", "md", "catsoop_06"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 20.6.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 63, "id": "d0455a94", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "e7ec87fa", "metadata": {"tags": ["learner", "catsoop_06", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "5dc326ab", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_20_7'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L20.7 Discussion Part I</h2>  \n", "\n", "| [Top](#section_20_0) | [Previous Section](#section_20_6) | [Exercises](#exercises_20_7) | [Next Section](#section_20_8) |\n"]}, {"cell_type": "markdown", "id": "3592b8e1", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "TEXT\n"]}, {"cell_type": "code", "execution_count": 70, "id": "faecea93", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "#code\n"]}, {"cell_type": "markdown", "id": "52713941", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_20_7'></a>     \n", "\n", "| [Top](#section_20_0) | [Restart Section](#section_20_7) | [Next Section](#section_20_8) |\n"]}, {"cell_type": "markdown", "id": "1a759706", "metadata": {"tags": ["learner", "md", "catsoop_07"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 20.7.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 73, "id": "89e5fe75", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "8a60932e", "metadata": {"tags": ["learner", "catsoop_07", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "e54e90a1", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_20_8'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L20.8 Discussion Part II</h2>     \n", "\n", "| [Top](#section_20_0) | [Previous Section](#section_20_7) | [Exercises](#exercises_20_8) |\n"]}, {"cell_type": "markdown", "id": "b38ce337", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "TEXT\n"]}, {"cell_type": "code", "execution_count": 80, "id": "b82269d7", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "#code\n"]}, {"cell_type": "markdown", "id": "df0e9ee5", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_20_8'></a>   \n", "\n", "| [Top](#section_20_0) | [Restart Section](#section_20_8) |\n"]}, {"cell_type": "markdown", "id": "8daccf53", "metadata": {"tags": ["learner", "md", "catsoop_08"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 20.8.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 83, "id": "6ef3a149", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "73475bf5", "metadata": {"tags": ["learner", "catsoop_08", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}