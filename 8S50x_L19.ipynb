{"cells": [{"cell_type": "markdown", "id": "1ee373c5", "metadata": {"tags": ["learner", "md"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 19: Deep Learning Regression Strategies</h1>\n"]}, {"cell_type": "markdown", "id": "41345f3a", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_19_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L19.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "fce9b388", "metadata": {"tags": ["learner", "md"]}, "source": ["<table style=\"width:100%\">\n", "    <colgroup>\n", "       <col span=\"1\" style=\"width: 40%;\">\n", "       <col span=\"1\" style=\"width: 15%;\">\n", "       <col span=\"1\" style=\"width: 45%;\">\n", "    </colgroup>\n", "    <tr>\n", "        <th style=\"text-align: left; font-size: 13pt;\">Section</th>\n", "        <th style=\"text-align: left; font-size: 13pt;\">Exercises</th>\n", "        <th style=\"text-align: left; font-size: 13pt;\">Summary</th>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_19_1\">L19.1 Discovering the Higgs with Deep Learning</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_19_1\">L19.1 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_19_2\">L19.2 Revisiting Deep Learning</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_19_2\">L19.2 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_19_3\">L19.3 An Example with PyTorch</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_19_3\">L19.3 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_19_4\">L19.4 Another Example: Part I</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_19_4\">L19.4 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_19_5\">L19.5 Another Example: Part II</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_19_5\">L19.5 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_19_6\">L19.6 The Tau Lepton: Part I</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_19_6\">L19.6 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_19_7\">L19.7 The Tau Lepton: Part II</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_19_7\">L19.7 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_19_8\">L19.8 The Tau Lepton: Part III</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_19_8\">L19.8 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_19_9\">L19.9 The Tau Lepton: Part IV</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_19_9\">L19.9 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "</table>\n", "\n"]}, {"cell_type": "markdown", "id": "467638f0", "metadata": {"tags": ["learner", "catsoop_00", "md"]}, "source": ["<h3>Learning Objectives</h3>\n", "\n", "It is often the case that you want to fit a function to data. However, sometimes you want to do this in many dimensions and you can't visualize it all. In this lecture, we are going to look at deep learning regression. This can help you to model these complex scenarios. \n", "\n", "Let's say you have some data $\\vec{x}$, this can be an n-dimensional set of inputs, and you want to predict an output $\\vec{y}$ from $\\vec{x}$ where $y$ canbe m-dimensional set of outputs. What we want then is to create a function\n", "\n", "\\begin{equation}\n", " \\vec{y} = f(\\vec{x})\n", "\\end{equation}\n", "\n", "Now in the past what we did was to do this for a one dimensional $y$ taking in a set of inputs $\\vec{x}$. In this lecture, we aim to generalize this to predict an arbitrary number with an arbitrary set of inputs. In particular, we will explore the following learning objectives:\n", "\n", " - Fitting an arbitrary 1D dataset with a neural net\n", " - Deep Learning algorithm Design\n", " - Higgs to Tau leptons\n", " - Observing the improvements\n", " - Optimized Target\n", " - The full mass regression\n", " - NN Architecture\n"]}, {"cell_type": "markdown", "id": "21325e06", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Installing Tools</h3>\n", "\n", "Before we do anything, lets make sure we install the tools we need for this."]}, {"cell_type": "code", "execution_count": 7, "id": "d59f3ad0", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "!pip install torch\n", "!pip install imageio"]}, {"cell_type": "markdown", "id": "dff0575f", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Importing Libraries</h3>\n", "\n", "Before beginning, run the cell below to import the relevant libraries for this notebook. \n", "Optionally, set the plot resolution and default figure size."]}, {"cell_type": "code", "execution_count": 7, "id": "0f3a0154", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "import torch\n", "import torch.nn as nn\n", "from torch.autograd import Variable\n", "import torch.nn.functional as F\n", "import torch.utils.data as Data\n", "\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "\n", "import numpy as np\n", "import imageio\n", "\n", "#set plot resolution\n", "#%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure size\n", "#plt.rcParams['figure.figsize'] = (9,6)\n"]}, {"cell_type": "markdown", "id": "1f186cb0", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_19_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L19.1 Discovering the Higgs with Deep Learning</h2>  \n", "\n", "| [Top](#section_19_0) | [Previous Section](#section_19_0) | [Exercises](#exercises_19_1) | [Next Section](#section_19_2) |\n"]}, {"cell_type": "markdown", "id": "990af1fa", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section."]}, {"cell_type": "code", "execution_count": 1, "id": "9d3798ed", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"data": {"text/html": ["\n", "        <iframe\n", "            width=\"970\"\n", "            height=\"550\"\n", "            src=\"https://mitx-8s50.github.io/slides/L19/slides_L19_01.html\"\n", "            frameborder=\"0\"\n", "            allowfullscreen\n", "            \n", "        ></iframe>\n", "        "], "text/plain": ["<IPython.lib.display.IFrame at 0x10db4d8e0>"]}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": ["#>>>RUN\n", "\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L19/slides_L19_01.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "491c85e8", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_19_1'></a>     \n", "\n", "| [Top](#section_19_0) | [Restart Section](#section_19_1) | [Next Section](#section_19_2) |\n"]}, {"cell_type": "markdown", "id": "eb71daaa", "metadata": {"tags": ["learner", "md", "catsoop_01"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 19.1.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 13, "id": "f14f5233", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "a12c7455", "metadata": {"tags": ["learner", "catsoop_01", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "e446de2e", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_19_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L19.2 Revisiting Deep Learning</h2>  \n", "\n", "| [Top](#section_19_0) | [Previous Section](#section_19_1) | [Exercises](#exercises_19_2) | [Next Section](#section_19_3) |\n"]}, {"cell_type": "markdown", "id": "7e429584", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section."]}, {"cell_type": "code", "execution_count": 5, "id": "9fef113b", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"data": {"text/html": ["\n", "        <iframe\n", "            width=\"970\"\n", "            height=\"550\"\n", "            src=\"images/slides_L19_02.html\"\n", "            frameborder=\"0\"\n", "            allowfullscreen\n", "            \n", "        ></iframe>\n", "        "], "text/plain": ["<IPython.lib.display.IFrame at 0x1115a2550>"]}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": ["#>>>RUN\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L19/slides_L19_02.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "a7430bef", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Fitting an arbitrary 1D dataset with a neural net</h3>\n", "\n", "What we would like to do is a fit a distribution without an initial choice fo a function. To envision this distribution, lets create a guassian function that is smeared out a little bit. From that, we can try to fit this dataset to get a functional form for this. "]}, {"cell_type": "code", "execution_count": null, "id": "83bf6d2b", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#>>>RUN\n", "\n", "#Lets Try GP on a gaussian random set of points\n", "def gaussian(mu,sigma,norm,offset):\n", "    \"\"\"Returns a gaussian function with the given parameters\"\"\"\n", "    return lambda x: norm*np.exp(-(1./2.)*((x-mu)/(sigma))**2)+offset\n", "\n", "Xin = np.mgrid[0:201] # points from 0 to 201\n", "data = gaussian(100., 20., 10., 5)(Xin) + 5*np.random.random(Xin.shape) # Guassian + semaring\n", "plt.plot(data,\"*\")"]}, {"cell_type": "markdown", "id": "5d340246", "metadata": {"tags": ["learner", "md"]}, "source": ["Now one way to model this distribution is through a Gaussian process. This will try to fit very many gaaussians that will allow us to extract a function. "]}, {"cell_type": "code", "execution_count": null, "id": "9f394406", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#>>>RUN\n", "\n", "#now lets run GP on this guy\n", "import george\n", "from george import kernels\n", "kernel = np.var(data) * kernels.ExpSquaredKernel(2.5)\n", "gp = george.GP(kernel)\n", "var=np.ones(len(Xin))\n", "gp.compute(Xin,var)\n", "x_pred = np.linspace(0, 200, 100)\n", "pred, pred_var = gp.predict(data, x_pred, return_var=True)\n", "\n", "plt.fill_between(x_pred, pred - np.sqrt(pred_var), pred + np.sqrt(pred_var),color=\"k\", alpha=0.2)\n", "plt.plot(x_pred, pred, \"k\", lw=1.5, alpha=0.5)\n", "plt.plot(Xin,data,\"*\")\n", "plt.xlabel(\"x\")\n", "plt.ylabel(\"y\");"]}, {"cell_type": "markdown", "id": "5492c50c", "metadata": {"tags": ["learner", "md"]}, "source": ["There are lots of flucations here. Clearly, the function is not ideal, and with a bit of tuning, we could probably approximate this. However, another way we can do this is with a neural network. "]}, {"cell_type": "markdown", "id": "9aeb48c9", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_19_2'></a>     \n", "\n", "| [Top](#section_19_0) | [Restart Section](#section_19_2) | [Next Section](#section_19_3) |\n"]}, {"cell_type": "markdown", "id": "9ec4af2f", "metadata": {"tags": ["learner", "md", "catsoop_02"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 19.2.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 23, "id": "81907601", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "a505e931", "metadata": {"tags": ["learner", "catsoop_02", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "1927751b", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_19_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L19.3 An Example with PyTorch</h2>  \n", "\n", "| [Top](#section_19_0) | [Previous Section](#section_19_2) | [Exercises](#exercises_19_3) | [Next Section](#section_19_4) |\n"]}, {"cell_type": "markdown", "id": "d09219b7", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "In the past setup for classification, we defined a loss function as: \n", "\\begin{equation}\n", "\\mathcal{L} = y \\log(f(x)) + (1-y)\\log(1-f(x))\n", "\\end{equation}\n", "where here $f(x)$ is our classifier with 1 being a high likelihood of a signal, with value $0$, and $1$ for the classifier having a high likelihood of a background. Note the goal here is to minimize the loss, so we want to make sure that f(x) and 1-f(x) are orthongal.  \n", "\n", "When we are running a deep learning algorithm, what we then are effectively doing is to to minimize the parameters: \n", "\\begin{equation}\n", "\\frac{\\partial \\mathcal{L}}{\\partial w_{i}}\\rightarrow 0\n", "\\end{equation}\n", "\n", "Now the power of deep learning is that we can fit any arbiratry function in the data. \n", "\n", "Let's start with a simplified dataset. "]}, {"cell_type": "code", "execution_count": 30, "id": "f60bceb6", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "torch.manual_seed(1)    # reproducible\n", "\n", "x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)  # x data (tensor), shape=(100, 1)\n", "y = x.pow(2) + 0.2*torch.rand(x.size())                 # noisy y data (tensor), shape=(100, 1)\n", "\n", "# torch can only train on Variable, so convert them to Variable\n", "x, y = Variable(x), Variable(y)\n", "\n", "# view data\n", "plt.figure(figsize=(10,4))\n", "plt.scatter(x.data.numpy(), y.data.numpy(), color = \"orange\")\n", "plt.title('Regression Analysis')\n", "plt.xlabel('Independent varible')\n", "plt.ylabel('Dependent varible')\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "2448a675", "metadata": {"tags": ["learner", "md"]}, "source": ["Now fit this functtion, we can imagine definig a new loss that we can minimize to fit this function. In this case, we will define a loss known as mean squared error (MSE) loss. This loss can be written as \n", "\\begin{equation}\n", "\\mathcal{L} = \\left(y-f(x)\\right)^{2}\n", "\\end{equation}\n", "More generally, we can write this over $N$ variables as \n", "\n", "\\begin{equation}\n", "\\mathcal{L} = \\frac{1}{N}\\sum_{i=1}^{N}\\left(y_{i}-f(x_{i})\\right)^{2}\n", "\\end{equation}\n", "\n", "There are severval varations on this loss, perhaps the other most commone one is known as mean absolute percentage error (MAPE). This loss is defined as \n", "\n", "\\begin{equation}\n", "\\mathcal{L} = \\frac{100\\%}{N}\\sum_{i=1}^{N}\\frac{|y_{i}-f(x_{i})|}{y_{i}}\n", "\\end{equation}\n", "\n", "Let's fit a function with deep learning regression to do this, we are going train a two layered dense network to predict this function. \n", "\n", "Our one layered model can be written as \n", "\\begin{equation}\n", " f(x) = W^{T}_{2}\\left(\\rm{Act}\\left(\\vec{W_{1}}x + \\vec{b}\\right)+b\\right)\n", "\\end{equation}\n", "\n", "Where $W_{1}$ and $W_{2}$ are matrices, well really vectors since the output dimension is 1 and the input dimension is 1. With these matrices, we can specify the number of hidden parameters, which will be the size of the alternative dimension. For 10 hidden parameters, $W_{i}$ will be a 10x1 matrix( 10 dimensional vector). $\\rm{Act}$ here is the activation function for this system. \n", "\n", "Lets go ahead and build the neural network with 10 hidden parameters, mean squared loss. Additionally, for the minimizer, we will use stochastic graident descent (SGD). "]}, {"cell_type": "code", "execution_count": 30, "id": "ac9b50bd", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "# this is one way to define a network\n", "class Net(torch.nn.Module):\n", "    def __init__(self, n_feature, n_hidden, n_output):\n", "        super(Net, self).__init__()\n", "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n", "        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n", "\n", "    def forward(self, x):\n", "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n", "        x = self.predict(x)             # linear output\n", "        return x\n", "\n", "net = Net(n_feature=1, n_hidden=20, n_output=1)     # define the network\n", "print(net)  # net architecture\n", "optimizer = torch.optim.SGD(net.parameters(), lr=0.2)#stochastic Gradient Descent\n", "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss"]}, {"cell_type": "markdown", "id": "50b9ee8c", "metadata": {"tags": ["learner", "md"]}, "source": ["Ok, now that we have our neural network, what we are going to do is run 200 epochs of the network training, and for each epcoh, we are going to make a plot. Then we will turn this plot into a moving video to see how the network is running. "]}, {"cell_type": "code", "execution_count": 30, "id": "29052969", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["def makePlot(x,y,prediction,ax,fig,images,t,loss):\n", "    # plot and show learning process\n", "    plt.cla()\n", "    ax.set_title('Regression Analysis', fontsize=35)\n", "    ax.set_xlabel('Independent variable', fontsize=24)\n", "    ax.set_ylabel('Dependent variable', fontsize=24)\n", "    ax.set_ylim(0,20)\n", "    ax.set_ylim(0,200)\n", "    ax.scatter(x.data.numpy(), y.data.numpy(), color = \"orange\")\n", "    ax.plot(x.data.numpy(), prediction.data.numpy(), 'g-', lw=3)\n", "    ax.text(0.6, 0.7, 'Epoch = %d' % t, fontdict={'size': 24, 'color':  'red'})\n", "    ax.text(0.6, 0.3, 'Loss = %.4f' % loss.data.numpy(),fontdict={'size': 24, 'color':  'red'}) \n", "    fig.canvas.draw()       # draw the canvas, cache the renderer\n", "    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n", "    image  = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n", "    images.append(image)\n", "\n", "def train(x,y,net,loss_func,opt,nepochs):\n", "    images = []\n", "    fig, ax = plt.subplots(figsize=(12,7))\n", "    for epoch in range(nepochs):\n", "        if epoch % 50 == 0: \n", "            print(\"epoch:\",epoch)\n", "        prediction = net(x)\n", "        loss = loss_func(prediction, y) \n", "        opt.zero_grad()\n", "        loss.backward() \n", "        optimizer.step()\n", "        if epoch % 4 == 0:\n", "            makePlot(x,y,prediction,ax,fig,images,epoch,loss)\n", "    return images\n", "    \n", "\n", "images=train(x,y,net,loss_func,optimizer,200)\n", "imageio.mimsave('./curve_1.gif', images, fps=10)"]}, {"cell_type": "markdown", "id": "d69b1c75", "metadata": {"tags": ["learner", "md"]}, "source": ["Ok, this is actually a great way to play around with models. For a quick look at how things change, try doubling the number of hidden parameters. "]}, {"cell_type": "markdown", "id": "0c9f00b3", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_19_3'></a>     \n", "\n", "| [Top](#section_19_0) | [Restart Section](#section_19_3) | [Next Section](#section_19_4) |\n"]}, {"cell_type": "markdown", "id": "64d0bc45", "metadata": {"tags": ["learner", "md", "catsoop_03"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 19.3.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 33, "id": "b3390197", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "a1324126", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "be2f50d3", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_19_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L19.4 Another Example: Part I</h2>  \n", "\n", "| [Top](#section_19_0) | [Previous Section](#section_19_3) | [Exercises](#exercises_19_4) | [Next Section](#section_19_5) |\n"]}, {"cell_type": "markdown", "id": "aa1a3b10", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "Ok, now lets look at another function to regress, lets try to regress \n", "\n", "\\begin{equation}\n", " f(x) = \\sin(x)\n", "\\end{equation}"]}, {"cell_type": "code", "execution_count": 40, "id": "c6069574", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "torch.manual_seed(1)    # reproducible\n", "x = torch.unsqueeze(torch.linspace(-10, 10, 1000), dim=1)  # x data (tensor), shape=(100, 1)\n", "y = torch.sin(x) + 0.2*torch.rand(x.size())                 # noisy y data (tensor), shape=(100, 1)\n", "\n", "# torch can only train on Variable, so convert them to Variable\n", "x, y = Variable(x), Variable(y)\n", "plt.figure(figsize=(10,4))\n", "plt.scatter(x.data.numpy(), y.data.numpy(), color = \"blue\")\n", "plt.title('Regression Analysis')\n", "plt.xlabel('Independent varible')\n", "plt.ylabel('Dependent varible')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "2bcc6fd8", "metadata": {"tags": ["learner", "md"]}, "source": ["### Challenge question\n", "\n", "Make a regression for the above dataset? How does this regression change with the number of paramters, say 100 hidden parameters?"]}, {"cell_type": "code", "execution_count": 40, "id": "97c70dd4", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["net = Net(n_feature=1, n_hidden=100, n_output=1)     # define the network\n", "images=train(x,y,net,loss_func,optimizer,200)\n", "imageio.mimsave('./curve_2.gif', images, fps=10)"]}, {"cell_type": "code", "execution_count": 40, "id": "d0ceaf00", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#redefine network\n", "net = Net(n_feature=1, n_hidden=100, n_output=1)     # define the network\n", "images=train(x,y,net,loss_func,optimizer,200)\n", "imageio.mimsave('./curve_2.gif', images, fps=10)"]}, {"cell_type": "markdown", "id": "cb7a8563", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Deep Learning algorithm Design</h3>\n", "\n", "Now to build some intutition of how neutral networks work. Let's take the above problem and see if we can really explin its behavior doing some deep learning R&D. To do that, lets first take an architecture similar to the last one, except with more than 2 layers. \n", "\n", "Let's start with a 3 layer network with 100 hidden parameters, and then 50 hidden parameters in the second layer.  "]}, {"cell_type": "code", "execution_count": 40, "id": "298a29ea", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["# another way to define a network\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, 100),\n", "        torch.nn.Linear(100, 50),\n", "        torch.nn.Linear(50, 1),\n", "    )\n", "print(net[0].weight[0:10])\n", "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n", "loss_func = torch.nn.MSELoss()\n", "images=train(x,y,net,loss_func,optimizer,200)\n", "imageio.mimsave('./curve_2.gif', images, fps=10)"]}, {"cell_type": "markdown", "id": "274d9e1b", "metadata": {"tags": ["learner", "md"]}, "source": ["Why did it just fit a line to this? \n", "\n", "The answer is a bit sublte, but the network is lacking \"expressiveness\" to solve this problem. A linear layer is just a matrix multiplication, what that means is that we can just multiply by our initial input by a constant and add an offset $ax+b$ with each layer. Now its true that $a$ can be a matrix. However that is just giving a vector of linear outputs.\n", "\n", "To fix this, in the next section we will add in an \"activation\" layer between them. "]}, {"cell_type": "markdown", "id": "e0f21fb0", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_19_4'></a>     \n", "\n", "| [Top](#section_19_0) | [Restart Section](#section_19_4) | [Next Section](#section_19_5) |\n"]}, {"cell_type": "markdown", "id": "086d2035", "metadata": {"tags": ["learner", "md", "catsoop_04"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 19.4.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 43, "id": "e27cf311", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "b5ff9563", "metadata": {"tags": ["learner", "catsoop_04", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "c3c9cac5", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_19_5'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L19.5 Another Example: Part II</h2>  \n", "\n", "| [Top](#section_19_0) | [Previous Section](#section_19_4) | [Exercises](#exercises_19_5) | [Next Section](#section_19_6) |\n"]}, {"cell_type": "markdown", "id": "c4a6f6f1", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "TEXT\n"]}, {"cell_type": "code", "execution_count": 50, "id": "437687d2", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "# another way to define a network\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, 100),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(100, 50),\n", "        torch.nn.Linear(50, 1),\n", "    )\n", "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n", "loss_func = torch.nn.MSELoss()\n", "images=train(x,y,net,loss_func,optimizer,200)\n", "imageio.mimsave('./curve_2.gif', images, fps=10)"]}, {"cell_type": "markdown", "id": "4d490f65", "metadata": {"tags": ["learner", "md"]}, "source": ["And finally, lets put a few activation layers"]}, {"cell_type": "code", "execution_count": 50, "id": "76a868ed", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "# another way to define a network\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, 100),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(100, 50),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(50, 1),\n", "    )\n", "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n", "loss_func = torch.nn.MSELoss()\n", "images=train(x,y,net,loss_func,optimizer,400)\n", "imageio.mimsave('./curve_2.gif', images, fps=10)"]}, {"cell_type": "markdown", "id": "ac9f3c52", "metadata": {"tags": ["learner", "md"]}, "source": ["### Challenge Question\n", "\n", "Given the observed trends, add more layers to your network. How many layers are needed to describe the whole oscillation? \n"]}, {"cell_type": "code", "execution_count": 50, "id": "3002338f", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, 100),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(100, 50),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(50, 50),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(50, 1),\n", "    )\n", "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n", "loss_func = torch.nn.MSELoss()\n", "images=train(x,y,net,loss_func,optimizer,400)\n", "imageio.mimsave('./curve_2.gif', images, fps=10)"]}, {"cell_type": "code", "execution_count": 50, "id": "6fd24f06", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["# answer\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, 100),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(100, 100),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(100, 100),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(100, 50),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(50, 1),\n", "    )\n", "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n", "images=train(x,y,net,loss_func,optimizer,1000)\n", "imageio.mimsave('./curve_2_c.gif', images, fps=10)"]}, {"cell_type": "markdown", "id": "5a56b43c", "metadata": {"tags": ["learner", "md"]}, "source": ["Now that we have all of this working, what would be the right architecture to perform a Gaussian regression? \n", "\n", "Lets setup the data using the torch tools.\n"]}, {"cell_type": "code", "execution_count": 50, "id": "9da1e90e", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["torch.manual_seed(1)    # reproducible\n", "\n", "x=torch.unsqueeze(torch.linspace(0, 200, 201), dim=1) \n", "y=torch.from_numpy(data.reshape(len(data),1).astype('float32'))\n", "\n", "# torch can only train on Variable, so convert them to Variable\n", "x, y = Variable(x), Variable(y)\n", "\n", "# view data\n", "plt.figure(figsize=(10,4))\n", "plt.scatter(x.data.numpy(), y.data.numpy(), color = \"orange\")\n", "plt.title('Regression Analysis')\n", "plt.xlabel('Independent varible')\n", "plt.ylabel('Dependent varible')\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "a3cf7c10", "metadata": {"tags": ["learner", "md"]}, "source": ["Since we have a gaussian, there are 3 inflection points, a 4 layer network with not so many parameters should do the trick. Lets try!"]}, {"cell_type": "code", "execution_count": 50, "id": "7186c0e6", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["# answer\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, 10),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(10, 10),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(10, 10),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(10, 1),\n", "    )\n", "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n", "images=train(x,y,net,loss_func,optimizer,300)\n", "imageio.mimsave('./curve_gaus.gif', images, fps=10)"]}, {"cell_type": "code", "execution_count": 50, "id": "5b2a149b", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["n_hidden=10\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, n_hidden),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(n_hidden, n_hidden),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(n_hidden, n_hidden),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(n_hidden, 1),\n", "    )\n", "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n", "images=train(x,y,net,loss_func,optimizer,1000)\n", "imageio.mimsave('./curve_gaus.gif', images, fps=10)"]}, {"cell_type": "markdown", "id": "e87e7c02", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_19_5'></a>     \n", "\n", "| [Top](#section_19_0) | [Restart Section](#section_19_5) | [Next Section](#section_19_6) |\n"]}, {"cell_type": "markdown", "id": "e570f505", "metadata": {"tags": ["learner", "md", "catsoop_05"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 19.5.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 53, "id": "f6a7bb49", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "bd1e1c0a", "metadata": {"tags": ["learner", "catsoop_05", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "e198f21b", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_19_6'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L19.6 The Tau Lepton: Part I</h2>  \n", "\n", "| [Top](#section_19_0) | [Previous Section](#section_19_5) | [Exercises](#exercises_19_6) | [Next Section](#section_19_7) |\n"]}, {"cell_type": "markdown", "id": "4b051e7e", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section."]}, {"cell_type": "code", "execution_count": 6, "id": "0bcbe36c", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"data": {"text/html": ["\n", "        <iframe\n", "            width=\"970\"\n", "            height=\"550\"\n", "            src=\"images/slides_L19_06.html\"\n", "            frameborder=\"0\"\n", "            allowfullscreen\n", "            \n", "        ></iframe>\n", "        "], "text/plain": ["<IPython.lib.display.IFrame at 0x1115a2fd0>"]}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": ["#>>>RUN\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L19/slides_L19_06.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "d6ad3f5d", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_19_6'></a>     \n", "\n", "| [Top](#section_19_0) | [Restart Section](#section_19_6) | [Next Section](#section_19_7) |\n"]}, {"cell_type": "markdown", "id": "24b1aca7", "metadata": {"tags": ["learner", "md", "catsoop_06"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 19.6.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 63, "id": "87f9a153", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "93b3f17b", "metadata": {"tags": ["learner", "catsoop_06", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "ba8bca06", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_19_7'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L19.7 The Tau Lepton: Part II</h2>  \n", "\n", "| [Top](#section_19_0) | [Previous Section](#section_19_6) | [Exercises](#exercises_19_7) | [Next Section](#section_19_8) |\n"]}, {"cell_type": "markdown", "id": "aefc9099", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section."]}, {"cell_type": "code", "execution_count": 7, "id": "119ca253", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"data": {"text/html": ["\n", "        <iframe\n", "            width=\"970\"\n", "            height=\"550\"\n", "            src=\"images/slides_L19_07.html\"\n", "            frameborder=\"0\"\n", "            allowfullscreen\n", "            \n", "        ></iframe>\n", "        "], "text/plain": ["<IPython.lib.display.IFrame at 0x1115a2eb0>"]}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": ["#>>>RUN\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L19/slides_L19_07.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "c2f26aa9", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Higgs to Tau leptons</h3>\n", "\n", "In the following section, we are going to use deep learning regression to solve an important problem in Higgs identification and reconstruction. In particular, what we are going to study is the Higgs boson decay to $\\tau$ leptons. \n", "\n", "The Higgs boson decays to many different objects. The most likely object that the Higgs boson decays to are b-quarks. The 4th most likely components that the Higgs boson decays to are the $\\tau$ leptons. Tau leptons are effectively Heavy electrons, or muons, and behave in similar ways to the electrons and muons.  \n", "\n", "We will look at the Higgs boson produced at the LHC. The Higgs boson can only be produced at the LHC, and then we can measure its properties by looking at the decay products at the collider. \n", "\n", "The important aspect of $\\tau$ leptons that we will focus on in this lecture is that a $\\tau$ can decay to neutrinos, and either an electron and a neutrino, a muon and a neutrino, or quarks. Electrons, muons and quarks can be reconstructed in a collider detector. However, the neutrinos cannot. Neutrinos are very weakly interacting and they will just escape the detector. \n", "\n", "The one constraint we have for neutrinos is what we call missing transverse energy. Missing transverse energy is a constraint that originates from the fact that when we collider protons along a specific axis, the momentum transverse to that axis is conserved, or in other words. \n", "\n", "\\begin{equation}\n", "\\sum_{i=1}^{N}\\vec{p_{T,i}} = 0 \\\\\n", "\\sum_{i\\neq {\\rm neutrino}}^{N}\\vec{p_{T,i}} + \\sum_{i= {\\rm neutrino}}^{N}\\vec{p_{T,i}} = 0 \\\\\n", "\\sum_{i= {\\rm neutrino}}^{N}\\vec{p_{T,i}} = -\\sum_{i\\neq {\\rm neutrino}}^{N}\\vec{p_{T,i}} \\\\\n", "\\rm{\\vec{MET}}  = -\\sum_{i\\neq {\\rm neutrino}}^{N}\\vec{p_{T,i}} \\\\\n", "\\end{equation}\n", "\n", "This is what we call the MET or missing transverse energy. This is our only constraint on the neutrinos and it gives us a vector in the plane perpendicular to the collision axis along the beam.  \n", "\n", "As was the case with the W and Z boson project, the best way to find the Higgs decaying to $\\tau$ leptons is to reconstruct the mass and look for a bump. The challenge then of finding this process, is to, first, identify the $\\tau$ leptons, and, second, reconstruct the mass of the Higgs boson decays. \n", "\n", "When we have a Higgs decay to $\\tau$ it decays to 2 tau leptons, which means that we will get anywhere from $2-4$ neutrinos. What we want to do in our machine learning algorithm is to identify the topology, and return back a 4-vector that is the sum of all the neutrinos together.  \n", "\n", "Ok, lets now load a dataset that we can use to run this problem. "]}, {"cell_type": "code", "execution_count": 50, "id": "58bc03db", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["import uproot\n", "from collections import OrderedDict \n", "reg    = uproot.open(\"Regression2.root\")[\"Tree\"]"]}, {"cell_type": "code", "execution_count": 50, "id": "daf2fef8", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#what are the inputs\n", "print(reg.keys())\n", "cut=reg['genpt1'].array() >  100\n", "vals=reg['genpt1'].array(library=\"np\")[cut]\n", "np.histogram(vals)"]}, {"cell_type": "markdown", "id": "2e6de9dd", "metadata": {"tags": ["learner", "md"]}, "source": ["This is a bit of a complicated dataset. Let's go through and explain....\n", "\n", "\n", "Ok, lets plot the reconstructed transverse momentum of one of the $\\tau$ and the generator transverse momentum of the true tau lepton. "]}, {"cell_type": "code", "execution_count": 50, "id": "de19a875", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["import numpy as np\n", "\n", "def plot(iVar,iMin,iMax,iColor,iLabel): \n", "    mask=(reg[iVar].array() > 0)\n", "    data=reg[iVar].array(library=\"np\")[mask]\n", "    counts, binEdges = np.histogram(data,bins=50,range=(iMin,iMax),density=False)\n", "    binCenters = (binEdges[1:]+binEdges[:-1])*.5\n", "    err = np.sqrt(counts)\n", "    plt.errorbar(binCenters, counts, yerr=err,fmt=\"o\",alpha=0.5,c=iColor,label=iLabel, ms=3)\n", "    plt.xlabel(\"mass\")\n", "    plt.ylabel(\"N events\")\n", "    return binCenters,counts,err\n", "plot(\"genpt1\",0,200,\"black\",\"true $p_{T}$\")\n", "plot(\"recopt1\",0,200,\"red\",\"observed $p_{T}$\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": 50, "id": "0da6dd45", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#To visualize the whole problem lets make a 2D plot\n", "mask=np.logical_and(reg[\"genpt1\"].array() > 0, reg[\"recopt1\"].array()>0)\n", "x=reg[\"genpt1\"].array(library=\"np\")[mask]\n", "y=reg[\"recopt1\"].array(library=\"np\")[mask]\n", "plt.xlabel(\"gen $p_{T}$\")\n", "plt.ylabel(\"reco $p_{T}$\")a\n", "plt.hist2d(x,y,bins=200)\n", "plt.xlim(0,100)\n", "plt.ylim(0,100)\n", "plt.show()"]}, {"cell_type": "markdown", "id": "1417d8f7", "metadata": {"tags": ["learner", "md"]}, "source": ["So what we see is that reconstructed momentum is not directly correlated with the generator momentum. This is different to the cases in the last section, and what it means is that we will eventually need more variables to actual get the true momentum of the $\\tau$ system. \n", "\n", "With all this information, lets go ahead and build a regression. We can start form our previous setup, and try to just take in the the reconstructed momentum, and predict the true momentum. "]}, {"cell_type": "code", "execution_count": 50, "id": "0898d46f", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#Let's prepare teh data to be pytorch friendly\n", "x=torch.from_numpy(reg[\"recopt1\"].array(library=\"np\")[mask].reshape(len(reg[\"recopt1\"].array(library=\"np\")[mask]),1))\n", "y=torch.from_numpy(reg[\"genpt1\"].array(library=\"np\")[mask].reshape(len(reg[\"genpt1\"].array(library=\"np\")[mask]),1))\n", "x, y = Variable(x), Variable(y)\n", "#torch_dataset = Data.TensorDataset(x, y)\n", "#loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)\n", "\n", "#Now lets make a simple model\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, 200),\n", "        torch.nn.LeakyReLU(),\n", "        torch.nn.Linear(200, 100),\n", "        torch.nn.LeakyReLU(),\n", "        torch.nn.Linear(100, 1),\n", "    )\n", "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n", "loss_func = torch.nn.MSELoss()"]}, {"cell_type": "code", "execution_count": 50, "id": "d7ab80c6", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["def makePlot(x,y,prediction,ax,fig,images,t,loss):\n", "    # plot and show learning process\n", "    plt.cla()\n", "    ax.set_title('Regression Analysis', fontsize=35)\n", "    ax.set_xlabel('Independent variable', fontsize=24)\n", "    ax.set_ylabel('Dependent variable', fontsize=24)\n", "    ax.hist(prediction.data.numpy(),color=\"red\",bins=20,range=(0,200),alpha=0.5,label='pred')\n", "    ax.hist(y.data.numpy(),color=\"black\" ,bins=20,range=(0,200),alpha=0.5,label='gen')\n", "    ax.hist(x.data.numpy(),color=\"green\",bins=20,range=(0,200),alpha=0.5,label='reco')\n", "    ax.scatter(x.data.numpy(), y.data.numpy(), color = \"orange\")\n", "    ax.plot(x.data.numpy(), prediction.data.numpy(), 'g-', lw=3)\n", "    ax.text(100, 2000, 'Epoch = %d' % t,fontdict={'size': 24, 'color':  'red'})\n", "    ax.text(100, 5000, 'Loss = %.4f' % loss.data.numpy(),fontdict={'size': 24, 'color':  'red'})\n", "    fig.canvas.draw()       # draw the canvas, cache the renderer\n", "    ax.legend()\n", "    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n", "    image  = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n", "    images.append(image)"]}, {"cell_type": "code", "execution_count": 50, "id": "c112f725", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["images=train(x,y,net,loss_func,optimizer,100)\n", "imageio.mimsave('./reg1.gif', images, fps=12)"]}, {"cell_type": "markdown", "id": "b35a06aa", "metadata": {"tags": ["learner", "md"]}, "source": ["So we can see, it tries to predict the average, but its not that great. Lets look at hte properties of this. "]}, {"cell_type": "code", "execution_count": 50, "id": "2814e479", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#What did this do?\n", "true=reg[\"genpt1\"].array(library=\"np\")[mask]\n", "reco=reg[\"recopt1\"].array(library=\"np\")[mask]\n", "pred=net(x)\n", "ratio=np.array(true/reco)\n", "ratiopred=y/pred\n", "plt.hist(ratio,color=\"red\",bins=20,range=(0,3),alpha=0.5,label=\"true\")\n", "plt.hist(ratiopred.data.numpy(),color=\"blue\",bins=20,range=(0,3),alpha=0.5,label=\"corr\")\n", "plt.legend()\n", "plt.show()\n", "print(\"True Mean: \",ratio.mean(),\"True StdDev:\",ratio.std())\n", "print(\"NN Mean: \",ratiopred.data.numpy().mean(),\"NN StdDev:\",ratiopred.data.numpy().std())"]}, {"cell_type": "markdown", "id": "00135026", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_19_7'></a>     \n", "\n", "| [Top](#section_19_0) | [Restart Section](#section_19_7) | [Next Section](#section_19_8) |\n"]}, {"cell_type": "markdown", "id": "49d0dd82", "metadata": {"tags": ["learner", "md", "catsoop_07"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 19.7.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 73, "id": "1486329d", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "d8af0c1d", "metadata": {"tags": ["learner", "catsoop_07", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "f846027c", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_19_8'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L19.8 The Tau Lepton: Part III</h2>  \n", "\n", "| [Top](#section_19_0) | [Previous Section](#section_19_7) | [Exercises](#exercises_19_8) | [Next Section](#section_19_9) |\n"]}, {"cell_type": "markdown", "id": "dd886a61", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section."]}, {"cell_type": "code", "execution_count": 8, "id": "33966ea7", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"data": {"text/html": ["\n", "        <iframe\n", "            width=\"970\"\n", "            height=\"550\"\n", "            src=\"images/slides_L19_08.html\"\n", "            frameborder=\"0\"\n", "            allowfullscreen\n", "            \n", "        ></iframe>\n", "        "], "text/plain": ["<IPython.lib.display.IFrame at 0x1115a2a60>"]}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": ["#>>>RUN\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L19/slides_L19_08.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "b91d2bf4", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Observing the improvements</h3>\n", "\n", "Ok, so we now have a very basic regressoin that predict a single neutrino from a single $\\tau$ lepton decay. This is a very simple regression, and we see that the prediction doesn't work so well. Also, its a bit hard to understand exactly what the training is doing. \n", "\n", "In light of this, lets write some code from these samples to actually reconstruct the Higgs boson mass, then we can go back to the training and see if we can really optimize it. \n", "\n", "If we look at the dataset, we find that we have two variables that give the mass. One is the \"True Higgs boson mass\", this is the Higgs boson mass that is created by our monte carlo simulation. For each event, we sample a Higgs boson mass distribution, this is our sampled event. \n", "\n", "For the second mass, what this is doing is take the reconstructed $\\tau$ object, without the neutrinos and compute the mass by adding the 4-vectors. In other words, the mass can be written as a sum of the reconstructed 4-vectors of the two identified and reconstructed $\\tau$ leptons. In other words, \n", "\\begin{equation}\n", "m_{\\rm vis} = \\left(E_{1} + E_{2} \\right)^2 - \\left(\\vec{p}_{1} + \\vec{p}_{2}\\right)^2\n", "\\end{equation}\n", "\n", "We call this the visible mass, for the simple fact that it excludes all the invisible particles (aka neutrinos). We will forgo the details of how the 4-vectors are reconstructed. However, just assume its a complciated process. Ok, let's plot the mass "]}, {"cell_type": "code", "execution_count": 50, "id": "a6d6fa0d", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#now lets construct the higgs mass\n", "plot(\"hmass\",0,200,\"black\",\"True Higgs Mass\")\n", "plot(\"recohmass\",0,200,\"red\",\" Reconstructed\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "b7d0a081", "metadata": {"tags": ["learner", "md"]}, "source": ["Now, to build a correction to the $\\tau$ lepton momentum, what we can do is is reconstruct the 4-vectors. To do this we are going to use the pylorentz vector, wich just does the calculation above. "]}, {"cell_type": "code", "execution_count": 50, "id": "66a8faea", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["!pip install pylorentz\n", "from pylorentz import Momentum4\n", "\n", "#Lets compute the mass on the fly\n", "def masscompute(iVec1,iVec2):\n", "    tau_1 = Momentum4.m_eta_phi_pt(iVec1[3], iVec1[1], iVec1[2], iVec1[0])\n", "    tau_2 = Momentum4.m_eta_phi_pt(iVec2[3], iVec2[1], iVec2[2], iVec2[0])\n", "    return (tau_1+tau_2).m\n", "    \n", "def hmass(massfunc):\n", "    mask=(reg[\"recohmass\"].array() > 0)\n", "    varlist=[\"recopt1\",\"recoeta1\",\"recophi1\",\"recomass1\",\"recopt2\",\"recoeta2\",\"recophi2\",\"recomass2\"]\n", "    arr=0\n", "    idx=0\n", "    for x in varlist:\n", "        pArr=reg[x].array(library=\"np\")[mask]\n", "        if idx == 0: \n", "            arr = pArr\n", "            idx = idx + 1\n", "        else:\n", "            arr=np.vstack((arr,pArr))\n", "    arr = arr.T\n", "    massc = lambda iarr: massfunc(iarr[0:4],iarr[4:8]) \n", "    hmasses = np.array([massc(p) for p in arr])\n", "    return hmasses\n", "\n", "rawmvis=hmass(masscompute)\n", "plt.hist(rawmvis,bins=50,range=(0,200),color='blue',label=\"Our Mass\")\n", "#plot(\"hmass\",0,200,\"black\",\"True Higgs Mass\")\n", "plot(\"recohmass\",0,200,\"red\",\"Reconstructed\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "cca05edf", "metadata": {"tags": ["learner", "md"]}, "source": ["Ok, great, looks like we can reconstruct the mass. Lets take a look at our how our regression mass looks like. To do this, what we will do is modify our mass computation to include the neural net clalculation."]}, {"cell_type": "code", "execution_count": 50, "id": "130d2efd", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#Lets compute the mass on the fly\n", "def masscompute(iVec1,iVec2):\n", "    pt1 = torch.tensor([iVec1[0]])\n", "    pt2 = torch.tensor([iVec2[0]])\n", "    corr1 = net(pt1).data.numpy()\n", "    corr2 = net(pt2).data.numpy()\n", "    tau_1 = Momentum4.m_eta_phi_pt(iVec1[3], iVec1[1], iVec1[2], corr1)\n", "    tau_2 = Momentum4.m_eta_phi_pt(iVec2[3], iVec2[1], iVec2[2], corr2)\n", "    return (tau_1+tau_2).m\n", "\n", "rawmass=hmass(masscompute)\n", "plt.hist(rawmass,bins=50,range=(0,200),color='blue',label=\"Our corrected Mass\")\n", "#plot(\"hmass\",0,200,\"black\",\"True Higgs Mass\")\n", "plot(\"recohmass\",0,200,\"red\",\"Reconstructed\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "b928996d", "metadata": {"tags": ["learner", "md"]}, "source": ["So this is making the mass narrower. However, as we know, this network is not necessarily respecting the mass. To see this, lets just make a toy dataset by sampling a distribution. "]}, {"cell_type": "code", "execution_count": 50, "id": "ca03d49d", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#Lets compute the mass on the fly\n", "def masscompute(iVec1,iVec2):\n", "    pVal=torch.distributions.Uniform(0, 100).sample((2,1))\n", "    corr = net(pVal).data.numpy()\n", "    tau_1 = Momentum4.m_eta_phi_pt(iVec1[3], iVec1[1], iVec1[2], corr[0])\n", "    tau_2 = Momentum4.m_eta_phi_pt(iVec2[3], iVec2[1], iVec2[2], corr[1])\n", "    return (tau_1+tau_2).m\n", "\n", "def masscomputeNoNN(iVec1,iVec2):\n", "    pVal=torch.distributions.Uniform(0, 100).sample((2,1))\n", "    tau_1 = Momentum4.m_eta_phi_pt(iVec1[3], iVec1[1], iVec1[2], pVal[0])\n", "    tau_2 = Momentum4.m_eta_phi_pt(iVec2[3], iVec2[1], iVec2[2], pVal[1])\n", "    return (tau_1+tau_2).m\n", "\n", "rawmass=hmass(masscompute)\n", "plt.hist(rawmass,bins=50,range=(0,200),color='blue',alpha=0.5,label=\"Adding the NN\")\n", "\n", "rawmass=hmass(masscomputeNoNN)\n", "plt.hist(rawmass,bins=50,range=(0,200),color='green',alpha=0.5,label=\"What the Mass is\")\n", "\n", "#plot(\"hmass\",0,200,\"black\",\"True Higgs Mass\")\n", "recohmassX,recohmassY,recohmassYErr=plot(\"recohmass\",0,200,\"red\",\"Reconstructed\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": 50, "id": "5dfb048a", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#Lets compute the mass on the fly\n", "def masscompute(iVec1,iVec2):\n", "    pVal=torch.distributions.Uniform(0, 20).sample((2,1))\n", "    pVal=pVal**2\n", "    corr = net(pVal).data.numpy()\n", "    tau_1 = Momentum4.m_eta_phi_pt(iVec1[3], iVec1[1], iVec1[2], corr[0])\n", "    tau_2 = Momentum4.m_eta_phi_pt(iVec2[3], iVec2[1], iVec2[2], corr[1])\n", "    return (tau_1+tau_2).m\n", "\n", "def masscomputeNoNN(iVec1,iVec2):\n", "    pVal=torch.distributions.Uniform(0, 20).sample((2,1))\n", "    pVal=pVal**2\n", "    tau_1 = Momentum4.m_eta_phi_pt(iVec1[3], iVec1[1], iVec1[2], pVal[0])\n", "    tau_2 = Momentum4.m_eta_phi_pt(iVec2[3], iVec2[1], iVec2[2], pVal[1])\n", "    return (tau_1+tau_2).m\n", "\n", "rawmass=hmass(masscompute)\n", "plt.hist(rawmass,bins=50,range=(0,500),color='blue',alpha=0.5,label=\"Adding the NN\")\n", "\n", "rawmass=hmass(masscomputeNoNN)\n", "plt.hist(rawmass,bins=50,range=(0,500),color='green',alpha=0.5,label=\"What the Mass is\")\n", "\n", "#plot(\"hmass\",0,200,\"black\",\"True Higgs Mass\")\n", "plot(\"recohmass\",0,500,\"red\",\"Reconstructed\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "cf3ddd19", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Optimized Target</h3>\n", "\n", "Now before we go ahead and try to make the most sophisticated deep learning regression. Lets go ahead and expore an optimzied target. \n", "\n", "In our first training we trained for $p_{T}^{Gen}$ of the neutrino. However, we could imagine instead of training for a dimensionless variable of : \n", "\\begin{equation}\n", "\\frac{p_{T}^{Gen}}{p_{T}^{Reco}}\n", "\\end{equation}\n", "instead. The advantage of this target is that it is unitless, which means that if there is an invrance against momentum, like lorentz invariance, we can capture it in this quantity. Defining losses, which respect invariant quantities in physics has become an industry over the past few years. Lets go ahead and repeat our training from before. Also, lets update our training performance code to reconstruct the mass. "]}, {"cell_type": "code", "execution_count": 50, "id": "dfe71127", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#Now lets make a simple model\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, 200),\n", "        torch.nn.LeakyReLU(),\n", "        torch.nn.Linear(200, 100),\n", "        torch.nn.LeakyReLU(),\n", "        torch.nn.Linear(100, 1),\n", "    )\n", "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n", "loss_func = torch.nn.MSELoss()\n", "x=torch.from_numpy(reg[\"recopt1\"].array(library=\"np\")[mask].reshape(len(reg[\"recopt1\"].array(library=\"np\")[mask]),1))\n", "y=torch.from_numpy(reg[\"genpt1\"].array(library=\"np\")[mask].reshape(len(reg[\"genpt1\"].array(library=\"np\")[mask]),1))\n", "x, y = Variable(x), Variable(y)\n", "ratio=torch.div(y,x)\n", "y=ratio\n", "\n", "#Lets compute the mass on the fly\n", "def masscomputeNN(iVec1,iVec2):\n", "    pt1 = torch.tensor([iVec1[0]])\n", "    pt2 = torch.tensor([iVec2[0]])\n", "    corr1 = net(pt1).data.numpy()*iVec1[0]\n", "    corr2 = net(pt2).data.numpy()*iVec2[0]\n", "    tau_1 = Momentum4.m_eta_phi_pt(iVec1[3], iVec1[1], iVec1[2], corr1)\n", "    tau_2 = Momentum4.m_eta_phi_pt(iVec2[3], iVec2[1], iVec2[2], corr2)\n", "    return (tau_1+tau_2).m\n", "\n", "#And lets plot the mass instead of the pT\n", "def makePlot(x,y,prediction,ax,fig,images,t,loss):\n", "    #compute the mass\n", "    rawmass=hmass(masscomputeNN)\n", "    # plot and show learning process\n", "    plt.cla()\n", "    ax.hist(rawmvis,bins=40,range=(0,250),color='blue',alpha=0.5,label='raw')\n", "    ax.hist(rawmass,bins=40,range=(0,250),color='red',alpha=0.5,label='regressed')\n", "    ax.text(150, 300, 'Epoch = %d' % t,fontdict={'size': 24, 'color':  'red'})\n", "    ax.text(150, 600, 'Loss = %.4f' % loss.data.numpy(),fontdict={'size': 24, 'color':  'red'})\n", "    ax.set_title('Regression Analysis', fontsize=35)\n", "    ax.set_xlabel('Mass', fontsize=24)\n", "    ax.set_ylabel('N', fontsize=24)\n", "    ax.set_ylim(0,2000)\n", "    fig.canvas.draw()       # draw the canvas, cache the renderer\n", "    ax.legend()\n", "    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n", "    image  = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n", "    images.append(image)"]}, {"cell_type": "code", "execution_count": 50, "id": "43c5847c", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#images=train(x,y,net,loss_func,optimizer,1000)\n", "#imageio.mimsave('./reg2_long.gif', images, fps=12)\n", "images=train(x,y,net,loss_func,optimizer,100)\n", "imageio.mimsave('./reg2_long.gif', images, fps=12)"]}, {"cell_type": "markdown", "id": "17b05e5f", "metadata": {"tags": ["learner", "md"]}, "source": ["### Challenge Question\n", "\n", "Repeat the above regression, but now make the true mass `genhmass` as the target variable, what happens to the regression? Why will this not work? "]}, {"cell_type": "markdown", "id": "225cc6db", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>The full Mass regression</h3>\n", "\n", "Ok, now that we have a setup that seems to be working the final challenge is to use all the possible information that we have in our event to do the best reconstruction we possibly can. In this case, what we will do is use as many different variables as we possibly can to predict the neutrino position. \n", "\n", "The way we are going to do this, is dig into the dataset a little bit. A $\\tau$ lepton is reconstructed in the same way a jet is reconstructed at the LHC. We first look for a clump of particles that have a large amount of energy in a cone. In this case, we also look for, what we called an isolated clump. Namely, we see a few particles in the cone that have very large momentum and, when we remove these particles, the rest of the momentum in the cone is small (typically less that 10 percent the total momentum). This final colleciton of particles, gives us a $\\tau$ lepton. \n", "\n", "Interestingly the decay components of the $\\tau$ lepton in the cone can tell us information about the energy of the neutrino. The reason comes from the decay of the $\\tau$ lepton. A $\\tau$ has a fixed mass at 1.75 GeV. As a consequence, the invariant mass of the all of the particles within a cone should sum up to the $\\tau$ lepton. In reality, we can't get this perfectly because our measurements are not perfect. However, we know that that\n", "\n", "\\begin{eqnarray}\n", "\\left(\\sum_{i} \\vec{p_{i}}\\right)^2 & \\approx & m^{2}_{\\tau} \\\\\n", "\\left(\\sum_{i\\neq{\\rm neutrino}} \\vec{p_{i}} + \\sum_{i={\\rm neutrino}} \\vec{p_{i}} \\right)^2 & \\approx & m^{2}_{\\tau} \\\\\n", "\\end{eqnarray}\n", "or in other words, the neutrino positions and directions are constrained by the other particles in the $\\tau$ decay. We don't need to expand this equation and solve it, we just need to know there is a correlation to know that we use the $\\tau$ decay components to solve for the neutrino position.  \n", "\n", "This all means that we can build a neural network with the particles as input to predict the neutrino positions. Let's go ahead and look at a few taus, and then we build our network and train it.  "]}, {"cell_type": "code", "execution_count": 50, "id": "66bf9ae9", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#Lets compute the mass on the fly\n", "def makedataset(iMask,iPart=\"part1\"):\n", "    varlist=[iPart+\"pt1\",iPart+\"eta1\",iPart+\"phi1\",iPart+\"id1\",iPart+\"pt2\",iPart+\"eta2\",iPart+\"phi2\",iPart+\"id2\",iPart+\"pt3\",iPart+\"eta3\",iPart+\"phi3\",iPart+\"id3\",iPart+\"pt4\",iPart+\"eta4\",iPart+\"phi4\",iPart+\"id4\",iPart+\"pt5\",iPart+\"eta5\",iPart+\"phi5\",iPart+\"id5\"]\n", "    arr=0\n", "    idx=0\n", "    for x in varlist:\n", "        pArr=reg[x].array(library=\"np\")[iMask]\n", "        if idx == 0: \n", "            arr = pArr\n", "            idx = idx + 1\n", "        else:\n", "            arr=np.vstack((arr,pArr))\n", "    arr = arr.T\n", "    return arr\n", "\n", "mask1=(reg[\"genpt1\"].array(library=\"np\") > 0)\n", "mask2=(reg[\"recopt1\"].array(library=\"np\") > 0)\n", "mask3=(reg[\"genpt2\"].array(library=\"np\") > 0)\n", "mask4=(reg[\"recopt2\"].array(library=\"np\") > 0)\n", "mask = np.logical_and.reduce([mask1,mask2,mask3,mask4])\n", "x=torch.from_numpy(makedataset(mask))\n", "yb=torch.from_numpy(reg[\"recopt1\"].array(library=\"np\")[mask].reshape(len(reg[\"recopt1\"].array(library=\"np\")[mask]),1))\n", "y=torch.from_numpy(reg[\"genpt1\"].array(library=\"np\")[mask].reshape(len(reg[\"genpt1\"].array(library=\"np\")[mask]),1))\n", "ratio=torch.div(y,yb)\n", "y=ratio\n", "x,y = Variable(x),Variable(y)\n", "torch_dataset = Data.TensorDataset(x, y)\n", "#print(x)"]}, {"cell_type": "markdown", "id": "d0b59627", "metadata": {"tags": ["learner", "md"]}, "source": ["To get a feel for how these corrections work, we can visualize a few events by making some evnet plots to look at the top 5 particles in position space. "]}, {"cell_type": "code", "execution_count": 50, "id": "516acfa8", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["ds=makedataset(mask)\n", "colors = ['g','r','b','y','orange']\n", "for i0 in range(5):\n", "    for ipart in range(5):\n", "        plt.scatter(ds[i0,4*ipart+1], ds[i0,4*ipart+2], s=ds[i0,4*ipart]*5000/yb[i0], c=colors[ipart], alpha=0.5)\n", "    plt.xlim(-0.5,0.5)\n", "    plt.ylim(-0.5,0.5)\n", "    plt.xlabel(\"$\\eta$\")\n", "    plt.ylabel(\"$\\phi$\")\n", "    plt.text(-0.3,0.4,\"Correction Factr\"+str(ratio[i0].numpy()[0]))\n", "    plt.show()"]}, {"cell_type": "markdown", "id": "99c1fea4", "metadata": {"tags": ["learner", "md"]}, "source": ["Broadly speaking, what we observe is that events that are spread out tend to have larger corrections that events that are not. This means we can use this shape to actually predict the correction. There is some more subtlety coming from the fact that $\\tau$ leptons decay into different particle types. That will allow us to understand the type of decay that occurred. For example, $\\tau$ lepton decays that have a lot of photons, will have a different energy distribution that $\\tau$ leptons that don't have any photons. \n", "\n", "Alright, lets go ahead and train our network. "]}, {"cell_type": "code", "execution_count": 50, "id": "602e7845", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#now lets see if we can improve this with something more complicated\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(20, 200),\n", "        torch.nn.LeakyReLU(),\n", "        torch.nn.Linear(200, 200),\n", "        torch.nn.LeakyReLU(),\n", "        torch.nn.Linear(200, 50),\n", "        torch.nn.LeakyReLU(),\n", "        torch.nn.Linear(50, 1),\n", "    )\n", "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n", "loss_func = torch.nn.MSELoss()"]}, {"cell_type": "code", "execution_count": 50, "id": "46d3b0ca", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["p1=torch.from_numpy(makedataset(mask,\"part1\"))\n", "p2=torch.from_numpy(makedataset(mask,\"part2\"))\n", "\n", "def masscomputeNN(iC1,iC2,iVec1,iVec2):\n", "    corr1 = iC1*iVec1[0]\n", "    corr2 = iC2*iVec2[0]\n", "    tau_1 = Momentum4.m_eta_phi_pt(iVec1[3], iVec1[1], iVec1[2], corr1)\n", "    tau_2 = Momentum4.m_eta_phi_pt(iVec2[3], iVec2[1], iVec2[2], corr2)\n", "    return (tau_1+tau_2).m\n", "\n", "#now lets compute the corrected mass on the data set\n", "def hmass(masscomputeNN):\n", "    corr1=net(p1)\n", "    corr2=net(p2)\n", "    varlist=[\"recopt1\",\"recoeta1\",\"recophi1\",\"recomass1\",\"recopt2\",\"recoeta2\",\"recophi2\",\"recomass2\"]\n", "    arr=np.vstack((corr1.data.numpy().T,corr2.data.numpy().T))\n", "    for x in varlist:\n", "        pArr=reg[x].array(library=\"np\")[mask]\n", "        arr=np.vstack((arr,pArr))\n", "    arr = arr.T\n", "    massc = lambda iarr: masscomputeNN(iarr[0],iarr[1],iarr[2:6],iarr[6:10]) \n", "    hmasses = np.array([massc(p) for p in arr])\n", "    return hmasses\n", "\n", "#now update to add history\n", "history_lr = {'loss':[], 'val_loss':[]}\n", "def train(x,y,net,loss_func,opt,nepochs):\n", "    images = []\n", "    fig, ax = plt.subplots(figsize=(12,7))\n", "    for epoch in range(nepochs):\n", "        if epoch % 50 == 0: \n", "            print(\"epoch:\",epoch)\n", "        prediction = net(x)\n", "        loss = loss_func(prediction, y) \n", "        opt.zero_grad()\n", "        loss.backward() \n", "        optimizer.step()\n", "        with torch.no_grad():#disable updating gradient\n", "            print('[%d] loss: %.4f ' % (epoch + 1, loss ))\n", "            history_lr['loss'].append(loss)\n", "        if epoch % 5 == 0:\n", "            makePlot(x,y,prediction,ax,fig,images,epoch,loss)\n", "    return images\n", "\n", "rawmass=hmass(masscomputeNN)\n", "plt.hist(rawmass,bins=50,range=(0,500),color='blue',alpha=0.5,label=\"Adding the NN\")\n", "plt.xlabel(\"mass(GeV)\")\n", "plt.ylabel(\"N$_{events}$\")\n", "plt.show()"]}, {"cell_type": "code", "execution_count": 50, "id": "1f9078eb", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["images=train(x,y,net,loss_func,optimizer,1000)\n", "imageio.mimsave('./reg2.gif', images, fps=12)\n", "#images=train(x,y,net,loss_func,optimizer,1000)\n", "#imageio.mimsave('./reg_long.gif', images, fps=12)"]}, {"cell_type": "code", "execution_count": 50, "id": "aeeccc06", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["plt.semilogy(history_lr['loss'], label='loss')\n", "plt.legend(loc=\"upper right\")\n", "plt.xlabel('epoch')\n", "plt.ylabel('loss (binary crossentropy)')\n", "plt.show()"]}, {"cell_type": "code", "execution_count": 50, "id": "d3af5171", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["def plotcorr(iVar,iNN,iMin,iMax,iColor,iLabel,iCorr=True): \n", "    corr1=net(p1)\n", "    data=reg[iVar].array(library=\"np\")[mask]\n", "    if iCorr:\n", "        data=data*corr1.data.numpy().T\n", "    counts, binEdges = np.histogram(data,bins=50,range=(iMin,iMax),density=False)\n", "    binCenters = (binEdges[1:]+binEdges[:-1])*.5\n", "    err = np.sqrt(counts)\n", "    plt.errorbar(binCenters, counts, yerr=err,fmt=\"o\",c=iColor, ms=3,label=iLabel)\n", "    \n", "plotcorr(\"genpt1\" ,net,0,200,\"black\",\"gen\",False)\n", "plotcorr(\"recopt1\",net,0,200,\"red\",\"reco\",False)\n", "plotcorr(\"recopt1\",net,0,200,\"blue\",\"corrected\")\n", "plt.legend()\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "2adf5fce", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_19_8'></a>     \n", "\n", "| [Top](#section_19_0) | [Restart Section](#section_19_8) | [Next Section](#section_19_9) |\n"]}, {"cell_type": "markdown", "id": "103a9b46", "metadata": {"tags": ["learner", "md", "catsoop_08"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 19.8.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 83, "id": "a14c4715", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "97d0d22a", "metadata": {"tags": ["learner", "catsoop_08", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "8d118b99", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_19_9'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L19.9 The Tau Lepton: Part IV</h2>     \n", "\n", "| [Top](#section_19_0) | [Previous Section](#section_19_8) | [Exercises](#exercises_19_9) |\n"]}, {"cell_type": "markdown", "id": "903ff83e", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section."]}, {"cell_type": "code", "execution_count": 2, "id": "88015a5d", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"data": {"text/html": ["\n", "        <iframe\n", "            width=\"970\"\n", "            height=\"550\"\n", "            src=\"https://mitx-8s50.github.io/slides/L19/slides_L19_09.html\"\n", "            frameborder=\"0\"\n", "            allowfullscreen\n", "            \n", "        ></iframe>\n", "        "], "text/plain": ["<IPython.lib.display.IFrame at 0x10db4dd30>"]}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": ["#>>>RUN\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L19/slides_L19_09.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "a113055f", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>15.7 NN Architecture</h3>\n", "\n", "Now one last thing that we can cnosider is to use a different type of architecture for the neutral network. This is where deep learning has become very valuable over the last 10 years. By making it such that entworks take only a subset of inputs at a time, we can effectively organize the training of the data, and guide the deep learning arhitecture to a better overall minimimu. \n", "\n", "For the $\\tau$ regression, the ideal tool for this is a recurrent neural network or an [RNN](https://en.wikipedia.org/wiki/Recurrent_neural_network). These networks are designed to take in data, piece by piece to build up a prediction. They are often used on time series datasets, like the LIGO dataset, to process data and infer the likelihood of an event or to apply a correction. Perhaps, their most common use has been in language translation to translate word by word, or phrase by phrase. \n", "\n", "In recent times, high energy physicsists have taken to using RNNs to proces data by feeding the RNN individual particles at a time. The nice thing is that since this takes in a series of particles, we could, in principle, give it a variable number of particles. \n", "\n", "In this example, we will use a long-short term memory (LSTM) network. Here is the pytorch for how its defined. "]}, {"cell_type": "code", "execution_count": 50, "id": "d38db36b", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["class LSTM(nn.Module):\n", "\n", "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n", "        super(LSTM, self).__init__()\n", "        \n", "        self.num_classes = num_classes\n", "        self.num_layers = num_layers\n", "        self.input_size = input_size\n", "        self.hidden_size = hidden_size\n", "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n", "                            num_layers=num_layers, batch_first=True)\n", "        \n", "        self.fc1 = nn.Linear(hidden_size, 20)\n", "        self.fc2 = nn.Linear(20, num_classes)\n", "\n", "    def forward(self, x):\n", "        h_0 = Variable(torch.zeros(\n", "            self.num_layers, x.size(0), self.hidden_size))\n", "        \n", "        c_0 = Variable(torch.zeros(\n", "            self.num_layers, x.size(0), self.hidden_size))\n", "        \n", "        # Propagate input through LSTM\n", "        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n", "        \n", "        h_out = h_out.view(-1, self.hidden_size)\n", "        \n", "        out = self.fc1(h_out)\n", "        out = F.relu(out)\n", "        out = self.fc2(out)\n", "        return out\n", "\n", "input_size = 4\n", "hidden_size = 128\n", "num_layers = 1\n", "num_classes = 1\n", "lstm = LSTM(num_classes, input_size, hidden_size, num_layers)\n", "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n", "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.01)\n", "lstm.train()"]}, {"cell_type": "code", "execution_count": 50, "id": "aacf38df", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["def makedatasetrnn(iMask,iPart=\"part1\"):\n", "    arr=makedataset(iMask,iPart)\n", "    return arr.reshape(len(arr),5,4)\n", "\n", "x=torch.from_numpy(makedatasetrnn(mask))"]}, {"cell_type": "code", "execution_count": 50, "id": "c9a53ad1", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["p1=torch.from_numpy(makedatasetrnn(mask,\"part1\"))\n", "p2=torch.from_numpy(makedatasetrnn(mask,\"part2\"))\n", "\n", "#now lets compute the corrected mass on the data set\n", "def hmass(masscomputeNN):\n", "    mask=(reg[\"recohmass\"].array(library=\"np\") > 0)\n", "    corr1=lstm(p1)\n", "    corr2=lstm(p2)\n", "    varlist=[\"recopt1\",\"recoeta1\",\"recophi1\",\"recomass1\",\"recopt2\",\"recoeta2\",\"recophi2\",\"recomass2\"]\n", "    arr=np.vstack((corr1.data.numpy().T,corr2.data.numpy().T))\n", "    for x in varlist:\n", "        pArr=reg[x].array(library=\"np\")[mask]\n", "        arr=np.vstack((arr,pArr))\n", "    arr = arr.T\n", "    massc = lambda iarr: masscomputeNN(iarr[0],iarr[1],iarr[2:6],iarr[6:10]) \n", "    hmasses = np.array([massc(p) for p in arr])\n", "    return hmasses\n", "\n", "outmass=hmass(masscomputeNN)\n", "plt.hist(outmass,bins=40,range=(0,250),color='blue')\n", "plt.xlabel(\"mass\")\n", "plt.ylabel(\"N$_{events}$\")\n", "plt.show()"]}, {"cell_type": "code", "execution_count": 50, "id": "b48eeff9", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["images=train(x,y,lstm,criterion,optimizer,500)\n", "imageio.mimsave('./reg_lstm.gif', images, fps=12)\n", "#images=train(x,y,lstm,criterion,optimizer,1000)\n", "#imageio.mimsave('./reg_lstm_long.gif', images, fps=12)"]}, {"cell_type": "code", "execution_count": 50, "id": "99fdcd0b", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["plt.semilogy(history_lr['loss'], label='loss')\n", "plt.legend(loc=\"upper right\")\n", "plt.xlabel('epoch')\n", "plt.ylabel('loss (binary crossentropy)')\n", "plt.show()"]}, {"cell_type": "code", "execution_count": 50, "id": "6fee7b6a", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["plotcorr(\"genpt1\" ,lstm,0,200,\"black\",\"gen\",False)\n", "plotcorr(\"recopt1\",lstm,0,200,\"red\",\"reco\",False)\n", "plotcorr(\"recopt1\",lstm,0,200,\"blue\",\"corrected\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": 50, "id": "17aca122", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["randscale=np.random.uniform(0, 1,len(y))\n", "\n", "#Lets compute the mass on the fly\n", "def makedataset(iMask,iPart=\"part1\"):\n", "    varlist=[iPart+\"pt1\",iPart+\"eta1\",iPart+\"phi1\",iPart+\"id1\",iPart+\"pt2\",iPart+\"eta2\",iPart+\"phi2\",iPart+\"id2\",iPart+\"pt3\",iPart+\"eta3\",iPart+\"phi3\",iPart+\"id3\",iPart+\"pt4\",iPart+\"eta4\",iPart+\"phi4\",iPart+\"id4\",iPart+\"pt5\",iPart+\"eta5\",iPart+\"phi5\",iPart+\"id5\"]\n", "    arr=0\n", "    idx=0\n", "    for x in varlist:\n", "        pArr=reg[x].array(library=\"np\")[iMask]\n", "        if \"pt\" in x:\n", "            pArr=pArr*randscale\n", "        if idx == 0: \n", "            arr = pArr\n", "            idx = idx + 1\n", "        else:\n", "            arr=np.vstack((arr,pArr))\n", "    arr = arr.T\n", "    return arr\n", "\n", "p1p=torch.from_numpy(makedatasetrnn(mask,\"part1\")).float()\n", "p2p=torch.from_numpy(makedatasetrnn(mask,\"part2\")).float()\n", "\n", "\n", "#now lets compute the corrected mass on the data set\n", "def hmass(masscomputeNN,iCorr=True):\n", "    mask=(reg[\"recohmass\"].array(library=\"np\") > 0)\n", "    corr1=lstm(p1p)\n", "    corr2=lstm(p2p)\n", "    varlist=[\"recopt1\",\"recoeta1\",\"recophi1\",\"recomass1\",\"recopt2\",\"recoeta2\",\"recophi2\",\"recomass2\"]\n", "    arr=np.vstack((corr1.data.numpy().T,corr2.data.numpy().T))\n", "    for x in varlist:\n", "        pArr=reg[x].array(library=\"np\")[mask]\n", "        if \"pt\" in x:\n", "            arr=arr*randscale\n", "        arr=np.vstack((arr,pArr))\n", "    arr = arr.T\n", "    if not iCorr:\n", "        arr[:,0]/=arr[:,0]\n", "        arr[:,1]/=arr[:,1]\n", "    massc = lambda iarr: masscomputeNN(iarr[0],iarr[1],iarr[2:6],iarr[6:10]) \n", "    hmasses = np.array([massc(p) for p in arr])\n", "    return hmasses\n", "\n", "\n", "rawmass=hmass(masscomputeNN,True)\n", "print(rawmass[-1])\n", "plt.hist(rawmass,bins=50,range=(0,200),color='blue',alpha=0.5,label=\"Adding the NN\")\n", "\n", "rawmass=hmass(masscomputeNN,False)\n", "print(rawmass[-1])\n", "plt.hist(rawmass,bins=50,range=(0,200),color='green',alpha=0.5,label=\"What the Mass is\")\n", "\n", "#plot(\"hmass\",0,200,\"black\",\"True Higgs Mass\")\n", "recohmassX,recohmassY,recohmassYErr=plot(\"recohmass\",0,200,\"red\",\"Reconstructed\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "fe8da95a", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_19_9'></a>   \n", "\n", "| [Top](#section_19_0) | [Restart Section](#section_19_9) |\n"]}, {"cell_type": "markdown", "id": "df9f7e93", "metadata": {"tags": ["learner", "md", "catsoop_09"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 19.9.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 93, "id": "5c040ac1", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "6e296394", "metadata": {"tags": ["learner", "catsoop_09", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}