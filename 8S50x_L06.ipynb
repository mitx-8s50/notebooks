{"cells": [{"cell_type": "markdown", "id": "43008ece", "metadata": {"tags": ["learner", "md"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 6: Confidence</h1>\n"]}, {"cell_type": "markdown", "id": "972a4cbb", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_6_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L6.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "2abe0e44", "metadata": {"tags": ["learner", "md"]}, "source": ["<table style=\"width:100%\">\n", "    <colgroup>\n", "       <col span=\"1\" style=\"width: 40%;\">\n", "       <col span=\"1\" style=\"width: 15%;\">\n", "       <col span=\"1\" style=\"width: 45%;\">\n", "    </colgroup>\n", "    <tr>\n", "        <th style=\"text-align: left; font-size: 13pt;\">Section</th>\n", "        <th style=\"text-align: left; font-size: 13pt;\">Exercises</th>\n", "        <th style=\"text-align: left; font-size: 13pt;\">Summary</th>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_6_1\">L6.1 Introduction to Confidence Intervals</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_6_1\">L6.1 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_6_2\">L6.2 z-Scores and Confidence Intervals for Other Distributions</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_6_2\">L6.2 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_6_3\">L6.3 Another Example and Rules of Significance</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_6_3\">L6.3 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_6_4\">L6.4 Moments of Distributions and Mapping</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_6_4\">L6.4 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_6_5\">L6.5 Monte Carlo Integration</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_6_5\">L6.5 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_6_6\">L6.6 Returning to Super Nova Data Fitting</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_6_6\">L6.6 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_6_7\">L6.7 Fitting with a More Accurate Model</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_6_7\">L6.7 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_6_8\">L6.8 Fit to Full Cosmological Model</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_6_8\">L6.8 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "</table>\n", "\n"]}, {"cell_type": "markdown", "id": "99982f15", "metadata": {"tags": ["learner", "catsoop_00", "md"]}, "source": ["<h3>Learning Objectives</h3>\n", "\n", "In this class, we will spend some time further understanding uncertainty in more complicated scenarios. \n", "\n", "In this lecture we will explore the following objectives:\n", "\n", "- How do we quote significance?\n", "- Asymmetric distributions\n", "- Moments of distributions\n", "- Numerical Integration\n", "- A more sophisticated fit\n", "- What have we learned about the properties of the universe"]}, {"cell_type": "markdown", "id": "3086e94d", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Importing Libraries</h3>\n", "\n", "Before beginning, run the cell below to import the relevant libraries for this notebook. \n", "Optionally, set the plot resolution and default figure size.\n"]}, {"cell_type": "code", "execution_count": 7, "id": "6c14442e", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "import numpy as np\n", "\n", "#set plot resolution\n", "#%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure size\n", "#plt.rcParams['figure.figsize'] = (9,6)\n"]}, {"cell_type": "markdown", "id": "6ead8fc5", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_6_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L6.1 Introduction to Confidence Intervals</h2>  \n", "\n", "| [Top](#section_6_0) | [Previous Section](#section_6_0) | [Exercises](#exercises_6_1) | [Next Section](#section_6_2) |\n"]}, {"cell_type": "markdown", "id": "147c804d", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section."]}, {"cell_type": "code", "execution_count": 1, "id": "5a00a6d9", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"data": {"text/html": ["\n", "        <iframe\n", "            width=\"970\"\n", "            height=\"550\"\n", "            src=\"https://mitx-8s50.github.io/slides/L06/slides_L06_01.html\"\n", "            frameborder=\"0\"\n", "            allowfullscreen\n", "            \n", "        ></iframe>\n", "        "], "text/plain": ["<IPython.lib.display.IFrame at 0x10d3e1f40>"]}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": ["#>>>RUN\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L06/slides_L06_01.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "f0f9fe2a", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "For the past few lectures, we have been considering uncertainties that we have defined to be the variance of a distribution, but what exactly do they mean. Lets go back to the p-value definition, given a pdf $p\\left(x|\\theta\\right)$\n", "\\begin{eqnarray}\n", "P(x|\\theta,x\\in \\Delta) & = &  \\int_{x}^{x+\\Delta} p\\left(x|\\theta\\right) dx \\\\\n", "P_{right}(x|\\theta, x\\leq x_{0}) & = &  \\int_{-\\infty}^{x} p\\left(x|\\theta\\right) dx\n", "\\end{eqnarray}\n", "The bottom integral is the cumulative distribution function (cdf), we can use this to derive relationships to the variances of distributions. Simply put we can compute the probability within various intervals of a distribution.  For the normal (gaussian) distribution variable $x$, we typically transform $x$ so that it can be written in temrs of a nomral distribution of unit $1$.\n", "\\begin{equation}\n", "\\mathcal{N}(0,1) = \\frac{x-\\bar{x}}{\\sigma}\n", "\\end{equation}\n", "Let's play around with a few things. "]}, {"cell_type": "code", "execution_count": 20, "id": "aef76fa4", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "from scipy import stats\n", "\n", "pM1=stats.norm.cdf(-2)\n", "p1=stats.norm.cdf(2)\n", "print(1-p1)"]}, {"cell_type": "markdown", "id": "d8bc3f46", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_6_1'></a>     \n", "\n", "| [Top](#section_6_0) | [Restart Section](#section_6_1) | [Next Section](#section_6_2) |\n"]}, {"cell_type": "markdown", "id": "c69e36aa", "metadata": {"tags": ["learner", "md", "catsoop_01"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 6.1.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 13, "id": "821c85c4", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "913e1ab0", "metadata": {"tags": ["learner", "catsoop_01", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "7f31256a", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_6_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L6.2 z-Scores and Confidence Intervals for Other Distributions</h2>  \n", "\n", "| [Top](#section_6_0) | [Previous Section](#section_6_1) | [Exercises](#exercises_6_2) | [Next Section](#section_6_3) |\n"]}, {"cell_type": "markdown", "id": "2b3b157b", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "TEXT\n"]}, {"cell_type": "code", "execution_count": 20, "id": "08027069", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "from scipy import stats\n", "\n", "#Lets do some integrals\n", "p50=stats.norm.cdf(0)\n", "p1=stats.norm.cdf(1)\n", "p2=stats.norm.cdf(2)\n", "p3=stats.norm.cdf(3)\n", "p5=stats.norm.cdf(5)\n", "pM1=stats.norm.cdf(-1)\n", "pM2=stats.norm.cdf(-2)\n", "pM3=stats.norm.cdf(-3)\n", "pM5=stats.norm.cdf(-5)\n", "#print(p50,p1,p2,p3,pM1,pM2,pM3)\n", "\n", "#Whats the probability of things fluctuation more that 1\\sigma\n", "print(p1-pM1,\"within 1 standard deviations\")\n", "print(p2-pM2,\"within 2 standard deviations\")\n", "print(p3-pM3,\"within 3 standard deviations\")\n", "print(p5-pM5,\"within 5 standard deviations\")\n", "\n", "#Sometimes we only consider 1-sided p-values\n", "print((1.-p1),\"to fluctuate above 1 standard deviation\")\n", "print((1.-p3),\"to fluctuate above 3 standard deviation\")\n", "print((1.-p5),\"to fluctuate above 5 standard deviation\")"]}, {"cell_type": "markdown", "id": "bf4802bf", "metadata": {"tags": ["learner", "md"]}, "source": ["These probability values define what we call confidence intervals. We often also write these as z-scores namely for a measurement the z-score is the probability that a measurement is within $z$ standard deviations of a distribution. \n", "\\begin{equation}\n", "\\bar{x}\\pm z \\sigma\n", "\\end{equation}\n", "Z-scores are often synonymously considered in the context of gaussian distributions with z-scores corresponding to 68%,95%, and 99.75% for 1,2, and 3$\\sigma$ deviations. However, for more complicated distributions the approach is a bit different. "]}, {"cell_type": "markdown", "id": "3ef38971", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Asymmetric distributions</h3>\n", "\n", "Now above we considered the simplest scenario where we looked at the variation of distributions, which we assumed were symmetrically distributed about zero. What if these distributions are asymmetric, how do we define the variations? Let's take a look at some asymmetric distributions. "]}, {"cell_type": "code", "execution_count": 20, "id": "5a59b9e9", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["df=9\n", "stats.norm.cdf(1.97)"]}, {"cell_type": "code", "execution_count": 20, "id": "0e2a9697", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "#code stolen from here: https://aegis4048.github.io/comprehensive_confidence_intervals_for_python_developers\n", "\n", "#Lets plot a chi2 with 9 degrees of freedom (df)\n", "df = 9 \n", "x = np.linspace(-1, 28, 1000)\n", "y = stats.chi2.pdf(x, df, loc=0, scale=1)\n", "\n", "# two-tailed\n", "#Note we will use this function percent point function(ppf), \n", "#which inverts the cdf and gives a z from a proabability\n", "two_right_tail = stats.chi2.ppf(1 - 0.025, df) #left value\n", "two_left_tail  = stats.chi2.ppf(1 - 0.975, df) #right value\n", "print(\"two tail values:\",two_right_tail,two_left_tail)\n", "\n", "# one tailed\n", "one_right_tail = stats.chi2.ppf(1 - 0.05, df)\n", "one_left_tail  = stats.chi2.ppf(1 - 0.95, df)\n", "print(\"one tail values:\",one_right_tail,one_left_tail)\n", "\n", "plt.style.use('seaborn-whitegrid')\n", "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n", "\n", "for ax in axes:\n", "    ax.plot(x, y, c='black')\n", "    ax.grid(False)\n", "    #ax.xaxis.set_major_formatter(plt.NullFormatter())\n", "    #ax.yaxis.set_major_formatter(plt.NullFormatter())\n", "\n", "#now lets fill this from the left\n", "axes[0].fill_between(x, 0, y, where=(np.array(x) > min(x)) & (np.array(x) <= two_left_tail), facecolor='grey')\n", "axes[0].fill_between(x, 0, y, where=(np.array(x) > two_left_tail) & (np.array(x) < two_right_tail), facecolor='lightgrey')\n", "axes[0].fill_between(x, 0, y, where=(np.array(x) > two_right_tail) & (np.array(x) <= max(x)), facecolor='grey')\n", "\n", "axes[1].fill_between(x, 0, y, where=(np.array(x) > min(x)) & (np.array(x) < one_right_tail), facecolor='lightgrey')\n", "axes[1].fill_between(x, 0, y, where=(np.array(x) > one_right_tail) & (np.array(x) <= max(x)), facecolor='grey')\n", "\n", "axes[2].fill_between(x, 0, y, where=(np.array(x) > min(x)) & (np.array(x) <= one_left_tail), facecolor='grey')\n", "axes[2].fill_between(x, 0, y, where=(np.array(x) > one_left_tail) & (np.array(x) <= max(x)), facecolor='lightgrey')\n", "\n", "fig.tight_layout()"]}, {"cell_type": "markdown", "id": "5466b43b", "metadata": {"tags": ["learner", "md"]}, "source": ["Now that we have taken a look a these distributions, we see a clear asymmetry in the p-values of the left and right. What is conserved is the integrals, but the values are different. Lets go ahead and compute the mean and variance of these distributions. To do this, we will sample a distribution using some new functions, this rvs (\"random variate samples\"). "]}, {"cell_type": "code", "execution_count": 20, "id": "3e17a644", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#Lets compute the mean and RMS of a sample from this distribution\n", "df=9\n", "#Lets sample this distribution\n", "y_chi2 = stats.chi2.rvs(size=1000,df=df)\n", "print(y_chi2[0:5]) #print some valued\n", "print(\"Sampled Mean:\",y_chi2.mean(),\"Sampled Stddev:\",y_chi2.std())\n", "\n", "z=1.5 #Lets deviation corresponding to 1.5sigma in a gauassian\n", "x = np.linspace(-1, 28, 1000)\n", "y = stats.chi2.pdf(x, df, loc=0, scale=1)\n", "two_right_tail = stats.chi2.ppf(1 - stats.norm.cdf(-z), df)\n", "two_left_tail = stats.chi2.ppf(1 - stats.norm.cdf(z), df)\n", "\n", "def plotItAll(x,y,y_chi2,z):\n", "    #Now lets plot the filled area using the true pdfs and the assumed variations if it were a gaussian\n", "    plt.style.use('seaborn-whitegrid')\n", "    #plot distribution\n", "    plt.plot(x, y, c='black',label=\"$\\chi^2$\")\n", "    #plot chi2\n", "    plt.hist(y_chi2, histtype='stepfilled', edgecolor='k', alpha=0.4, color='gray', density=True,bins=20,label=\"events\")\n", "    #true values\n", "    plt.fill_between(x, 0, y, where=(np.array(x) > min(x)) & (np.array(x) <= two_left_tail), facecolor='grey')\n", "    plt.fill_between(x, 0, y, where=(np.array(x) > two_right_tail) & (np.array(x) <= max(x)), facecolor='grey')\n", "    #Mean +/- 1 sigma\n", "    plt.axvline(y_chi2.mean(), c='red',label=\"mean\")\n", "    plt.axvline(y_chi2.mean()+y_chi2.std()*z, c='blue',label=\"+/-$\\sigma_{gaus}$\")\n", "    plt.axvline(y_chi2.mean()-y_chi2.std()*z, c='blue')\n", "    plt.xlabel(\"x\")\n", "    plt.ylabel(\"$\\chi^{2}$\")\n", "    plt.legend(loc='upper right')\n", "    plt.show()\n", "plotItAll(x,y,y_chi2,z)"]}, {"cell_type": "markdown", "id": "82dc07b0", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_6_2'></a>     \n", "\n", "| [Top](#section_6_0) | [Restart Section](#section_6_2) | [Next Section](#section_6_3) |\n"]}, {"cell_type": "markdown", "id": "058e5602", "metadata": {"tags": ["learner", "md", "catsoop_02"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 6.2.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 23, "id": "91ae8bd5", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "65ccc5d0", "metadata": {"tags": ["learner", "catsoop_02", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "898ef830", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_6_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L6.3 Another Example and Rules of Significance</h2>  \n", "\n", "| [Top](#section_6_0) | [Previous Section](#section_6_2) | [Exercises](#exercises_6_3) | [Next Section](#section_6_4) |\n"]}, {"cell_type": "markdown", "id": "bd2c26e3", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section."]}, {"cell_type": "code", "execution_count": 2, "id": "29a442a9", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"data": {"text/html": ["\n", "        <iframe\n", "            width=\"970\"\n", "            height=\"550\"\n", "            src=\"https://mitx-8s50.github.io/slides/L06/slides_L06_03.html\"\n", "            frameborder=\"0\"\n", "            allowfullscreen\n", "            \n", "        ></iframe>\n", "        "], "text/plain": ["<IPython.lib.display.IFrame at 0x10f3d4b80>"]}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": ["#>>>RUN\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L06/slides_L06_03.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "2677e77a", "metadata": {"tags": ["learner", "md"]}, "source": ["### Challenge Question\n", "\n", "Lets consider a Cauchy distribution presented [here](https://en.wikipedia.org/wiki/Cauchy_distribution) and given by the form\n", "\\begin{equation}\n", "f(x;x_{0},\\gamma) = \\frac{1}{\\pi \\gamma} \\left(\\frac{1}{\\left(x-x_0\\right)^{2} + \\gamma^{2}}\\right)\n", "\\end{equation}\n", "available also in scipy stats with ```scipy.stats.cauchy```. For this function compare the gaussian 1 standard deviation with the true Cauchy p-value"]}, {"cell_type": "code", "execution_count": 20, "id": "99436b54", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#Lets try this with an even crazier distribution\n", "fig, ax = plt.subplots(figsize=(12, 6))\n", "plt.xlim([-300,300])\n", "y_cauchy = stats.cauchy.rvs(size=10000)\n", "print(y_cauchy.mean(),y_cauchy.std())\n", "\n", "#choose 1-sigma z-score\n", "z=1.0\n", "two_right_tail = stats.cauchy.ppf(1 - stats.norm.cdf(-z))\n", "two_left_tail = stats.cauchy.ppf(1 - stats.norm.cdf(z))\n", "\n", "#plot distribution\n", "x = np.linspace(-10, 10, 1000)\n", "y = stats.cauchy.pdf(x, loc=0, scale=1)\n", "plt.plot(x, y, c='black',label=\"f\")\n", "plt.hist(y_cauchy, histtype='stepfilled', edgecolor='k', alpha=0.4, color='gray', density=True,bins=10000)\n", "#plot true-sigma\n", "plt.fill_between(x, 0, y, where=(np.array(x) > min(x)) & (np.array(x) <= two_left_tail), facecolor='grey',label='+/- true $\\sigma$')\n", "plt.fill_between(x, 0, y, where=(np.array(x) > two_right_tail) & (np.array(x) <= max(x)), facecolor='grey')\n", "plt.axvline(y_cauchy.mean(), c='red',label=\"mean\")\n", "plt.axvline(y_cauchy.mean()+y_cauchy.std()*z, c='blue',label='+/- gaus $\\sigma$')\n", "plt.axvline(y_cauchy.mean()-y_cauchy.std()*z, c='blue')\n", "plt.xlabel(\"x\")\n", "plt.ylabel(\"f(x)\")\n", "plt.legend(loc='upper right')\n", "plt.yscale('log')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "8a5d6427", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_6_3'></a>     \n", "\n", "| [Top](#section_6_0) | [Restart Section](#section_6_3) | [Next Section](#section_6_4) |\n"]}, {"cell_type": "markdown", "id": "881296a2", "metadata": {"tags": ["learner", "md", "catsoop_03"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 6.3.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 33, "id": "a13de900", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "ab641c01", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "d4dcf05c", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_6_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L6.4 Moments of Distributions and Mapping</h2>  \n", "\n", "| [Top](#section_6_0) | [Previous Section](#section_6_3) | [Exercises](#exercises_6_4) | [Next Section](#section_6_5) |\n"]}, {"cell_type": "markdown", "id": "906fa341", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section."]}, {"cell_type": "code", "execution_count": 3, "id": "946265e5", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"data": {"text/html": ["\n", "        <iframe\n", "            width=\"970\"\n", "            height=\"550\"\n", "            src=\"images/slides_L06_04.html\"\n", "            frameborder=\"0\"\n", "            allowfullscreen\n", "            \n", "        ></iframe>\n", "        "], "text/plain": ["<IPython.lib.display.IFrame at 0x1110d56a0>"]}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": ["#>>>RUN\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L06/slides_L06_04.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "4d705679", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Moments of distributions</h3>\n", "\n", "So you see that with certain types of distributions things can deviate wildly from what is expected. Namely in the case of a cauchy or asymmetric distribution, the standard deviation of a data sample is not a reflection of the p-value of the distribution at all. To describe this distribution there are a number of ways to mitigate this issue. The first is to introduce higher order moments of a pdf. \n", "\n", "Let's assume our distributions are centered about zero (we can always re center them later). Then we can write moments as\n", "\\begin{equation}\n", " \\mu_{n}=m^{n}(x)=E[x^{n}p(x)] = \\int_{-\\infty}^{\\infty} x^{n} p(x) dx\n", "\\end{equation}\n", "\n", "The mean is $E[p(x)]=m^{1}(x)$, and the variance of the distribution is just $V[p(x)]=m^{2}(x)$. These you should know and love by now, but we can keep going in moments to get more distributions. The next moment is skewness, this tells you how asymmetric a distribution is; $\\mathrm{Skew}=m^{3}(x)$, and going further we have the $\\mathrm{Kurtosis}=m^{4}(x)$ this tells you how important the tails of a distribution are. \n", "\n", "There are lots of distribtuions in data. Here is a \"story book of distributions\":\n", "http://bois.caltech.edu/dist_stories/t3b_probability_stories.html#Cauchy-distribution\n", "\n", "Let's take a look at the moments of our two example distributions above. "]}, {"cell_type": "code", "execution_count": 20, "id": "48e9cb10", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["def raw_moment(X, k, c=0):\n", "    return ((X - c)**k).mean()\n", "\n", "def central_moment(X, k):\n", "    if k == 1:\n", "        return X.mean()\n", "    return raw_moment(X=X, k=k, c=X.mean())\n", "\n", "def print_moments(X,label):\n", "    print(label+\" mean:\",central_moment(X,1))\n", "    print(label+\" var:\" ,central_moment(X,2))\n", "    print(label+\" skew:\",central_moment(X,3))\n", "    print(label+\" kurtosis:\",central_moment(X,4))\n", "\n", "N=1000000\n", "y_norm = stats.norm.rvs(size=N)\n", "print_moments(y_norm,\"normal\")\n", "\n", "df=9\n", "y_chi2 = stats.chi2.rvs(size=N,df=df)\n", "print_moments(y_chi2,\"chi2 df \"+str(df))\n", "\n", "y_cauchy = stats.cauchy.rvs(size=N)\n", "print_moments(y_cauchy,\"cauchy\")\n"]}, {"cell_type": "markdown", "id": "bc99c2e0", "metadata": {"tags": ["learner", "md"]}, "source": ["Now, the nice thing about moments is that you can use these values to map any distribtuion to any other distribution. \n", "I am not really doing justice to the above, but moments are in fact incredibly useful. A chunk of my PhD thesis is dedicated to using moments of a Gaussian (cumulants) to model distributions with a small amount of data. \n", "\n", "In light of this, what we can imagine doing, like a fourier transform, you can transform one distribution into another through the use of the CDFs. The strategy here is to match quantiles with each other, such that you have for a transform from $x\\rightarrow x^{\\prime}$:\n", "\n", "\\begin{equation}\n", "\\int_{-\\infty}^{x} p_1(x) dx = \\int_{-\\infty}^{x^\\prime} p_2(x^{\\prime}) dx^{\\prime}\n", "\\end{equation}\n", "\n", "To do this mapping there are many different approaches, one such approach of mapping distributions is the [box-cox method](https://en.wikipedia.org/wiki/Power_transform), which aims to use the moments to transform distributions into Gaussians.\n"]}, {"cell_type": "code", "execution_count": 20, "id": "e9b42058", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["x = stats.lognorm.rvs(s=1, loc=0, scale=5, size=1000, random_state=4)\n", "\n", "# plot\n", "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n", "axes[0].hist(x)\n", "xaxis = np.linspace(0, 80, 1000)\n", "axes[0].plot(xaxis,len(x)*10*stats.lognorm.pdf(xaxis,s=1, loc=0, scale=5))\n", "stats.probplot(x, dist=stats.norm, plot=axes[1])\n", "fig.tight_layout()\n", "\n", "# box-cox transform\n", "xt, lmbda = stats.boxcox(x)\n", "\n", "# plot\n", "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n", "axes[0].hist(xt)\n", "xaxis = np.linspace(-2, 5, 1000)\n", "axes[0].plot(xaxis,len(x)*0.6*stats.norm.pdf(xaxis,1.7,1))\n", "\n", "stats.probplot(xt, dist=stats.norm, plot=axes[1])\n", "fig.tight_layout()"]}, {"cell_type": "markdown", "id": "52b8ff71", "metadata": {"tags": ["learner", "md"]}, "source": ["These types of transforms are particularly useful when are trying to match a simulated distribution with a data distribution. If both the simulated and true distributions are trying to describe the same thing, you can use a transform like this to map one distribution onto another. "]}, {"cell_type": "markdown", "id": "d96ceae0", "metadata": {"tags": ["learner", "md"]}, "source": ["### Challenge Question\n", "Take the absolute value of a cauchy distribution and map it onto a gaussian. Approximately, where is zero?"]}, {"cell_type": "code", "execution_count": 20, "id": "2e850942", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["x = np.abs(stats.cauchy.rvs(size=10000))\n", "\n", "# plot\n", "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n", "axes[0].hist(x)\n", "xaxis = np.linspace(-80, 80, 1000)\n", "axes[0].plot(xaxis,len(x)*10*stats.cauchy.pdf(xaxis))\n", "stats.probplot(x, dist=stats.norm, plot=axes[1])\n", "fig.tight_layout()\n", "\n", "# box-cox transform\n", "xt, lmbda = stats.boxcox(x)\n", "\n", "# plot\n", "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n", "axes[0].hist(xt)\n", "xaxis = np.linspace(-2, 5, 1000)\n", "axes[0].plot(xaxis,len(x)*0.6*stats.norm.pdf(xaxis,0,1))\n", "\n", "stats.probplot(xt, dist=stats.norm, plot=axes[1])\n", "fig.tight_layout()"]}, {"cell_type": "markdown", "id": "caf20297", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_6_4'></a>     \n", "\n", "| [Top](#section_6_0) | [Restart Section](#section_6_4) | [Next Section](#section_6_5) |\n"]}, {"cell_type": "markdown", "id": "b59093ce", "metadata": {"tags": ["learner", "md", "catsoop_04"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 6.4.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 43, "id": "1e0d27a0", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "6268ba7f", "metadata": {"tags": ["learner", "catsoop_04", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "8b9aa448", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_6_5'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L6.5 Monte Carlo Integration</h2>  \n", "\n", "| [Top](#section_6_0) | [Previous Section](#section_6_4) | [Exercises](#exercises_6_5) | [Next Section](#section_6_6) |\n"]}, {"cell_type": "markdown", "id": "d505de43", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section."]}, {"cell_type": "code", "execution_count": 5, "id": "1789f406", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"data": {"text/html": ["\n", "        <iframe\n", "            width=\"970\"\n", "            height=\"550\"\n", "            src=\"images/slides_L06_05.html\"\n", "            frameborder=\"0\"\n", "            allowfullscreen\n", "            \n", "        ></iframe>\n", "        "], "text/plain": ["<IPython.lib.display.IFrame at 0x1117bdcd0>"]}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": ["#>>>RUN\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L06/slides_L06_05.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "c0282f08", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Numerical Integration</h3>\n", "\n", "You have probably noticed that I didn't bother to compute integrals to get the moments. Instead I did what is referred to as Monte-Carlo Integration (or more generally bootstrapping) where I just sampled a distribution and integrated by sampling. Namely \n", "\\begin{equation}\n", "E[x^{n}p(x)] = \\int_{-\\infty}^{\\infty} x^{n} p(x) dx = \\frac{1}{N}\\sum_{i=1}^{N} x_{i}^{n}\n", "\\end{equation}\n", "Where here the $x_{i}\\in p(x)$ are sampled from the probability distribution function. So what happens, when we have a distribution, but we don't know the analytic form. How can we sample it?  \n", "\n", "There are a lot of ways to do this, perhaps the best well known is Markov Chain Monte Carlo (MCMC). However the simplest is to just turn our distribution into a 2D image and random sample points on the image. Instead of writing the points out lets just do it. \n", "\n", "First lets integrate a quarter circle, we know that the integral is given by $A=\\frac{\\pi}{4}$.\n"]}, {"cell_type": "code", "execution_count": 20, "id": "4a146e1c", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["import math\n", "\n", "#First lets just compute the area of a quarter circle with radius 1\n", "def quarterarea(iN):\n", "    area=0\n", "    lXin = np.array([])\n", "    lYin = np.array([])\n", "    lXout = np.array([])\n", "    lYout = np.array([])\n", "    for i0 in range(iN):\n", "        #Sample X and Y\n", "        pX = np.random.uniform(0,1)\n", "        pY = np.random.uniform(0,1)\n", "        #Check if its radius is in 1\n", "        if math.sqrt(pX**2+pY**2) < 1:\n", "            lXin = np.append(lXin,pX)\n", "            lYin = np.append(lYin,pY)\n", "            area += 1 # count it\n", "        else:\n", "            lXout = np.append(lXout,pX)\n", "            lYout = np.append(lYout,pY)\n", "    return (float(area)/float(iN)),lXin,lYin,lXout,lYout\n", "\n", "#sample points\n", "lN=100000\n", "a,lXin,lYin,lXout,lYout=quarterarea(lN)\n", "print(\"Pi (4*area):\",a*4,\"+/-\",4*a/math.sqrt(lN)) #gotta put an uncertainty\n", "plt.plot(lXin,lYin,marker='.',linestyle = 'None')\n", "plt.plot(lXout,lYout,marker='.',linestyle = 'None')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "9b024d5b", "metadata": {"tags": ["learner", "md"]}, "source": ["The idea with Monte Carlo integration is that we calculate an integral by evaluating the function. We don't actually have to compute the integral. This avoids what is potentially a very complicated step. "]}, {"cell_type": "code", "execution_count": 20, "id": "b584e82f", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["from scipy import optimize as opt \n", "\n", "#Now lets consider integrating some random function\n", "def f(x):\n", "    return x**4 + 3*(x-2)**3 - 15*(x)**2 + 1\n", "\n", "def fneg(x):\n", "    return -1*(x**4 + 3*(x-2)**3 - 15*(x)**2 + 1)\n", "\n", "#First thing is to define a range in x\n", "xmin=-6\n", "xmax=3\n", "x = np.linspace(xmin, xmax, 100)\n", "#plt.plot(x, f(x));\n", "\n", "#Now we need to find a range in y\n", "sol=opt.minimize_scalar(f,bounds=(xmin, xmax), method='Brent')\n", "ymin=sol.fun\n", "#y-max is to get the minimum of negative f\n", "sol=opt.minimize_scalar(fneg,bounds=(xmin, xmax), method='Brent')\n", "ymax=-1*sol.fun\n", "n=1000\n", "#now, lets sample a 2D grid y-min and y-max and compute the integral\n", "lXin = np.array([])\n", "lYin = np.array([])\n", "lXout = np.array([])\n", "lYout = np.array([])\n", "for i0 in range(lN):\n", "    pX = abs(xmax-xmin)*np.random.normal(0,1)+xmin\n", "    pY = abs(ymax-ymin)*np.random.normal(0,1)+ymin\n", "    pYMin = f(pX)\n", "    if pY < pYMin:\n", "        lXin = np.append(lXin,pX)\n", "        lYin = np.append(lYin,pY)\n", "    else:\n", "        lXout = np.append(lXout,pX)\n", "        lYout = np.append(lYout,pY)\n", "\n", "plt.plot(x, f(x));\n", "plt.plot(lXin,lYin,marker='.',linestyle = 'None')\n", "plt.plot(lXout,lYout,marker='.',linestyle = 'None')\n", "plt.axvline(sol.x, c='red')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "cc2ac1c0", "metadata": {"tags": ["learner", "md"]}, "source": ["From the above scenario, we can see that the orange points are below the line from roughly -800 to 0, and the green points are above the line. We can make a histogram of this."]}, {"cell_type": "code", "execution_count": 20, "id": "0fe42adf", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["plt.hist(lXin,alpha=0.5,density=True)\n", "plt.hist(lXout,alpha=0.5,density=True)\n", "plt.show()\n", "\n", "print(\"Integral above:\",len(lXin),\"Integral below:\",len(lXout))"]}, {"cell_type": "markdown", "id": "22716f30", "metadata": {"tags": ["learner", "md"]}, "source": ["This procedure is known as \"Area-based\" sampling, and is considered a method of Monte-Carlo Integration. Monte-Carlo integration is a rich field. All high energy physics simulations are based on it. Basically our function we sample from starts with a collision and computing the probability that this could be any other collision. We then proceed to put this single collision through a point by point simulation, of each particle going through all the detectors. Finally we aggregate our distributions based on this. You will see the usefulness of Monte Carlo Simulation next week. \n", "\n", "There is a lot more you can do see [here](https://colab.research.google.com/drive/1nU4E_pFWjSFPNigzy8-LcJpef26IKY6x?usp=sharing)\n", "\n", "Ok, so I have just dumped a whole toolkit of statistical tools on you after we did our original fit to the expansion of the universe. Lets go back and analyze our fit again and see if we can make some conclusions. Recall that we started the previous lecture by looking at the fit residuals. Let's now do this. However, now let's divide by the uncertainty in each measured point, to see how well our fit is behaving. "]}, {"cell_type": "markdown", "id": "ae518ffa", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_6_5'></a>     \n", "\n", "| [Top](#section_6_0) | [Restart Section](#section_6_5) | [Next Section](#section_6_6) |\n"]}, {"cell_type": "markdown", "id": "8affd2c2", "metadata": {"tags": ["learner", "md", "catsoop_05"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 6.5.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 53, "id": "0670e593", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "e6690133", "metadata": {"tags": ["learner", "catsoop_05", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "05685850", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_6_6'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L6.6 Returning to Super Nova Data Fitting</h2>  \n", "\n", "| [Top](#section_6_0) | [Previous Section](#section_6_5) | [Exercises](#exercises_6_6) | [Next Section](#section_6_7) |\n"]}, {"cell_type": "markdown", "id": "6923a57d", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>A more sophisticated fit</h3>\n", "\n", "Now that we have gone on an excursion to understand properties of fits, lets go ahead an analyze our supernovae data, and try to pull in all of the info that we can. Lets first look at our linear fit.  One sec, while we load it all. \n"]}, {"cell_type": "code", "execution_count": 60, "id": "430851a8", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "import csv\n", "#Lets try to understand how good the fits we made in last class are, lets load the supernova data again\n", "label='sn_z_mu_dmu_plow_union2.1.txt'\n", "def distanceconv(iMu):\n", "    power=iMu/5+1\n", "    return 10**power\n", "\n", "def distanceconverr(iMu,iMuErr):\n", "    power=iMu/5+1\n", "    const=math.log(10)/5.\n", "    return const*(10**power)*iMuErr\n", "\n", "def load(iLabel,iMaxZ=0.1):\n", "    redshift=np.array([])\n", "    distance=np.array([])\n", "    distance_err=np.array([])\n", "    with open(iLabel,'r') as csvfile:\n", "        plots = csv.reader(csvfile, delimiter='\\t')\n", "        for row in plots:\n", "            if float(row[1]) > iMaxZ:\n", "                continue\n", "            redshift = np.append(redshift,float(row[1]))\n", "            distance = np.append(distance,distanceconv(float(row[2])))\n", "            distance_err = np.append(distance_err,distanceconverr(float(row[2]),float(row[3])))\n", "    return redshift,distance,distance_err  \n", "        \n", "#Now lets run the regression again\n", "#Lets run the regression again\n", "def variance(isamples):\n", "    mean=isamples.mean()\n", "    n=len(isamples)\n", "    tot=0\n", "    for pVal in isamples:\n", "        tot+=(pVal-mean)**2\n", "    return tot/n\n", "\n", "def covariance(ixs,iys):\n", "    meanx=ixs.mean()\n", "    meany=iys.mean()\n", "    n=len(ixs)\n", "    tot=0\n", "    for i0 in range(len(ixs)):\n", "        tot+=(ixs[i0]-meanx)*(iys[i0]-meany)\n", "    return tot/n\n", "\n", "def linear(ix,ia,ib):\n", "    return ia*ix+ib\n", "\n", "redshift,distance,distance_err=load(label)\n", "var=variance(redshift)\n", "cov=covariance(redshift,distance)\n", "A=cov/var\n", "const=distance.mean()-A*redshift.mean()\n", "xvals = np.linspace(0,0.1,100)\n", "yvals = []\n", "for pX in xvals:\n", "    yvals.append(linear(pX,A,const))\n", "\n", "plt.plot(xvals,yvals)\n", "plt.errorbar(redshift,distance,yerr=distance_err,marker='.',linestyle = 'None', color = 'black')\n", "plt.xlabel(\"z(redshift)\")\n", "plt.ylabel(\"distance(pc)\")\n", "plt.show()"]}, {"cell_type": "markdown", "id": "fd3464e3", "metadata": {"tags": ["learner", "md"]}, "source": ["Now that we have loaded, lets actually look at the residuals. "]}, {"cell_type": "code", "execution_count": 60, "id": "bd561696", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["def residualsComp(redshift,distance,distance_err):\n", "    #Compute residuals\n", "    residuals=np.array([])\n", "    for i0 in range(len(redshift)):\n", "        pResid=linear(redshift[i0],A,const)-distance[i0]\n", "        residuals = np.append(residuals,pResid/distance_err[i0])\n", "    #Make a hsitogram\n", "    y0, bin_edges = np.histogram(residuals, bins=30)\n", "    bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n", "    norm0=len(residuals)*(bin_edges[-1]-bin_edges[0])/30.\n", "    plt.errorbar(bin_centers,y0/norm0,yerr=y0**0.5/norm0,marker='.',drawstyle = 'steps-mid')\n", "    #Plot a gaussian\n", "    k=np.arange(bin_edges[0],bin_edges[-1],0.05)\n", "    normal=stats.norm.pdf(k,0,1)\n", "    #First lets look at the moments \n", "    normalpoints=stats.norm.rvs(0,1,1000)\n", "    print_moments(residuals,\"residuals\")\n", "    print_moments(normalpoints,\"normal distribution\")\n", "\n", "    #Now lets plot it\n", "    plt.plot(k,normal,'o-')\n", "    plt.xlabel(\"number of successes\")\n", "    plt.ylabel(\"probability\")\n", "    plt.show()\n", "    return residuals\n", "residuals=residualsComp(redshift,distance,distance_err)"]}, {"cell_type": "markdown", "id": "f77f4886", "metadata": {"tags": ["learner", "md"]}, "source": ["What we see is that the mean of the resiudals is very lose to 0 with a variance of 1, and a skew and kurtosis really close to a normal distribtuion. This looks likes our residuals are gaussian. What do you think that means for our fit? \n", "\n", "Lets check it out. "]}, {"cell_type": "code", "execution_count": 60, "id": "55c8564d", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#now lets look at the chi2\n", "chi2=np.sum(residuals**2)\n", "\n", "print(\"Total chi2:\",chi2,\"NDOF\",len(residuals)-2)\n", "print(\"Normalized chi2:\",chi2/(len(residuals)-2))\n", "print(\"Probability of chi2:\",1-stats.chi2.cdf(chi2,(len(residuals)-2)))\n", "\n", "\n", "#Lets plot it for good measure too\n", "x = np.linspace(0,len(residuals)*2)\n", "chi2d=stats.chi2.pdf(x,len(residuals-2)) # 40 bins\n", "plt.plot(x,chi2d,label='chi2')\n", "plt.axvline(chi2, c='red')\n", "plt.legend(loc='lower right')\n", "\n", "ndof=(len(residuals)-2)\n", "chi2ppf0=stats.chi2.ppf(0.5,ndof)\n", "chi2ppf1=stats.chi2.ppf(0.025,ndof)\n", "chi2ppf2=stats.chi2.ppf(1-0.025,ndof)\n", "print(\"Central Vvalue\",chi2ppf0)\n", "print(\"Sigma Low\",chi2ppf1-chi2ppf0)\n", "print(\"Sigma High\",chi2ppf2-chi2ppf0)\n", "\n", "plt.axvline(chi2ppf1, c='blue')\n", "plt.axvline(chi2ppf2, c='blue')"]}, {"cell_type": "markdown", "id": "f5e416f7", "metadata": {"tags": ["learner", "md"]}, "source": ["We see that the $\\chi^{2}$ value is very close to the number of degrees of freedom, with a normalized $\\chi^{2}/NDF \\approx1$, that is a legitimately good fit! Lets loosen the data and have you try another fit. "]}, {"cell_type": "markdown", "id": "2f6d29c3", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_6_6'></a>     \n", "\n", "| [Top](#section_6_0) | [Restart Section](#section_6_6) | [Next Section](#section_6_7) |\n"]}, {"cell_type": "markdown", "id": "25bb72fa", "metadata": {"tags": ["learner", "md", "catsoop_06"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 6.6.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 63, "id": "f066a61d", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "65b08afa", "metadata": {"tags": ["learner", "catsoop_06", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "39369163", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_6_7'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L6.7 Fitting with a More Accurate Model</h2>  \n", "\n", "| [Top](#section_6_0) | [Previous Section](#section_6_6) | [Exercises](#exercises_6_7) | [Next Section](#section_6_8) |\n"]}, {"cell_type": "markdown", "id": "9c61b920", "metadata": {"tags": ["learner", "md"]}, "source": ["### Challenge Question\n", "\n", "Repeat the linear fit above now without a cut on the redshift. What happens? "]}, {"cell_type": "code", "execution_count": 60, "id": "d55ad732", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#solution\n", "\n", "#run regression\n", "redshift,distance,distance_err=load(label,10000)\n", "var=variance(redshift)\n", "cov=covariance(redshift,distance)\n", "A=cov/var\n", "const=distance.mean()-A*redshift.mean()\n", "xvals = np.linspace(0,1.4,100)\n", "yvals = []\n", "for pX in xvals:\n", "    yvals.append(linear(pX,A,const))\n", "\n", "#plot it\n", "plt.plot(xvals,yvals)\n", "plt.errorbar(redshift,distance,yerr=distance_err,marker='.',linestyle = 'None', color = 'black')\n", "plt.xlabel(\"z(redshift)\")\n", "plt.ylabel(\"distance(pc)\")\n", "plt.show()\n", "\n", "residuals=residualsComp(redshift,distance,distance_err)\n", "#now lets look at the chi2\n", "chi2=np.sum(residuals**2)\n", "print(\"Total chi2:\",chi2,\"NDOF\",len(residuals)-2)\n", "print(\"Normalized chi2:\",chi2/(len(residuals)-2))\n", "print(\"Probability of chi2:\",1-stats.chi2.cdf(chi2,(len(residuals)-2)))\n", "\n", "#Lets plot it for good measure too\n", "x = np.linspace(0,len(residuals)*2)\n", "chi2d=stats.chi2.pdf(x,len(residuals-2)) # 40 bins\n", "plt.plot(x,chi2d,label='chi2')\n", "plt.axvline(chi2, c='red')\n", "plt.legend(loc='lower right')"]}, {"cell_type": "markdown", "id": "e59c2a60", "metadata": {"tags": ["learner", "md"]}, "source": ["This is what we technically call a \"Shit Fit\". We knew this already, but now we have put a barrage of statistical tools that confirmed our chi-by-eye  assesment. "]}, {"cell_type": "markdown", "id": "4da195aa", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>The properties of the universe</h3>\n", "\n", "Now let's go and run our fit function that included the parameters of the universe. Last time we did this with `lmfit`. This time, we are just going to do it with `scipy.optimize`, and we will go step-by-step through the fit and what it is doing. \n", "\n", "For the fit function, recall that we would like to fit the actual equation for the evolution fo the universe. "]}, {"cell_type": "code", "execution_count": 70, "id": "8fbf18ff", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "#We are not going to plot the fit first, lets just use our barrage of statistics to check if its ok\n", "def hubble(z,Om):\n", "    pVal=Om*(1+z)**3+(1.-Om)\n", "    return np.sqrt(pVal)\n", "\n", "def lumidistance(x,h0,Om):\n", "    integral=0\n", "    nint=100\n", "    for i0 in range(nint):\n", "        z=x*float(i0)/100.\n", "        dz=x/float(nint)\n", "        pVal=1./(1e-5+hubble(z,Om))\n", "        integral += pVal*dz\n", "    d=(1.+x)*integral*(1e6*3e5/h0)\n", "    return d\n", "\n", "def loglike(x):\n", "    lTot=0\n", "    for i0 in range(len(redshift)):\n", "        xtest=lumidistance(redshift[i0],x[0],x[1])\n", "        #lTot = lTot+(distance[i0]-xtest)**2\n", "        lTot = lTot+((1./distance_err[i0])**2)*(distance[i0]-xtest)**2\n", "    return lTot #*0.5 The above is 2 times loglike\n", "\n", "def residuals(x):\n", "    residuals=np.array([])\n", "    for i0 in range(len(redshift)):\n", "        pResid=lumidistance(redshift[i0],sol.x[0],sol.x[1])-distance[i0]\n", "        residuals = np.append(residuals,pResid/distance_err[i0])\n", "    return residuals\n"]}, {"cell_type": "markdown", "id": "7d5de616", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_6_7'></a>     \n", "\n", "| [Top](#section_6_0) | [Restart Section](#section_6_7) | [Next Section](#section_6_8) |\n"]}, {"cell_type": "markdown", "id": "507c72fb", "metadata": {"tags": ["learner", "md", "catsoop_07"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 6.7.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 73, "id": "e138c742", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "3f5ebae2", "metadata": {"tags": ["learner", "catsoop_07", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "39174ac6", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_6_8'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L6.8 Fit to Full Cosmological Model</h2>     \n", "\n", "| [Top](#section_6_0) | [Previous Section](#section_6_7) | [Exercises](#exercises_6_8) |\n"]}, {"cell_type": "markdown", "id": "c732c03b", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "TEXT\n"]}, {"cell_type": "code", "execution_count": 80, "id": "37bc0f8a", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "import lmfit\n", "\n", "print(lumidistance(0.5,70,0.3))\n", "model  = lmfit.Model(lumidistance)\n", "p = model.make_params(h0=70,Om=0.2)\n", "result = model.fit(data=distance, params=p, x=redshift, weights=1./distance_err)\n", "lmfit.report_fit(result)\n", "plt.figure()\n", "result.plot()"]}, {"cell_type": "markdown", "id": "66a5dbae", "metadata": {"tags": ["learner", "md"]}, "source": ["Ok, so this time we are going to minimize (negative of) loglikelihood, let's minimize the fit."]}, {"cell_type": "code", "execution_count": 80, "id": "95fe3b85", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "x0 = np.array([60.,0.2])\n", "ps = [x0]\n", "bnds = ((0, 1000), (0, 1.0))\n", "sol=opt.minimize(loglike, x0,bounds=bnds, tol=1e-6)\n", "print(sol)\n", "residuals=residuals(sol.x)\n", "print_moments(residuals,\"residuals\")\n", "chi2=np.sum(residuals**2)\n", "print(\"Total chi2:\",chi2,\"NDOF\",len(residuals)-2)\n", "print(\"Normalized chi2:\",chi2/(len(residuals)-2))\n", "print(\"Probability of chi2:\",1-stats.chi2.cdf(chi2,(len(residuals)-2)))\n", "\n", "#Lets plot it for good measure too\n", "x = np.linspace(0,len(residuals)*2)\n", "chi2d=stats.chi2.pdf(x,len(residuals-2)) # 40 bins\n", "plt.plot(x,chi2d,label='chi2')\n", "plt.axvline(chi2, c='red')\n", "plt.legend(loc='lower right')"]}, {"cell_type": "markdown", "id": "2d2b1de3", "metadata": {"tags": ["learner", "md"]}, "source": ["What can we say about this fit, is there something off?\n", "\n", "Let's plot the residuals, the fit function and scan the likelihood for our parameter uncertainties. There is one thing off, can you figure it out?"]}, {"cell_type": "code", "execution_count": 80, "id": "148dca0e", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "#Plot it against the data\n", "xvals = np.linspace(0,1.4,100)\n", "yvals = []\n", "for pX in xvals:\n", "    yvals.append(lumidistance(pX,sol.x[0],sol.x[1]))\n", "\n", "plt.errorbar(redshift,distance,yerr=distance_err,marker='.',linestyle = 'None', color = 'black')\n", "plt.plot(xvals,yvals)\n", "plt.show()\n", "\n", "#Histogram the residuals\n", "y0, bin_edges = np.histogram(residuals, bins=30)\n", "bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n", "norm0=len(residuals)*(bin_edges[-1]-bin_edges[0])/30.\n", "plt.errorbar(bin_centers,y0/norm0,yerr=y0**0.5/norm0,marker='.',drawstyle = 'steps-mid')\n", "k=np.arange(bin_edges[0],bin_edges[-1],0.05)\n", "normal=stats.norm.pdf(k,0,1)\n", "plt.plot(k,normal,'o-')\n", "plt.show()\n", "\n", "x = np.linspace(len(residuals)*0.5,len(residuals)*1.5)\n", "chi2d=stats.chi2.pdf(x,len(residuals-2)) # 40 bins\n", "plt.plot(x,chi2d,label='chi2')\n", "plt.axvline(chi2, c='red')\n", "plt.legend(loc='lower right')\n", "plt.show()\n", "\n", "\n"]}, {"cell_type": "markdown", "id": "93bbb52c", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_6_8'></a>   \n", "\n", "| [Top](#section_6_0) | [Restart Section](#section_6_8) |\n"]}, {"cell_type": "markdown", "id": "f8f3e664", "metadata": {"tags": ["learner", "md", "catsoop_08"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 6.8.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 83, "id": "653af647", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "0def0b91", "metadata": {"tags": ["learner", "catsoop_08", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}