{"cells": [{"cell_type": "markdown", "id": "405ab769", "metadata": {"tags": ["learner", "md"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 7: Correlations</h1>\n"]}, {"cell_type": "markdown", "id": "049bd965", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_7_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "346057f6", "metadata": {"tags": ["learner", "md"]}, "source": ["<table style=\"width:100%\">\n", "    <colgroup>\n", "       <col span=\"1\" style=\"width: 40%;\">\n", "       <col span=\"1\" style=\"width: 15%;\">\n", "       <col span=\"1\" style=\"width: 45%;\">\n", "    </colgroup>\n", "    <tr>\n", "        <th style=\"text-align: left; font-size: 13pt;\">Section</th>\n", "        <th style=\"text-align: left; font-size: 13pt;\">Exercises</th>\n", "        <th style=\"text-align: left; font-size: 13pt;\">Summary</th>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_1\">L7.1 Understanding Best Fit (Revisited)</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_1\">L7.1 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_2\">L7.2 Minimizing a Surface (1D Scan)</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_2\">L7.2 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_3\">L7.3 Minimizing a Surface (2D Scan)</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_3\">L7.3 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_4\">L7.4 Correlations Between Fit Parameters: Part 1</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_4\">L7.4 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_5\">L7.5 Correlations Between Fit Parameters: Part 2</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_5\">L7.5 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "</table>\n", "\n"]}, {"cell_type": "markdown", "id": "40b4dece", "metadata": {"tags": ["learner", "catsoop_00", "md"]}, "source": ["<h3>Learning Objectives</h3>\n", "\n", "Text needed\n"]}, {"cell_type": "markdown", "id": "18839162", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Importing Libraries</h3>\n", "\n", "Before beginning, run the cell below to import the relevant libraries for this notebook. \n", "Optionally, set the plot resolution and default figure size.\n"]}, {"cell_type": "code", "execution_count": 7, "id": "ddd927e2", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "import numpy as np\n", "\n", "#set plot resolution\n", "#%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure size\n", "#plt.rcParams['figure.figsize'] = (9,6)\n"]}, {"cell_type": "markdown", "id": "bab8d709", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_7_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.1 Understanding Best Fit (Revisited)</h2>  \n", "\n", "| [Top](#section_7_0) | [Previous Section](#section_7_0) | [Exercises](#exercises_7_1) | [Next Section](#section_7_2) |\n"]}, {"cell_type": "markdown", "id": "66626a36", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section."]}, {"cell_type": "code", "execution_count": 5, "id": "2edba24a", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"data": {"text/html": ["\n", "        <iframe\n", "            width=\"970\"\n", "            height=\"550\"\n", "            src=\"images/slides_L07_01.html\"\n", "            frameborder=\"0\"\n", "            allowfullscreen\n", "            \n", "        ></iframe>\n", "        "], "text/plain": ["<IPython.lib.display.IFrame at 0x111321490>"]}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": ["#>>>RUN\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L07/slides_L07_01.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "c289c79e", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Uncertainty on more than one parameter</h3>\n", "\n", "In the end of last week, we started to fit more than one parameter.  What does this mean? Well... this means we minimized two parameters instead of just one. If we have two parameters and they are both gaussian, then the sum of these two paramters is a $\\chi^{2}$ of now **2** degrees of freedom. From this, we can start to understand what is going on. \n", "\n", "Let's go to our fit, and perform all the steps of the minimization that `lmfit` just did, but by hand. To do this we will again minimize the function \n", "\\begin{equation}\n", " f(x,a,b) = a + b \\sin(x)\n", "\\end{equation}\n", "to the Auger data, which means that we need to find the mimimum of both $a$ and $b$, this means minimizing our likelihood for both $a$ and $b$. \n", "\n", "\\begin{equation}\n", " \\frac{\\partial \\mathcal{L}}{\\partial a} = 0 \\\\\n", " \\frac{\\partial \\mathcal{L}}{\\partial b} = 0 \\\\\n", "\\end{equation}  \n", "\n", "or writing this out using the $\\chi^{2}$ we have, taking $\\sigma = \\sqrt{x_{i}}$\n", "\n", "\\begin{eqnarray}\n", "\\chi^{2}(x|a,b) &=& \\sum_{i=1}^{N} \\frac{(x_{i}-f(x_{i},a,b))^2}{x_{i}}\\\\\n", "\\frac{\\partial \\chi^{2}(x|a,b) }{\\partial a} = 0 \\\\\n", "\\frac{\\partial \\chi^{2}(x|a,b) }{\\partial b} = 0\n", "\\end{eqnarray}\n", "\n", "The above minima is exactly what we need to do to minimize this. Lets go ahead and do that and look at how things vary. We will do this for each parameter treating them independently. "]}, {"cell_type": "code", "execution_count": null, "id": "0877ca84", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#>>>RUN\n", "\n", "############ This is all code from lecture 4\n", "import numpy as np\n", "import csv\n", "import math\n", "import lmfit\n", "import matplotlib.pyplot as plt\n", "from scipy import optimize as opt\n", "\n", "def rad(iTheta):\n", "    return iTheta/180. * math.pi\n", "\n", "def rad1(iTheta):\n", "    return iTheta/180. * math.pi-math.pi\n", "\n", "def load(label):\n", "    dec=np.array([])\n", "    ra=np.array([])\n", "    az=np.array([])\n", "    with open(label,'r') as csvfile:\n", "        plots = csv.reader(csvfile,delimiter=' ')\n", "        for pRow in plots:\n", "            if '#' in pRow[0] or pRow[0]=='':\n", "                continue\n", "            dec = np.append(dec,rad(float(pRow[2])))\n", "            ra  = np.append(ra,rad1(float(pRow[3])))\n", "            az  = np.append(az,rad(float(pRow[4])))\n", "    return dec,ra,az\n", "\n", "def prephist(iRA):\n", "    y0, bin_edges = np.histogram(iRA, bins=30)\n", "    x0 = 0.5*(bin_edges[1:] + bin_edges[:-1])\n", "    y0 = y0.astype('float')\n", "    return x0,y0,1./(y0**0.5)\n", "\n", "label8='events_a8_1space.dat'\n", "dec,ra8,az=load(label8)\n", "xhist,yhist,xweights=prephist(ra8)\n", "\n", "\n", "########## Tlast fit code\n", "\n", "def fnew(x,a,b):\n", "    pVal=b*np.sin(x)\n", "    return a+pVal\n", "\n", "model  = lmfit.Model(fnew)\n", "p = model.make_params(a=1000,b=10)\n", "result = model.fit(data=yhist,x=xhist, params=p, weights=xweights)\n", "lmfit.report_fit(result)\n", "plt.figure()\n", "result.plot()\n", "plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "id": "2bc7bd1a", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["import scipy.stats as stats\n", "print(stats.norm.cdf(1)-stats.norm.cdf(-1))\n", "def pval(iVal):\n", "    return stats.norm.cdf(iVal)-stats.norm.cdf(-iVal)\n", "\n", "def chi2Val(iGausSigma,iNDOF):\n", "    val=stats.chi2.ppf(pval(iGausSigma),iNDOF)\n", "    return val\n", "print(chi2Val(1,2))"]}, {"cell_type": "markdown", "id": "b3d32074", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_7_1'></a>     \n", "\n", "| [Top](#section_7_0) | [Restart Section](#section_7_1) | [Next Section](#section_7_2) |\n"]}, {"cell_type": "markdown", "id": "faee5f63", "metadata": {"tags": ["learner", "md", "catsoop_01"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 7.1.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 13, "id": "201dd47c", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "c57e5a05", "metadata": {"tags": ["learner", "catsoop_01", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "58ae74fc", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_7_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.2 Minimizing a Surface (1D Scan)</h2>  \n", "\n", "| [Top](#section_7_0) | [Previous Section](#section_7_1) | [Exercises](#exercises_7_2) | [Next Section](#section_7_3) |\n"]}, {"cell_type": "markdown", "id": "4e796e13", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "TEXT\n"]}, {"cell_type": "code", "execution_count": null, "id": "624ca31f", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#This is the new code\n", "def chi2(iX):\n", "    lTot=0\n", "    for val in range(len(yhist)):\n", "        xtest=fnew(xhist[val],iX[0],iX[1])\n", "        lTot = lTot+(1./(xtest+1e-5))*(yhist[val]-xtest)**2\n", "    return lTot\n", "\n", "def chi2minX(xval):\n", "    val=chi2([xval,sol.x[1]])\n", "    minval=chi2(sol.x)+1\n", "    return val-minval\n", "\n", "def chi2minY(yval):\n", "    val=chi2([sol.x[0],yval])\n", "    minval=chi2(sol.x)+1\n", "    return val-minval\n", "\n", "def chi2uncX(sol):\n", "    solX1=opt.root_scalar(chi2minX,bracket=[sol.x[0], sol.x[0]*1.02],method='brentq')\n", "    solX2=opt.root_scalar(chi2minX,bracket=[sol.x[0]*0.98, sol.x[0]],method='brentq')\n", "    print(\"a:\",sol.x[0],\"+/-\",abs(solX2.root-solX1.root)/2.)\n", "    print(\"Reminder the Poission uncertianty would be:\",math.sqrt(sol.x[0]/40))\n", "    return solX1, solX2\n", "\n", "def chi2uncY(sol):\n", "    solY1=opt.root_scalar(chi2minY,bracket=[sol.x[1],    sol.x[1]*1.2],method='brentq')\n", "    solY2=opt.root_scalar(chi2minY,bracket=[sol.x[1]*0.8, sol.x[1]],method='brentq')\n", "    print(\"b:\",sol.x[1],\"+/-\",abs(solY2.root-solY1.root)/2.)\n", "    return solY1, solY2\n", "\n", "\n", "#First we minimize\n", "x0 = np.array([1000,10]) # initial conditions\n", "ps = [x0]\n", "sol=opt.minimize(chi2, x0)\n", "print(sol)"]}, {"cell_type": "code", "execution_count": null, "id": "a1abb7a3", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#Scan near the minimum of each value\n", "x = np.linspace(sol.x[0]*0.99,sol.x[0]*1.01, 100)\n", "y = np.linspace(sol.x[1]*0.5,sol.x[1]*1.5, 100)\n", "\n", "#Now lets fix one parameter at the minimum, and profile the other\n", "plt.plot(x, chi2([x,sol.x[1]]),label='chi2');\n", "plt.axhline(sol.fun+1, c='red')\n", "plt.xlabel(\"b-value\")\n", "plt.ylabel(\"$\\chi^{2}$\")\n", "plt.show()\n", "\n", "#Now for the other parameter\n", "plt.plot(y, chi2([sol.x[0],y]),label='chi2');\n", "plt.axhline(sol.fun+1, c='red')\n", "plt.xlabel(\"b-value\")\n", "plt.ylabel(\"$\\chi^{2}$\")\n", "plt.show()\n", "\n", "solX1, solX2 = chi2uncX(sol)\n", "solY1, solY2 = chi2uncY(sol)"]}, {"cell_type": "markdown", "id": "8883ef62", "metadata": {"tags": ["learner", "md"]}, "source": ["So now we have used our minimizer in 2D to profile the $\\chi^{2}$ distribution, and we have obtained an uncertainty by looking at $\\Delta \\chi^{2}$, and you can see that our semi by-hand manipulation got us to the best parameters of the fitting. Additionally by profiling $\\chi^{2}$ in each of the single parameters we obtained the same uncertiinties. \n", "\n", "As a small note our final fitted uncertainty is a little bit larger than the poission uncertainty. In fact for all fits there is a rule that our uncertainties on any parameter have to be larger than a certain number. This bound is known as the Cram\u00e9r-Rao bound. I won't derive it or go into in detail, but the Cram\u00e9r-Rao bound states. \n", "\\begin{equation}\n", "\\mathrm{Var}(\\theta|\\hat{\\theta}) \\geq \\frac{1}{\\mathcal{I}(\\theta)} \\\\\n", "\\mathcal{I}(\\theta) = E_{p(X|\\theta)}\\left[-\\frac{\\partial^{2}}{\\partial\\theta^{2}}\\log\\left(p\\left(x|\\theta\\right)\\right)\\right]\n", "\\end{equation}\n", "Where here we call $\\mathcal{I}(\\theta)$ the Fisher information. While I don't want to go into this more, this result is powerful because it means that there is limit to when you should stop searching for a best fit. "]}, {"cell_type": "markdown", "id": "ca4e59a0", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_7_2'></a>     \n", "\n", "| [Top](#section_7_0) | [Restart Section](#section_7_2) | [Next Section](#section_7_3) |\n"]}, {"cell_type": "markdown", "id": "0118dd7f", "metadata": {"tags": ["learner", "md", "catsoop_02"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 7.2.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 23, "id": "3413389e", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "20390457", "metadata": {"tags": ["learner", "catsoop_02", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "edc5407d", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_7_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.3 Minimizing a Surface (2D Scan)</h2>  \n", "\n", "| [Top](#section_7_0) | [Previous Section](#section_7_2) | [Exercises](#exercises_7_3) | [Next Section](#section_7_4) |\n"]}, {"cell_type": "markdown", "id": "25b125c9", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section."]}, {"cell_type": "code", "execution_count": 4, "id": "4abb69b4", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"data": {"text/html": ["\n", "        <iframe\n", "            width=\"970\"\n", "            height=\"550\"\n", "            src=\"images/slides_L07_03.html\"\n", "            frameborder=\"0\"\n", "            allowfullscreen\n", "            \n", "        ></iframe>\n", "        "], "text/plain": ["<IPython.lib.display.IFrame at 0x1113210a0>"]}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": ["#>>>RUN\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L07/slides_L07_03.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "d0e436c8", "metadata": {"tags": ["learner", "md"]}, "source": ["To really visualize the whole thing lets make one more plot the 2D $\\chi^2$ distribution. "]}, {"cell_type": "code", "execution_count": null, "id": "c5ef26bf", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#define the 2D X and Y grid\n", "x = np.linspace(sol.x[0]*0.99,sol.x[0]*1.01, 100) #grid in x\n", "y = np.linspace(sol.x[1]*0.5,sol.x[1]*1.5, 100)#grid in y\n", "X, Y = np.meshgrid(x, y) #2d grid\n", "\n", "#Now our z-axis will be the chi2 of the 2D grid (this is complicated code)\n", "Z = np.array([chi2([x,y]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n", "\n", "#and plot\n", "def plotColorsAndContours(X,Y,Z):\n", "    fig, ax = plt.subplots(1, 1)\n", "    c = ax.pcolor(X,Y,Z,cmap='RdBu')\n", "    cb=fig.colorbar(c, ax=ax)\n", "    plt.xlabel(\"a\")\n", "    plt.ylabel(\"b\")\n", "    cb.set_label(\"$\\chi^{2}$\")\n", "    #Now lets plot the isobars of Delta chi^2\n", "    levels = [0.1,1,2.3,4,9, 16, 25, 36, 49, 64, 81, 100]\n", "    for i0 in range(len(levels)):\n", "        levels[i0] = levels[i0]+sol.fun\n", "    c = plt.contour(X, Y, Z, levels,colors=['red', 'blue', 'yellow','green'])\n", "    #plt.show()\n", "    \n", "plotColorsAndContours(X,Y,Z)"]}, {"cell_type": "markdown", "id": "521cb612", "metadata": {"tags": ["learner", "md"]}, "source": ["Now lets got back to our fit, we obtained the 1D results, but we made this 2D plot, and we drew a funny yellow line on it above. What exactly is the uncertainty profile when we are profiling 2 variables at once. Lets go back to our our Taylor expansion result above, but lets write it in terms of all variables $\\vec{\\theta}$:  \n", "\n", "\\begin{equation}\n", "\\chi^{2}(x_{i},\\vec{\\theta})=\\chi^{2}_{min}(x_{i},\\vec{\\theta})+\\frac{1}{2}(\\theta_{i}-\\theta_{j})^{T}\\frac{\\partial^{2}}{\\partial \\theta_{i}\\theta_{0}}\\chi^{2}_{min}(x_{i},\\vec{\\theta}_{0})(\\theta_{j}-\\theta_{0})\n", "\\end{equation}\n", "We can write this out in 2D as:\n", "\\begin{equation}\n", "\\chi^{2}(x,\\vec{\\theta})=\\chi_{min}^{2}(x,\\vec{\\theta})+\\frac{1}{2}\\left(\\begin{array}{cc}\n", "\\theta_{a}-\\theta_{a-min} & \\theta_{b}-\\theta_{b-min}\\end{array}\\right)\\left(\\begin{array}{cc}\n", "\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}^{2}} & \\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}\\partial\\theta_{b}}\\\\\n", "\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}\\partial\\theta_{b}} & \\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{b}^{2}}\n", "\\end{array}\\right)\\left(\\begin{array}{c}\n", "\\theta_{a}-\\theta_{a-min}\\\\\n", "\\theta_{b}-\\theta_{b-min}\n", "\\end{array}\\right)\n", "\\end{equation}\n", "Now in the case where $\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}\\partial\\theta_{b}}\\approx0$ we can simplify this distribution by a lot. Lets do it \n", "\n", "\\begin{equation}\n", "\\chi^{2}(x,\\vec{\\theta})=\\chi_{min}^{2}(x,\\vec{\\theta})+\\frac{1}{2}\\left(\\begin{array}{cc}\n", "\\Delta\\theta_{a} & \\Delta\\theta_{b}\\end{array}\\right)\\left(\\begin{array}{cc}\n", "\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}^{2}} & 0\\\\\n", "0 & \\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{b}^{2}}\n", "\\end{array}\\right)\\left(\\begin{array}{c}\n", "\\Delta\\theta_{a}\\\\\n", "\\Delta\\theta_{b}\n", "\\end{array}\\right)\n", "\\end{equation}\n", "\n", "This all becomes a 2D quadratic equation\n", "\n", "\\begin{align*}\n", "\\chi^{2}(x,\\vec{\\theta}) & =\\chi_{min}^{2}(x,\\vec{\\theta})+\\frac{1}{2}\\left(\\begin{array}{cc}\n", "\\Delta\\theta_{a}^{2}\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}^{2}}+ & \\Delta\\theta_{b}^{2}\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{b}^{2}}\\end{array}\\right)\\\\\n", " & =\\chi_{min}^{2}(x,\\vec{\\theta})+\\left(\\begin{array}{cc}\n", "\\frac{\\Delta\\theta_{a}^{2}}{\\sigma_{\\theta_{a}}^{2}}+ & \\frac{\\Delta\\theta_{b}^{2}}{\\sigma_{\\theta_{b}}^{2}}\\end{array}\\right)\n", "\\end{align*}\n", "\n", "So let's plot this quadratic and compare it to our minimum contours. \n", "\n", "Now before we do this I want to point out one more thing, we are now profiling two parameters at once in this 2D plot, which means the sum of our independent variables will be a $\\chi^{2}$ for 2 degrees of freedom. The 1 $\\sigma$ confidence interval for 2-degrees of freedom is computed by taking $\\Delta \\chi^2(x,\\nu=2)=x~\\mathrm{where~}\\mathrm{cdf}\\left(\\chi^{2}(x,2)=0.683\\right)\\approx2.3$. This is the yellow line on the contour.\n"]}, {"cell_type": "code", "execution_count": null, "id": "24453d6f", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#Lets plot the uncertainties  from hess_inv\n", "print(np.sqrt(2*sol.hess_inv))\n", "#the diagonals are approximately the errors\n", "\n", "#Make a the expression in the above equation x and x0 are 2 vectors\n", "def quadratic2D(x,x0,sigma0):\n", "    lVals=x-x0\n", "    lVals=(lVals**2)/(sigma0)/sigma0\n", "    return np.sum(lVals)\n", "\n", "plotColorsAndContours(X,Y,Z)\n", "\n", "#Now plot the quadratic circles\n", "def plotQuadrticCricles(sigx,sigy):\n", "    levels = [0.1,1,2.3,4,9, 16, 25, 36, 49, 64, 81, 100]\n", "    ZQ = np.array([quadratic2D([x,y],sol.x,[sigx,sigy]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n", "    c = plt.contour(X, Y, ZQ, levels,colors=['red', 'blue', 'yellow','green'],linestyles='dashed')\n", "\n", "sigx=(solX2.root-solX1.root)/2.\n", "sigy=(solY2.root-solY1.root)/2.\n", "plotQuadrticCricles(sigx,sigy)\n", "plt.show()"]}, {"cell_type": "markdown", "id": "cd3a9a2c", "metadata": {"tags": ["learner", "md"]}, "source": ["This gives us the same yellow line for our variance band. If you look close you do see a difference. This makes us beg the question of what happens when $\\frac{\\partial^{2}\\chi^{2}}{\\partial \\theta_{a}\\theta_{b}}\\neq0$. Lets try another two parameter fit. To see that, lets try to minimize with a different function. "]}, {"cell_type": "markdown", "id": "79299eda", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_7_3'></a>     \n", "\n", "| [Top](#section_7_0) | [Restart Section](#section_7_3) | [Next Section](#section_7_4) |\n"]}, {"cell_type": "markdown", "id": "daf8a015", "metadata": {"tags": ["learner", "md", "catsoop_03"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 7.3.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 33, "id": "701c8cfa", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "c629164d", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "72049b17", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_7_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.4 Correlations Between Fit Parameters: Part 1</h2>  \n", "\n", "| [Top](#section_7_0) | [Previous Section](#section_7_3) | [Exercises](#exercises_7_4) | [Next Section](#section_7_5) |\n"]}, {"cell_type": "markdown", "id": "d18ab56b", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section."]}, {"cell_type": "code", "execution_count": 1, "id": "c186d466", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"data": {"text/html": ["\n", "        <iframe\n", "            width=\"970\"\n", "            height=\"550\"\n", "            src=\"https://mitx-8s50.github.io/slides/L07/slides_L07_04.html\"\n", "            frameborder=\"0\"\n", "            allowfullscreen\n", "            \n", "        ></iframe>\n", "        "], "text/plain": ["<IPython.lib.display.IFrame at 0x10cb63250>"]}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": ["#>>>RUN\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L07/slides_L07_04.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "93d297bb", "metadata": {"tags": ["learner", "md"]}, "source": ["### Challenge question:\n", "\n", "Make the above 2D contour plot with circles using the minimization function\n", "\n", "\\begin{equation}\n", " f(x) = a x + b (1-x)\n", "\\end{equation}\n", "\n", "Try to use the quadratic2D approximation, why does the 2D quadratic not agree? Why are the uncertainties so far off? "]}, {"cell_type": "code", "execution_count": null, "id": "3ff471a2", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["def fnew(x,a,b):\n", "    pVal=b*(1-x)\n", "    return a*x+pVal\n", "\n", "model  = lmfit.Model(fnew)\n", "p = model.make_params(a=1000,b=1000)\n", "\n", "result = model.fit(data=yhist,x=xhist, params=p, weights=xweights)\n", "lmfit.report_fit(result)\n", "plt.figure()\n", "result.plot()\n", "plt.show()\n", "\n", "\n", "x0 = np.array([1000,1000])\n", "ps = [x0]\n", "sol=opt.minimize(chi2, x0)\n", "print(sol)\n", "\n", "x = np.linspace(sol.x[0]*0.99,sol.x[0]*1.01, 100)\n", "y = np.linspace(sol.x[1]*0.99,sol.x[1]*1.01, 100)\n", "X, Y = np.meshgrid(x, y)\n", "Z = np.array([chi2([x,y]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n", "plotColorsAndContours(X,Y,Z)\n", "\n", "solX1, solX2 = chi2uncX(sol)\n", "solY1, solY2 = chi2uncY(sol)\n", "plotQuadrticCricles(sigx,sigy)\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "44f6e341", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_7_4'></a>     \n", "\n", "| [Top](#section_7_0) | [Restart Section](#section_7_4) | [Next Section](#section_7_5) |\n"]}, {"cell_type": "markdown", "id": "380c5d57", "metadata": {"tags": ["learner", "md", "catsoop_04"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 7.4.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 43, "id": "fafd3199", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "a01747e5", "metadata": {"tags": ["learner", "catsoop_04", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "3fecddd7", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_7_5'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.5 Correlations Between Fit Parameters: Part 2</h2>     \n", "\n", "| [Top](#section_7_0) | [Previous Section](#section_7_4) | [Exercises](#exercises_7_5) |\n"]}, {"cell_type": "markdown", "id": "5df5c59d", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Dealing with Correlated uncertainties</h3>\n", "\n", "Now look at our parameters our uncertainty estimate is smaller than what we observed above. Incidently the uncertainties quoted now differ from what we got by varying from the $\\chi^{2}$ minimum. What we are doing is moving up and down the 2D plot and looking at $\\delta \\chi^{2}$. However given the parameters are so correlated, you see that this isn't really a reflection of the true uncertainty in the sense that we can move along $x$ and $y$ and still be within the yellow or even blue ellipse. It's quite clear when you overlap the uncertainty from the quadratic function, which draws circles not ellipses. \n", "\n", "So then what is the right uncertainty? \n", "\n", "Well from our fit using lmfit above we see that the fit outputs a parameter $C(a,b)=0.872$ this is the correlation between the parameters of $a$ and $b$ we also see from our optimization function, we get a computation of something labelled as the hess_inv. This we can write noting the relation of the $\\chi^{2}$ Hessian and uncertainties as \n", "\\begin{equation}\n", "\\chi^{2}(x_{i},\\vec{\\theta})=\\chi^{2}_{min}(x_{i},\\vec{\\theta})+\\frac{1}{2}(\\theta_{i}-\\theta_{j})^{T}\\frac{\\partial^{2}}{\\partial \\theta_{i}\\theta_{0}}\\chi^{2}_{min}(x_{i},\\vec{\\theta}_{0})(\\theta_{j}-\\theta_{0})\n", "\\end{equation}\n", "\n", "Which allows us to write the uncertainties as\n", "\\begin{equation}\n", " \\frac{2}{\\sigma^{2}}=\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{i}\\partial\\theta_{j}} \\\\\n", " \\sigma^{2}    = 2\\left(\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{i}\\partial\\theta_{j}}\\right)^{-1} \n", "\\end{equation}\n", "\n", "Now in the case where the off diagonals of the Hessian were zero we had the clear case that the samples are not correlated, and as a consequence, \n", "\\begin{equation}\n", "\\left(\\begin{array}{cc}\n", "\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}^{2}} & 0\\\\\n", "0 & \\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{b}^{2}}\n", "\\end{array}\\right)\\rightarrow\\left(\\begin{array}{cc}\n", "\\frac{2}{\\sigma_{a}^{2}} & 0\\\\\n", "0 & \\frac{2}{\\sigma_{b}^{2}}\n", "\\end{array}\\right)\n", "\\end{equation}\n", "\n", "Now we have something more complicated. However this is a natural way to define correlations. Lets first verify our intuition for our minimization scheme by taking our $2x2$ Hessian metrix and diagonalizing, computing the eigenvectors and the eigenvalues and then drawing an ellipse. We can diagonalize the matrix as: \n", "\\begin{equation}\n", "A^{-1}2\\left(\\begin{array}{cc}\n", "\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}^{2}} & \\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}\\partial\\theta_{b}}\\\\\n", "\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}\\partial\\theta_{b}} & \\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{b}^{2}}\n", "\\end{array}\\right)^{-1}A=\\left(\\begin{array}{cc}\n", "\\sigma_{1}^{2} & 0\\\\\n", "0 & \\sigma_{2}^{2}\n", "\\end{array}\\right)\n", "\\end{equation}\n", "It is important to note for any N dimensional hessian, provided the determinant is not zero, we can find an basis of independent variables that are not correlated. That is to say we can always diagonalize our Hessian, and the eignevectors of our Hessian are the independent values with variances given by the eigenvalues. \n", "\n", "Knowing how this works, lets play with the Hessian. When we run our minimizer, we get Hessian inverse, let's play with it. \n"]}, {"cell_type": "code", "execution_count": 50, "id": "023a3c6f", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "print(np.sqrt(2*sol.hess_inv))\n", "#The diagonals are the uncertainty lmfit quotes\n", "\n", "#Really the best way to do this is to get the eigen values using an linear algebra problem\n", "import numpy.linalg as la\n", "w, v=la.eig(2*sol.hess_inv)\n", "print(\"values\",w,\"vectors\",v)\n", "\n", "#Now lets plot the eigenvectors\n", "from matplotlib.patches import Ellipse\n", "def get_cov_ellipse(cov, centre, nstd, **kwargs):\n", "    \"\"\"\n", "    Return a matplotlib Ellipse patch representing the covariance matrix\n", "    cov centred at centre and scaled by the factor nstd.\n", "\n", "    \"\"\"\n", "    # Find and sort eigenvalues and eigenvectors into descending order\n", "    eigvals, eigvecs = np.linalg.eigh(cov)\n", "    order = eigvals.argsort()[::-1]\n", "    eigvals, eigvecs = eigvals[order], eigvecs[:, order]\n", "\n", "    # The anti-clockwise angle to rotate our ellipse by \n", "    vx, vy = eigvecs[:,0][0], eigvecs[:,0][1]\n", "    theta = np.arctan2(vy, vx)\n", "\n", "    # Width and height of ellipse to draw\n", "    width, height = 2 * nstd * np.sqrt(eigvals)\n", "    return Ellipse(xy=centre, width=width, height=height,angle=np.degrees(theta), **kwargs)\n", "\n", "err_ellipse=get_cov_ellipse(2*sol.hess_inv,sol.x,1)\n", "fig, ax = plt.subplots(1, 1)\n", "c = ax.pcolor(X,Y,Z,cmap='RdBu')\n", "fig.colorbar(c, ax=ax)\n", "levels = [0.1,1,2.3,4,9, 16, 25, 36, 49, 64, 81, 100]\n", "for i0 in range(len(levels)):\n", "    levels[i0] = levels[i0]+sol.fun\n", "c = plt.contour(X, Y, Z, levels,colors=['red', 'blue', 'yellow','green'])\n", "ax.add_artist(err_ellipse)\n", "plt.show()"]}, {"cell_type": "markdown", "id": "6827fd0d", "metadata": {"tags": ["learner", "md"]}, "source": ["From the above we can now formulate how we should describe the uncertinaties in our system. When we had a decorrelated system we had a total uncrtainty in our $\\chi^{2}$ given by \n", "\\begin{equation}\n", "\\sigma_{tot}^{2} = \\sigma_{a}^2+\\sigma_{b}^2\n", "\\end{equation}\n", "\n", "Now we have the full ellipse\n", "\\begin{equation}\n", "\\sigma_{tot}^{2} = \\sigma_{a}^2+\\sigma_{b}^2+2\\sigma_{ab}\n", "\\end{equation}\n", "\n", "Where we define that $\\sigma_{ab}=\\rm{COV(a,b)}$. There are a number of ways to call this variable, they all are equivalent, but lets be careful to write it out. We can write the error matrix in 2D as:\n", "\\begin{equation}\n", "\\left(\\begin{array}{cc}\n", "\\sigma_{a}^{2} & {\\rm COV}(a,b)\\\\\n", "{\\rm COV}(a,b) & \\sigma_{b}^{2}\n", "\\end{array}\\right)=\\sum_{i=1}^{N}\\left(\\begin{array}{cc}\n", "\\left(a_{i}-\\bar{a}\\right)^{2} & \\left(a_{i}-\\bar{a}\\right)\\left(b_{i}-\\bar{b}\\right)\\\\\n", "\\left(a_{i}-\\bar{a}\\right)\\left(b_{i}-\\bar{b}\\right) & \\left(b_{i}-\\bar{b}\\right)^{2}\n", "\\end{array}\\right)\n", "\\end{equation}\n", "where on the right side we have written it in terms of the computation over events. Recall that for the a linear regression the slope is just the $\\rm{COV(X,Y)/VAR(X)}$, so the covariance matrix is intricately tied with slope. \n", "\n", "We can also write it as the correlation matrix where we normalize by the uncertainties:\n", "\\begin{equation}\n", "\\rho=\\left(\\frac{1}{\\sigma_{a}}  \\frac{1}{\\sigma_{b}} \\right)^{T}\\left(\\begin{array}{cc}\n", "1 & \\frac{{\\rm COV}(a,b)}{\\sigma_{a}\\sigma_{b}}\\\\\n", "\\frac{{\\rm COV}(a,b)}{\\sigma_{a}\\sigma_{b}} & 1\n", "\\end{array}\\right)\\left(\\frac{1}{\\sigma_{a}}   \\frac{1}{\\sigma_{b}} \\right) \n", "\\end{equation}\n", "\n", "Recall that the covariance is what we originally used to compute the linear slope of the points, this is exactly the same here. In fact, instead of scanning the likelihood analytically we could have sampled the points, and done a linear regression. The resulting slope and line can be related to our eigenvectors. One last think to mention is that if variables are correlated, we can use the correlation to propagate the uncertainties. \n", "\\begin{equation}\n", "\\sigma_{f}^{2} = \\left(\\frac{\\partial f}{\\partial x}\\right)^2\\sigma_{x}^2 + \\left(\\frac{\\partial f}{\\partial y}\\right)^{2}+\\left(\\frac{\\partial f}{\\partial x}\\right)\\left(\\frac{\\partial f}{\\partial y}\\right)\\sigma_{xy}\n", "\\end{equation}\n", "\n", "Now lets check we can get the corelation matrix."]}, {"cell_type": "code", "execution_count": 50, "id": "a868991e", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#Now lets get the correlation C(a,b) (see below)\n", "w, v=np.linalg.eig(2*sol.hess_inv)\n", "print(\"c(a,b)\",v[0,1]/v[0,0])\n", "print(\"A deceptively wrong way to get correlation:\",sol.hess_inv[0,1]/sol.hess_inv[0,0])"]}, {"cell_type": "markdown", "id": "46d9089d", "metadata": {"tags": ["learner", "md"]}, "source": ["Finally, let's compute it from some toy data. "]}, {"cell_type": "code", "execution_count": 50, "id": "8c40ace3", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["import lmfit\n", "\n", "def fnew(x,a,b):\n", "    pVal=b*(1-x)\n", "    return a*x+pVal\n", "\n", "#Randomly sample points in the above range\n", "def maketoy(iy):\n", "    toy=np.array([])\n", "    for i0 in range(len(iy)):\n", "        pVal = np.random.normal (iy[i0],np.sqrt([iy[i0]]))\n", "        toy = np.append(toy,float(pVal))\n", "    return toy\n", "\n", "def fittoy(ibin,iy):\n", "    toy=maketoy(iy)\n", "    model  = lmfit.Model(fnew)\n", "    p = model.make_params(a=1000,b=10)\n", "    xweights=np.array([])\n", "    for i0 in range(len(toy)):\n", "        xweights = np.append(xweights,1./math.sqrt(toy[i0]))\n", "    result = model.fit(data=toy,x=ibin, params=p, weights=xweights)\n", "    return result.params[\"a\"].value,result.params[\"b\"].value\n", "\n", "ntoys=1000\n", "lAs=np.array([])\n", "lBs=np.array([])\n", "for i0 in range(ntoys):\n", "    pA,pB=fittoy(xhist,yhist)\n", "    lAs = np.append(lAs,pA)\n", "    lBs = np.append(lBs,pB)\n", "\n", "err_ellipse=get_cov_ellipse(2*sol.hess_inv,sol.x, 1)\n", "\n", "fig, ax = plt.subplots(1, 1)\n", "c = ax.pcolor(X,Y,Z,cmap='RdBu')\n", "fig.colorbar(c, ax=ax)\n", "c = plt.contour(X, Y, Z, levels,colors=['red', 'blue', 'yellow','green'])\n", "ax.add_artist(err_ellipse)\n", "plt.plot(lAs,lBs,c='black',marker='.',linestyle = 'None')\n", "plt.show()\n", "\n", "#Now lets run a. linear regression\n", "def variance(isamples):\n", "    mean=isamples.mean()\n", "    n=len(isamples)\n", "    tot=0\n", "    for pVal in isamples:\n", "        tot+=(pVal-mean)**2\n", "    return tot/n\n", "\n", "def covariance(ixs,iys):\n", "    meanx=ixs.mean()\n", "    meany=iys.mean()\n", "    n=len(ixs)\n", "    tot=0\n", "    for i0 in range(len(ixs)):\n", "        tot+=(ixs[i0]-meanx)*(iys[i0]-meany)\n", "    return tot/n\n", "\n", "print(\"A:\",lAs.mean(),\"+/-\",lAs.std())\n", "print(\"B:\",lBs.mean(),\"+/-\",lBs.std())\n", "print(\"Cov:\",covariance(lAs,lBs),\"A Variance:\",variance(lAs),\"B Variance:\",variance(lBs))\n", "print(\"Check with Hessian:\",2*sol.hess_inv)\n", "print(\"Cor:\",covariance(lAs,lBs)/math.sqrt(variance(lAs)*variance(lBs)),\"A Variance:\",1.,\"B Variance:\",1.)"]}, {"cell_type": "markdown", "id": "623d613f", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_7_5'></a>   \n", "\n", "| [Top](#section_7_0) | [Restart Section](#section_7_5) |\n"]}, {"cell_type": "markdown", "id": "2275efc2", "metadata": {"tags": ["learner", "md", "catsoop_05"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 7.5.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 53, "id": "c61252e2", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "761f3b65", "metadata": {"tags": ["learner", "catsoop_05", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}