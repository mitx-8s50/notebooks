{"cells": [{"cell_type": "markdown", "id": "701b2911", "metadata": {"tags": ["learner", "md"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 12: Hypothesis Testing Part 2</h1>\n"]}, {"cell_type": "markdown", "id": "6fdaa155", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_12_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L12.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "15c89328", "metadata": {"tags": ["learner", "md"]}, "source": ["<table style=\"width:100%\">\n", "    <colgroup>\n", "       <col span=\"1\" style=\"width: 40%;\">\n", "       <col span=\"1\" style=\"width: 15%;\">\n", "       <col span=\"1\" style=\"width: 45%;\">\n", "    </colgroup>\n", "    <tr>\n", "        <th style=\"text-align: left; font-size: 13pt;\">Section</th>\n", "        <th style=\"text-align: left; font-size: 13pt;\">Exercises</th>\n", "        <th style=\"text-align: left; font-size: 13pt;\">Summary</th>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_12_1\">L12.1 Origin of t-test</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_12_1\">L12.1 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_12_2\">L12.2 Higgs Boson Discovery and F-statistic</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_12_2\">L12.2 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_12_3\">L12.3 Fitting Higgs Data</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_12_3\">L12.3 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_12_4\">L12.4 Computation Using F-statistic</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_12_4\">L12.4 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_12_5\">L12.5 Fitting the Higgs Signal Part 1</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_12_5\">L12.5 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_12_6\">L12.6 Fitting the Higgs Signal Part 2</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_12_6\">L12.6 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_12_7\">L12.7 Fitting Using Higher Order Polynomials</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_12_7\">L12.7 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_12_8\">L12.8 Building Interpolated Distributions</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_12_8\">L12.8 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_12_9\">L12.9 Dealing with non-Analytic Forms</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_12_9\">L12.9 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "</table>\n", "\n"]}, {"cell_type": "markdown", "id": "a5b9f322", "metadata": {"tags": ["learner", "catsoop_00", "md"]}, "source": ["<h3>Learning Objectives</h3>\n", "\n", "In this class, we will formalize how we actualy make a scientific hypothesis and test it.\n", "\n", "We will explore the following objectives:\n", "\n", "- t-test\n", "- f-test\n", "- Fitting for the Higgs boson signal Background\n", "- Fitting For the Higgs Boson\n", "- Combining p-values\n", "- Building Interpolated distributions\n", "- Dealing with non-analytic forms"]}, {"cell_type": "markdown", "id": "78f33535", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Importing Libraries</h3>\n", "\n", "Before beginning, run the cell below to import the relevant libraries for this notebook. \n", "Optionally, set the plot resolution and default figure size.\n"]}, {"cell_type": "code", "execution_count": 7, "id": "f9cc7fd9", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "import numpy as np\n", "\n", "#set plot resolution\n", "#%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure size\n", "#plt.rcParams['figure.figsize'] = (9,6)\n"]}, {"cell_type": "markdown", "id": "b9aef2b3", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_12_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L12.1 Origin of t-test</h2>  \n", "\n", "| [Top](#section_12_0) | [Previous Section](#section_12_0) | [Exercises](#exercises_12_1) | [Next Section](#section_12_2) |\n"]}, {"cell_type": "markdown", "id": "df6d0ffb", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section."]}, {"cell_type": "code", "execution_count": 1, "id": "375fbf4b", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"data": {"text/html": ["\n", "        <iframe\n", "            width=\"970\"\n", "            height=\"550\"\n", "            src=\"https://mitx-8s50.github.io/slides/L12/slides_L12_01.html\"\n", "            frameborder=\"0\"\n", "            allowfullscreen\n", "            \n", "        ></iframe>\n", "        "], "text/plain": ["<IPython.lib.display.IFrame at 0x10d9d3790>"]}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": ["#>>>RUN\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L12/slides_L12_01.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "8952050e", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>t-test</h3>\n", "\n", "Now, just a fun aside. In addtiion to the $\\chi^{2}$ test there is an another hypothesis test that was created known as the t-test. The origin of the t-test is that Guiness beer brewers were looking to compare beers from different sets of barley to see if they had a similar flavor. The issue was that they only had a few batches of barley, and a few batches of beer to compare distributions, in the limit of small number, we cannot assume distributions are gaussian, and, as a consequence, we cannot use the $\\chi^{2}$ test. In light of this, the [\"Student t-test\"](https://en.wikipedia.org/wiki/Student%27s_t-test) was invented by William Gossett. Its called student t-test because Gossett published his paper as \"student\" to avoid Guiness beer proprietary regulations. \n", "\n", "The t-test, says that for $N$ samples $x_{1},...,x_{n}$ each from an underlying Guassian distribution with mean $\\mu$ and variance $\\sigma^{2}$, we can define the following measured mean ($\\bar{x}$) and variance($s^{2}$): \n", "\\begin{equation}\n", "\\bar{x} = \\frac{1}{N}\\sum_{i} x_{i} \\\\\n", "s^{2}   = \\frac{1}{N-1}\\sum_{i} \\left(x_{i}-\\bar{x}\\right)^{2}\\\\\n", "\\end{equation}\n", "And the distribution of the mean and variance combined defined as the t-statistic\n", "\\begin{equation}\n", "t = \\frac{\\bar{x}-\\mu}{S/\\sqrt{N}}\n", "\\end{equation}\n", "will follow a $t$ distriubtion of $\\nu=N-1$ degrees of freedom, given by\n", "\\begin{equation}\n", " t(x) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\sqrt{\\nu\\pi}\\Gamma\\left(\\frac{\\nu}{2}\\right)} \\left(1+\\frac{x^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}}\n", "\\end{equation}\n", "\n", "The t-distribution is a low-stats combination of binomial distributions. In practice, this gives us another way to test if two distributions are equal. This one avoids using histograms. Let's see how it behaves. Lets run a t-test to see how far our distributions are from $\\mu=0$"]}, {"cell_type": "code", "execution_count": null, "id": "193975c7", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#>>>RUN\n", "\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import scipy.stats as stats\n", "\n", "#now lets test the consistency of that our observation is consistent\n", "def ttest(isamples,iMu=0):\n", "    t = (isamples.mean()-iMu)/(isamples.std()/np.sqrt(len(isamples)))\n", "    p = 1 - stats.t.cdf(t,df=len(isamples)-1)\n", "    return p\n", "\n", "xvals=[]\n", "pvalues1=[]\n", "pvalues2=[]\n", "pvalues3=[]\n", "for i0 in range(50):\n", "    testsamples1 = np.random.normal(0.2,1, i0)\n", "    pvalue = ttest(testsamples1)\n", "    pvalues1.append(pvalue)\n", "    testsamples2 = np.random.normal(1,1, i0)\n", "    pvalue = ttest(testsamples2)\n", "    pvalues2.append(pvalue)\n", "    testsamples3 = np.random.normal(2,1, i0)\n", "    pvalue = ttest(testsamples3)\n", "    pvalues3.append(pvalue)\n", "    xvals.append(i0)\n", "\n", "fig, ax = plt.subplots(figsize=(9,6))\n", "plt.style.use('fast')\n", "ax.plot(xvals,pvalues1,label=\"\\mu=0.2\")\n", "ax.plot(xvals,pvalues2,label=\"\\mu=1.0\")\n", "ax.plot(xvals,pvalues3,label=\"\\mu=2.0\")\n", "ax.set_xlabel('N')\n", "ax.set_ylabel('p-value')\n", "ax.set_yscale('log')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "11d7d02e", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>t-test</h3>\n", "\n", "As you can see this is a bit easier to compute that a likelihood or $\\chi^{2}$ test, and in general its more sensitive. This is allows us to take into account the full set of distributions. \n", "\n", "For completeness, the general form to compare two distribution, the modified t-statistic becomes: \n", "\n", "\\begin{equation}\n", "t = \\frac{\\bar{x}_{a}-\\bar{x}_{b}}{s\\left(\\frac{1}{N_{a}} + \\frac{1}{N_{b}} \\right)} \\\\\n", "s = \\sqrt{\\frac{(N_{a}-1)s_{a}^2 + (N_{b}-1)s_{b}^2 }{N_{a}+N_{b}-2} }\n", "\\end{equation}\n", "\n", "Where $s_{a}^2$ is the variance of $x_{a}$ and $\\bar{x}_{a}$ is its mean, and the same goes for $b$. This distribution becomes most useful, when we only have a few events.\n", "\n", "This again follows a $t$ distribution with number of degrees of freedom given by $\\nu=N_{a}+N_{b}-2$. Note that if the variances are large between the two distributions, this statistic breaks down. See [here](https://en.wikipedia.org/wiki/Student%27s_t-test) for more details."]}, {"cell_type": "markdown", "id": "4a0966b2", "metadata": {"tags": ["learner", "md"]}, "source": ["### Challenge Question\n", "\n", "Compute the t-statistic p-value for two gaussian distributions of different means with 50 events?\n", "Why is it so much more powerful than the $\\chi^{2}$ statistic? "]}, {"cell_type": "code", "execution_count": null, "id": "2a918319", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#>>>RUN\n", "\n", "testSamplesA = np.random.normal(0  ,1, 50)\n", "testSamplesB = np.random.normal(0.2,1, 50)\n", "\n", "#solution\n", "def ttest_comp(iSamplesA,iSamplesB):\n", "    s2 = (len(iSamplesA)-1)*iSamplesA.std()**2 + (len(iSamplesB)-1)*iSamplesB.std()**2\n", "    s2 = s2/(len(iSamplesA)+ len(iSamplesB) - 2)\n", "    s  = np.sqrt(s2)*(1/len(iSamplesA) + 1/len(iSamplesB))\n", "    t  = (iSamplesA.mean()-iSamplesB.mean())/s\n", "    p  = stats.t.cdf(t,df=len(iSamplesA)+len(iSamplesB)-2)\n", "    return p\n", "print(ttest_comp(testSamplesA,testSamplesB))\n", "#its more powerful, because the chi2 distribution assuems binning. Consider doing the chi2 statistic with many bins"]}, {"cell_type": "markdown", "id": "681cd05d", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_12_1'></a>     \n", "\n", "| [Top](#section_12_0) | [Restart Section](#section_12_1) | [Next Section](#section_12_2) |\n"]}, {"cell_type": "markdown", "id": "6fe63183", "metadata": {"tags": ["learner", "md", "catsoop_01"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 12.1.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 13, "id": "52f1e1f5", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "3f06ee15", "metadata": {"tags": ["learner", "catsoop_01", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "17598e94", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_12_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L12.2 Higgs Boson Discovery and F-statistic</h2>  \n", "\n", "| [Top](#section_12_0) | [Previous Section](#section_12_1) | [Exercises](#exercises_12_2) | [Next Section](#section_12_3) |\n"]}, {"cell_type": "markdown", "id": "670467d1", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section."]}, {"cell_type": "code", "execution_count": 6, "id": "cd6274bd", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"data": {"text/html": ["\n", "        <iframe\n", "            width=\"970\"\n", "            height=\"550\"\n", "            src=\"images/slides_L12_02.html\"\n", "            frameborder=\"0\"\n", "            allowfullscreen\n", "            \n", "        ></iframe>\n", "        "], "text/plain": ["<IPython.lib.display.IFrame at 0x10f582c10>"]}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": ["#>>>RUN\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L12/slides_L12_02.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "fc55df9e", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>f-test (Chow-test)</h3>\n", "\n", "In the interest of generalizing the t-test, the statistician Ronald Fisher developed the [f-test](https://en.wikipedia.org/wiki/F-test). This is really a generalization of t-test. This has become very useful in physics due to the work of Gregory Chow at MIT in the late 1950s. At that time, he developed the Chow test aimed at trying to come up with a way for how well a fit is behaving. To undersand the chow-test lets delve into the the f-test. \n", "\n", "The f-test is used when you want to compare a few distributions with each other. Imagine for example you have $N$ groups of data, each with $m$ points. If these samples are all from a Gaussian distribution of mean $\\mu$ and variance $\\sigma^{2}$, ie $\\mathcal{N}(x,\\mu,\\sigma)$. Then we can define a new statistic defined conceptually as \n", "\\begin{equation}\n", " f = \\frac{\\rm variance~across~samples}{\\rm variance~within~each~sample}  \n", "\\end{equation}\n", "which should be close to 1 if the samples are all from the same underlying distribution, but it should not be 1 if the samples are from different distributions. \n", "\n", "Lets say we have two distributions $a$ and $b$ each with number of degrees of freedom given by $n_{a}$ and $n_{b}$, we can then write the f-distribution as the ratio of their variances: \n", "\\begin{equation}\n", " f = \\frac{\\frac{S^{2}_{a}}{n_{a}} }{ \\frac{S^{2}_{b}}{n_{b}} }\\\\\n", "\\end{equation}\n", "\n", "More generally, we can write the f-statistic as the variance of distinct samples over the average variance over the individual samples. For a total amount of $N$ samples with $K$ groups, each with $n_{i}$ events within and mean $\\bar{x}_{i}$, we can write the f-statistic as \n", "\\begin{equation}\n", " \\sigma^{2}_{\\rm group} = \\frac{1}{K-1}\\sum_{i=1}^{K} n_{i} \\left(\\bar{x}_{i}-\\bar{x}\\right)^2 \\\\\n", " \\sigma^2 = \\frac{1}{N-K}\\sum_{i=1}^{K}\\sum_{j=1}^{n_{i}}\\left(x_{ij}-\\bar{x}_{i}\\right)^{2}\\\\ \n", " f = \\frac{  \\sigma^{2}_{\\rm group} }{\\sigma^{2}}\n", "\\end{equation}\n", "The idea is that the numerator and denominator are both $\\chi^{2}$ distributed variables with $K-1$ degrees of freedom on top, and $N-K$ degrees of freedom on the bottom. This statistic is most powerful for checking if the variances are consistent with being from the same distribution or a different distribution. \n", "\n", "From the above formulas, it has been derived that the f-statistic follows [$f$-distribution](https://en.wikipedia.org/wiki/F-distribution), which has a very complex form that we will write here once for posterity. \n", "\\begin{equation}\n", "f(x,d_{1}=K-1,d_{2}=N-K) = \\frac{1}{\\beta\\left(\\frac{d_{1}}{2},\\frac{d_{2}}{2}\\right)}\\left(\\frac{d_{1}}{d_{2}}\\right)^{\\frac{d_{1}}{2}}x^{\\frac{d_{1}}{2}-1}\\left(1+\\frac{d_{1}}{d_{2}}x\\right)^{-\\frac{d_{1}+d_{2}}{2}}\\\\\n", "\\end{equation}\n", "where $\\beta(x,y)$ is the [$\\beta$-function](https://en.wikipedia.org/wiki/Beta_function).\n", "\n", "So why do we care about the f-test? \n", "\n", "Recall, that to get a good fit you want the fit residuals to look like a gaussian distribution. Unfortunately, its often that case that the fit residuals are not gaussian. Lets say you fit a line to a distribution, and the fit is not good. \n", "\n", "Well then you can try fitting a more complicated function, how about a quadratic. The residuals seem better. \n", "\n", "What about a 3rd order polynomial? \n", "\n", "When do we know where to stop. That is where the f-statistic can help us. The idea is that we can compare the fit residuals from each function. If the fit residual ratio has a high likeilhood given an f-distribution, then we know that the additional polynomial is not needed. \n", "\n", "More generally, the f-distribution tells you how do you tell that fit is better? One ways is to see if the $\\chi^{2}$ is better, but what if the $\\chi^{2}$ is approximately the same? Moreover what if your $\\chi^{2}$ was originally good, but not gaussian. Comparing the residuals of the two fits, can tell us if our new fit function is actually better. The f-test helps us quantify this. \n", "\n", "\n", "Well lets say we want to compare the variance of two fits. \n", "If they variances are from the same underlying distribution they will follow the f-statistic. This comes up when we are trying to figure out if our fit is actually working. \n", "\n", "To do that, lets try to fit some Higgs boson data, lets load 2011, Higgs boson to two photons. First lets look at the data. \n"]}, {"cell_type": "code", "execution_count": null, "id": "fa6956c1", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#>>>RUN\n", "\n", "import numpy as np\n", "import csv\n", "import matplotlib.pyplot as plt\n", "\n", "#Lets fit a bunch of polynomails with lmfit\n", "x = []\n", "y = []\n", "y_err = []\n", "label='out_2011.txt'\n", "with open(label,'r') as csvfile:\n", "    plots = csv.reader(csvfile, delimiter=' ')\n", "    for row in plots:\n", "        if float(row[1]) > 150 or float(row[1]) < 110:\n", "            continue\n", "        x.append(float(row[1]))\n", "        y.append(float(row[2]))\n", "        #add poisson uncertainties                                                                                                 \n", "        y_err.append(np.sqrt(float(row[2])))\n", "\n", "weights = np.linspace(0.,len(y),num=len(y))\n", "for i0 in range(len(y)):\n", "    weights[i0] = float(1./y_err[i0])\n", "\n", "#Now we plot it. \n", "plt.errorbar(x,y,y_err, lw=2,fmt=\".k\", capsize=0)\n", "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n", "plt.ylabel(\"$N_{events}$\")\n", "plt.show()"]}, {"cell_type": "markdown", "id": "6e368ce5", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_12_2'></a>     \n", "\n", "| [Top](#section_12_0) | [Restart Section](#section_12_2) | [Next Section](#section_12_3) |\n"]}, {"cell_type": "markdown", "id": "fef9d02d", "metadata": {"tags": ["learner", "md", "catsoop_02"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 12.2.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 23, "id": "d8ec9ffb", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "b9b03bfb", "metadata": {"tags": ["learner", "catsoop_02", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "d45ac93b", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_12_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L12.3 Fitting Higgs Data</h2>  \n", "\n", "| [Top](#section_12_0) | [Previous Section](#section_12_2) | [Exercises](#exercises_12_3) | [Next Section](#section_12_4) |\n"]}, {"cell_type": "markdown", "id": "4a69b9c0", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section."]}, {"cell_type": "code", "execution_count": 8, "id": "7054ffa8", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"data": {"text/html": ["\n", "        <iframe\n", "            width=\"970\"\n", "            height=\"550\"\n", "            src=\"images/slides_L12_03.html\"\n", "            frameborder=\"0\"\n", "            allowfullscreen\n", "            \n", "        ></iframe>\n", "        "], "text/plain": ["<IPython.lib.display.IFrame at 0x10f582d30>"]}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": ["#>>>RUN\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L12/slides_L12_03.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "a3dcb661", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Fitting Higgs Data</h3>\n", "\n", "The Higgs Data looks like a falling distribution, and its not obvious what to fit this with. Lets just fit it with a bunch of polynomial functions, and see how it works. "]}, {"cell_type": "code", "execution_count": null, "id": "866f32c8", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#>>>RUN\n", "\n", "import lmfit \n", "\n", "def pol0(x,p0):\n", "    pols=[p0]\n", "    y = np.polyval(pols,x)\n", "    return y\n", "\n", "def pol1(x,p0,p1):\n", "    pols=[p0,p1]\n", "    y = np.polyval(pols,x)\n", "    return y\n", "\n", "def pol2(x, p0, p1,p2):\n", "    pols=[p0,p1,p2]\n", "    y = np.polyval(pols,x)\n", "    return y\n", "\n", "def pol3(x, p0, p1,p2,p3):\n", "    pols=[p0,p1,p2,p3]\n", "    y = np.polyval(pols,x)\n", "    return y\n", "\n", "def pol4(x, p0, p1,p2,p3,p4):\n", "    pols=[p0,p1,p2,p3,p4]\n", "    y = np.polyval(pols,x)\n", "    return y\n", "\n", "def pol5(x, p0, p1,p2,p3,p4,p5):\n", "    pols=[p0,p1,p2,p3,p4,p5]\n", "    y = np.polyval(pols,x)\n", "    return y\n", "\n", "def fitModel(iX,iY,iWeights,iFunc):\n", "    model  = lmfit.Model(iFunc)\n", "    p = model.make_params(p0=0,p1=0,p2=0,p3=0,p4=0,p5=0)\n", "    result = model.fit(data=iY,params=p,x=iX,weights=iWeights)\n", "    #result = lmfit.minimize(binnedLikelihood, params, args=(iX,iY,(iY**0.5),iFunc))\n", "    output = model.eval(params=result.params,x=iX)\n", "    return output\n", "\n", "result0 = fitModel(x,y,weights,pol0)\n", "result1 = fitModel(x,y,weights,pol1)\n", "result2 = fitModel(x,y,weights,pol2)\n", "result3 = fitModel(x,y,weights,pol3)\n", "result4 = fitModel(x,y,weights,pol4)\n", "result5 = fitModel(x,y,weights,pol5)\n", "\n", "plt.errorbar(x,y,y_err, lw=2,fmt=\".k\", capsize=0,label=\"data\")\n", "plt.plot(x,result0,label=\"pol0\")\n", "plt.plot(x,result1,label=\"pol1\")\n", "plt.plot(x,result2,label=\"pol2\")\n", "plt.plot(x,result3,label=\"pol3\")\n", "plt.plot(x,result4,label=\"pol4\")\n", "plt.plot(x,result5,label=\"pol5\")\n", "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n", "plt.ylabel(\"$N_{events}$\")\n", "plt.legend()\n", "plt.show()\n", "\n", "#res0.plot()\n", "#result1.plot()\n", "#result2.plot()\n", "#result3.plot()\n", "#result4.plot()\n", "#result5.plot()"]}, {"cell_type": "markdown", "id": "c7f31c1d", "metadata": {"tags": ["learner", "md"]}, "source": ["Let's look at one of the higher order polynomials. "]}, {"cell_type": "code", "execution_count": null, "id": "9610bf9b", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#>>>RUN\n", "\n", "plt.errorbar(x,y,y_err, lw=2,fmt=\".k\", capsize=0,label=\"data\")\n", "plt.plot(x,result5,label=\"pol5\")\n", "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n", "plt.ylabel(\"$N_{events}$\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "b4514ef7", "metadata": {"tags": ["learner", "md"]}, "source": ["You can see its starting to pick up the fluctuations, if add even higher order polynomials, it will pick up even more fluctations. The f-test can tell us when we are adding too many polynomials. To see this, lets look at the residuals. "]}, {"cell_type": "code", "execution_count": null, "id": "7e513c9f", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#>>>RUN\n", "\n", "def residual(iY,iFunc,iYErr):\n", "    resid = (iY-iFunc)/iYErr\n", "    tmp_vals, tmp_bin_edges = np.histogram(resid, bins=10,range=[-7,7])\n", "    tmp_bin_centers = 0.5*(tmp_bin_edges[1:] + tmp_bin_edges[:-1])\n", "    print(\"Mean:\",resid.mean(),\"\\tSTD:\",resid.std())\n", "    return tmp_bin_centers,tmp_vals\n", "\n", "delta_p0,delta_y0 = residual(y,result0,y_err)\n", "delta_p1,delta_y1 = residual(y,result1,y_err)\n", "delta_p5,delta_y5 = residual(y,result5,y_err)\n", "plt.errorbar(delta_p0,delta_y0,yerr=delta_y0**0.5,label=\"pol0\",marker='.',drawstyle = 'steps-mid')\n", "plt.errorbar(delta_p1,delta_y1,yerr=delta_y1**0.5,label=\"pol1\",marker='.',drawstyle = 'steps-mid')\n", "plt.errorbar(delta_p5,delta_y5,yerr=delta_y5**0.5,label=\"pol5\",marker='.',drawstyle = 'steps-mid')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "87e13b8b", "metadata": {"tags": ["learner", "md"]}, "source": ["So as we go to a higher order polynomial,we get to progressively smaller standard deviation. The issue is when do we stop. Lets now compute the f-statistic and compare it to our samples.  The f-statistic is defined at the ratio of the RMS distributions, we can write this as: \n", "\n", "\\begin{equation}\n", " f = \\frac{  \\sigma^{2}_{\\rm group} }{\\sigma^{2}} \\\\\n", " \\sigma^{2}_{\\rm group} = \\frac{ -\\sum_{i=1}^{N} \\left(y_{i}-f_{2}(x_{i})\\right)^{2} + \\sum_{i=1}^{N} \\left(y_{i}- f_{1}(x_{i})\\right)^{2}}{\\Delta^{2\\rightarrow 1}_{\\rm dof}} \\\\\n", " \\sigma^{2} = \\frac{1}{N - n_{\\rm f_{2}~dof} }\\sum_{i=1}^{N} \\left(y_{i}- f_{1}(x_{i})\\right)^{2}\n", "\\end{equation}\n", "\n", "or in other words the variation from a higher order polynomial to a lower order polynomial, should be smaller than the average variation of the residuals. This is the f-statistic. In the next section we will compute the f-statistic for a few instances."]}, {"cell_type": "markdown", "id": "f533e50a", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_12_3'></a>     \n", "\n", "| [Top](#section_12_0) | [Restart Section](#section_12_3) | [Next Section](#section_12_4) |\n"]}, {"cell_type": "markdown", "id": "38855a67", "metadata": {"tags": ["learner", "md", "catsoop_03"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 12.3.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 33, "id": "7e658287", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "7f455b13", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "484e04db", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_12_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L12.4 Computation Using F-statistic</h2>  \n", "\n", "| [Top](#section_12_0) | [Previous Section](#section_12_3) | [Exercises](#exercises_12_4) | [Next Section](#section_12_5) |\n"]}, {"cell_type": "markdown", "id": "f27e0ba3", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "TEXT\n"]}, {"cell_type": "code", "execution_count": 40, "id": "e521b001", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "import scipy.stats as stats \n", "\n", "def residual2(iY,iFunc,iYErr):\n", "    residval = (iY-iFunc)\n", "    return np.sum(residval**2)\n", "    \n", "def ftest(iY,iYerr,f1,f2,ndof1,ndof2):\n", "    r1=residual2(iY,f1,iYerr)\n", "    r2=residual2(iY,f2,iYerr)\n", "    sigma2group=(r1-r2)/(ndof2-ndof1)\n", "    sigma2=r2/(len(y)-ndof2)\n", "    return sigma2group/sigma2\n", "\n", "f10=ftest(y,y_err,result0,result1,1,2)\n", "f21=ftest(y,y_err,result1,result2,2,3)\n", "f32=ftest(y,y_err,result2,result3,3,4)\n", "f43=ftest(y,y_err,result3,result4,4,5)\n", "f54=ftest(y,y_err,result4,result5,5,6)\n", "\n", "xrange=np.linspace(0,300,100)\n", "farr=1-stats.f.cdf(xrange,1,len(y)-5) \n", "fig, ax = plt.subplots(figsize=(9,6))\n", "\n", "ax.axvline(x=f10,linewidth=3,c='b',label='0 to 1')\n", "ax.axvline(x=f21,linewidth=3,c='g',label='1 to 2')\n", "ax.axvline(x=f32,linewidth=3,c='purple',label='2 to 3')\n", "ax.axvline(x=f43,linewidth=3,c='yellow',label='3 to 4')\n", "ax.axvline(x=f54,linewidth=3,c='orange',label='4 to 5')\n", "\n", "ax.set_yscale('log')\n", "plt.plot(xrange,farr,label='f(1,N)')\n", "plt.legend()\n", "plt.xlabel('f-statistic')\n", "plt.ylabel('p-value')\n", "plt.show()\n", "\n", "xrange=np.linspace(0,3,100)\n", "farr=1-stats.f.cdf(xrange,1,len(y)-5) \n", "fig, ax = plt.subplots(figsize=(9,6))\n", "ax.axvline(x=f32,linewidth=3,c='purple',label='2 to 3')\n", "ax.axvline(x=f43,linewidth=3,c='yellow',label='3 to 4')\n", "ax.axvline(x=f54,linewidth=3,c='orange',label='4 to 5')\n", "ax.set_yscale('log')\n", "plt.xlabel('f-statistic')\n", "plt.plot(xrange,farr,label='f(1,N)')\n", "plt.ylabel('p-value')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "5a3006c9", "metadata": {"tags": ["learner", "md"]}, "source": ["So, we see that large probabilities are present already in the 2 to 3 transition, which means that we likely only need a 2nd order polynomial. Let's check the $\\chi^{2}$ value as well. "]}, {"cell_type": "code", "execution_count": 40, "id": "b88e7cdd", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "def chi2(iY,iFunc,iYErr,iNDOF):\n", "    resid = (iY-iFunc)/iYErr\n", "    chi2value = np.sum(resid**2)\n", "    print(\"Mean:\",resid.mean(),\"\\tSTD:\",resid.std())\n", "    chi2prob=1-stats.chi2.cdf(chi2value,len(iY)-iNDOF)\n", "    print(\"chi2 prob:\",chi2prob)\n", "    return chi2value/(len(iY)-iNDOF)\n", "\n", "chi2value=chi2(y,result2,y_err,3)\n", "print(\"Normalized chi2:\",chi2value)\n", "plt.errorbar(x,y,y_err, lw=2,fmt=\".k\", capsize=0,label=\"data\")\n", "plt.plot(x,result2,label=\"pol2\")\n", "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n", "plt.ylabel(\"$N_{events}$\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "2ae97e67", "metadata": {"tags": ["learner", "md"]}, "source": ["Overall, this show the signs of a good fit."]}, {"cell_type": "markdown", "id": "3250b4d1", "metadata": {"tags": ["learner", "md"]}, "source": ["### Challenge Question\n", "\n", "Compare the $\\chi^{2}$ probabilities of the fit? Could we have just done this with a $\\chi^{2}$ test? "]}, {"cell_type": "code", "execution_count": 40, "id": "80889dfd", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "#chi2(y,result1,y_err,2)\n", "chi2(y,result1,y_err,2)"]}, {"cell_type": "markdown", "id": "9550ec9b", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_12_4'></a>     \n", "\n", "| [Top](#section_12_0) | [Restart Section](#section_12_4) | [Next Section](#section_12_5) |\n"]}, {"cell_type": "markdown", "id": "0eed6d01", "metadata": {"tags": ["learner", "md", "catsoop_04"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 12.4.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 43, "id": "8b097693", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "a53f2c46", "metadata": {"tags": ["learner", "catsoop_04", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "c58fe9f9", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_12_5'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L12.5 Fitting the Higgs Signal Part 1</h2>  \n", "\n", "| [Top](#section_12_0) | [Previous Section](#section_12_4) | [Exercises](#exercises_12_5) | [Next Section](#section_12_6) |\n"]}, {"cell_type": "markdown", "id": "6bd1aa7b", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "With all of these pieces together, I would like to compute the significance of the Higgs boson discovery in one of its main channels. To do this, we are going to use all of the tools that we have been going over. Let's first look at the data. For the Higgs boson data, there are 2 years of data each with 5 categories. Here is what all of them look like. "]}, {"cell_type": "code", "execution_count": 50, "id": "739ed23b", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "def load(iLabel,iRange=False):\n", "    x = np.array([])\n", "    y = np.array([])\n", "    label=iLabel\n", "    with open(label,'r') as csvfile:\n", "        plots = csv.reader(csvfile, delimiter=' ')\n", "        for row in plots:\n", "            if not iRange and (float(row[1]) > 150 or float(row[1]) < 110):\n", "                continue\n", "            x = np.append(x,float(row[1]))\n", "            y = np.append(y,float(row[2]))\n", "            #add poisson uncertainties                                                                                                 \n", "    weights = 1./y**0.5 \n", "    return x,y,y**0.5,weights\n", "\n", "def plot(ax,iLabel):\n", "    x,y,y_err,weights=load(iLabel)\n", "    #Now we plot it. \n", "    ax.errorbar(x,y,y_err, lw=2,fmt=\".k\", capsize=0,label=iLabel)\n", "    #ax.x_label(\"$m_{\\gamma\\gamma}$\")\n", "    #ax.y_label(\"$N_{events}$\")\n", "    ax.legend()\n", "    #ax.show()\n", "    \n", "fig, axs = plt.subplots(2, 3)\n", "#2012 data    \n", "plot(axs[0,0],\"out.txt\")\n", "plot(axs[0,1],\"out2.txt\")\n", "plot(axs[0,2],\"out3.txt\")\n", "plot(axs[1,0],\"out4.txt\")\n", "plot(axs[1,1],\"out5.txt\")\n", "plt.show()\n", "\n", "fig, axs = plt.subplots(2, 3)\n", "#2011 data    \n", "plot(axs[0,0],\"out_2011.txt\")\n", "plot(axs[0,1],\"out2_2011.txt\")\n", "plot(axs[0,2],\"out3_2011.txt\")\n", "plot(axs[1,0],\"out4_2011.txt\")\n", "plot(axs[1,1],\"out5_2011.txt\")\n"]}, {"cell_type": "markdown", "id": "88f3e4fb", "metadata": {"tags": ["learner", "md"]}, "source": ["As you can see, from the above plots, there are way more points in the 2012 data. Lets take the category with the largest number of points, and perform an f-test on it, we can neglect the signal for now, but we will get back to that in a sec.  "]}, {"cell_type": "code", "execution_count": 50, "id": "ec254f2f", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "def fitAll(iLabel,iPlot=False):\n", "    x,y,y_err,weights=load(iLabel)\n", "    result0 = fitModel(x,y,weights,pol0)\n", "    result1 = fitModel(x,y,weights,pol1)\n", "    result2 = fitModel(x,y,weights,pol2)\n", "    result3 = fitModel(x,y,weights,pol3)\n", "    result4 = fitModel(x,y,weights,pol4)\n", "    result5 = fitModel(x,y,weights,pol5)\n", "\n", "    if iPlot:\n", "        plt.errorbar(x,y,y_err, lw=2,fmt=\".k\", capsize=0,label=\"data\")\n", "        plt.plot(x,result0,label=\"pol0\")\n", "        plt.plot(x,result1,label=\"pol1\")\n", "        plt.plot(x,result2,label=\"pol2\")\n", "        plt.plot(x,result3,label=\"pol3\")\n", "        plt.plot(x,result4,label=\"pol4\")\n", "        plt.plot(x,result5,label=\"pol5\")\n", "        plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n", "        plt.ylabel(\"$N_{events}$\")\n", "        plt.legend()\n", "        plt.show()\n", "    return x,y,y_err,result0,result1,result2,result3,result4,result5\n", "\n", "def ftestAll(iLabel):\n", "    x,y,y_err,result0,result1,result2,result3,result4,result5=fitAll(iLabel)\n", "    f10=ftest(y,y_err,result0,result1,1,2)\n", "    f21=ftest(y,y_err,result1,result2,2,3)\n", "    f32=ftest(y,y_err,result2,result3,3,4)\n", "    f43=ftest(y,y_err,result3,result4,4,5)\n", "    f54=ftest(y,y_err,result4,result5,4,5)\n", "    print(\"f 1 to 0:\",1-stats.f.cdf(f10,1,len(y)-1))\n", "    print(\"f 2 to 1:\",1-stats.f.cdf(f21,1,len(y)-2))\n", "    print(\"f 3 to 2:\",1-stats.f.cdf(f32,1,len(y)-3))\n", "    print(\"f 4 to 3:\",1-stats.f.cdf(f43,1,len(y)-4))\n", "    print(\"f 5 to 4:\",1-stats.f.cdf(f54,1,len(y)-5))\n", "    \n", "fitAll(\"out.txt\",True)\n", "ftestAll(\"out.txt\")\n", "\n", "print(\"out2 Test\")\n", "fitAll(\"out2.txt\",True)\n", "ftestAll(\"out2.txt\")"]}, {"cell_type": "markdown", "id": "096874e3", "metadata": {"tags": ["learner", "md"]}, "source": ["So from this looks like a 4th order polynomialgives an f-test above roughly 5% for both the category with the largest yield and the second largest yield. This seems reaonsable for us to use as our background function. Let's proceed with a signal function.  "]}, {"cell_type": "markdown", "id": "372b69dd", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_12_5'></a>     \n", "\n", "| [Top](#section_12_0) | [Restart Section](#section_12_5) | [Next Section](#section_12_6) |\n"]}, {"cell_type": "markdown", "id": "1cd07257", "metadata": {"tags": ["learner", "md", "catsoop_05"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 12.5.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 53, "id": "b5b6e377", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "8d33c4c4", "metadata": {"tags": ["learner", "catsoop_05", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "693a60a3", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_12_6'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L12.6 Fitting the Higgs Signal Part 2</h2>  \n", "\n", "| [Top](#section_12_0) | [Previous Section](#section_12_5) | [Exercises](#exercises_12_6) | [Next Section](#section_12_7) |\n"]}, {"cell_type": "markdown", "id": "8af1f320", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "Now, to fit a Higgs signal, what we want to do is a hypothesis test like we did above. Except now, we will cast our hypothesis, slightly differently to before. \n", "\n", "**Null Hypothesis** The Higgs signal has a mass of $m_{\\gamma\\gamma}$ at a specific $m_{0}$, and a fixed width 1.2 GeV. \n", "\n", "**Alternative Hypothesis** The Higgs signal is not there. \n"]}, {"cell_type": "code", "execution_count": 60, "id": "e260c0fc", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "def sigpol4(x,p0,p1,p2,p3,p4,amp,mass,sigma):\n", "    bkg=pol4(x,p0,p1,p2,p3,p4)\n", "    sig=amp*stats.norm.pdf(x,mass,sigma)\n", "    return sig+bkg\n", "\n", "def fitModel(iX,iY,iWeights,iM,iFunc):\n", "    model  = lmfit.Model(iFunc)\n", "    p = model.make_params(p0=0,p1=0,p2=0,p3=0,p4=0,p5=0,amp=0,mass=iM,sigma=1.2)\n", "    try:\n", "        p[\"mass\"].vary=False\n", "        p[\"sigma\"].vary=False\n", "    except:\n", "      a=1\n", "      #print(\"Mass and Sigma not in fit\")\n", "    result = model.fit(data=iY,params=p,x=iX,weights=iWeights)\n", "    output = model.eval(params=result.params,x=iX)\n", "    return output,result.residual\n", "\n", "def fitSig(iLabel,iM,SBfunc,Bfunc,iPlot=False):\n", "    x,y,y_err,weights=load(iLabel)\n", "    resultSB,likeSB=fitModel(x,y,weights,iM,SBfunc)\n", "    resultB, likeB =fitModel(x,y,weights,iM,Bfunc)\n", "    if iPlot:\n", "        plt.errorbar(x,y,y_err, lw=2,fmt=\".k\", capsize=0,label=\"data\")\n", "        plt.plot(x,resultSB,label=\"S+B\")\n", "        plt.plot(x,resultB, label=\"B\")\n", "        plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n", "        plt.ylabel(\"$N_{events}$\")\n", "        plt.legend()\n", "        plt.show()\n", "    return np.sum(likeB**2)-np.sum(likeSB**2)\n", "\n", "NLL=fitSig(\"out.txt\",125,sigpol4,pol4,True)\n", "print(\"2NLL:\",NLL,\"p-value\",1-stats.chi2.cdf(NLL,1))\n", "\n", "NLL=fitSig(\"out2.txt\",125,sigpol4,pol4,True)\n", "print(\"2NLL:\",NLL,\"p-value\",1-stats.chi2.cdf(NLL,1))\n", "\n", "NLL=fitSig(\"out3.txt\",125,sigpol4,pol4,True)\n", "print(\"2NLL:\",NLL,\"p-value\",1-stats.chi2.cdf(NLL,1))\n", "\n", "NLL=fitSig(\"out4.txt\",125,sigpol4,pol4,True)\n", "print(\"2NLL:\",NLL,\"p-value\",1-stats.chi2.cdf(NLL,1))\n", "\n", "NLL=fitSig(\"out5.txt\",125,sigpol4,pol4,True)\n", "print(\"2NLL:\",NLL,\"p-value\",1-stats.chi2.cdf(NLL,1))"]}, {"cell_type": "markdown", "id": "a7b3fc73", "metadata": {"tags": ["learner", "md"]}, "source": ["Wow, we see a fairly significant Higgs bump at 125, but lets scan the mass and make the so called p-value plot. This is just a plot of the significance as a function of mass. "]}, {"cell_type": "code", "execution_count": 60, "id": "e1b1a24f", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "def pvalueCalc(iLabel,pMass,iSBFunc,iBFunc):\n", "    NLL=fitSig(iLabel,pMass,iSBFunc,iBFunc,False)\n", "    NLLp = 1-stats.chi2.cdf(NLL,1)\n", "    return NLLp\n", "\n", "def pvaluePlot(iLabel,iSBFunc,iBFunc):\n", "    pvalue = np.array([])\n", "    massrange=np.linspace(110,150,120)\n", "    for pMass in massrange:\n", "        pvalue = np.append(pvalue,pvalueCalc(iLabel,pMass,iSBFunc,iBFunc))\n", "    return massrange,pvalue\n", "\n", "m0,p0 = pvaluePlot(\"out.txt\",sigpol4,pol4)\n", "m1,p1 = pvaluePlot(\"out2.txt\",sigpol4,pol4)\n", "m2,p2 = pvaluePlot(\"out3.txt\",sigpol4,pol4)\n", "m3,p3 = pvaluePlot(\"out4.txt\",sigpol4,pol4)\n", "m4,p4 = pvaluePlot(\"out5.txt\",sigpol4,pol4)\n", "\n", "plt.plot(m0,p0,label=\"Category 1\")\n", "plt.plot(m1,p1,label=\"Category 2\")\n", "plt.plot(m2,p2,label=\"Category 3\")\n", "plt.plot(m3,p3,label=\"Category 4\")\n", "plt.plot(m4,p4,label=\"Category 5\")\n", "plt.ylim((0.0001,1))\n", "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n", "plt.ylabel(\"p-value\")\n", "plt.yscale(\"log\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "fd74759f", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_12_6'></a>     \n", "\n", "| [Top](#section_12_0) | [Restart Section](#section_12_6) | [Next Section](#section_12_7) |\n"]}, {"cell_type": "markdown", "id": "0f1f80f8", "metadata": {"tags": ["learner", "md", "catsoop_06"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 12.6.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 63, "id": "04717209", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "989c4690", "metadata": {"tags": ["learner", "catsoop_06", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "1748ed70", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_12_7'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L12.7 Fitting Using Higher Order Polynomials</h2>  \n", "\n", "| [Top](#section_12_0) | [Previous Section](#section_12_6) | [Exercises](#exercises_12_7) | [Next Section](#section_12_8) |\n"]}, {"cell_type": "markdown", "id": "a9b0c185", "metadata": {"tags": ["learner", "md"]}, "source": ["### Challenge Question\n", "\n", "Compute the Higgs boson p-value signficance plot for category 1 with a 5th order polynomial? How does it compare. "]}, {"cell_type": "code", "execution_count": 60, "id": "46289a2c", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "def pol5(x, p0, p1,p2,p3,p4,p5):\n", "    pols=[p0,p1,p2,p3,p4,p5]\n", "    y = np.polyval(pols,x)\n", "    return y\n", "\n", "def sigpol5(x,p0,p1,p2,p3,p4,p5,amp,mass,sigma):\n", "    bkg=pol5(x,p0,p1,p2,p3,p4,p5)\n", "    sig=amp*stats.norm.pdf(x,mass,sigma)\n", "    return sig+bkg\n", "\n", "m0,p0 = pvaluePlot(\"out.txt\",sigpol5,pol5)\n", "m1,p1 = pvaluePlot(\"out2.txt\",sigpol5,pol5)\n", "m2,p2 = pvaluePlot(\"out3.txt\",sigpol5,pol5)\n", "m3,p3 = pvaluePlot(\"out4.txt\",sigpol5,pol5)\n", "m4,p4 = pvaluePlot(\"out5.txt\",sigpol5,pol5)\n", "\n", "plt.plot(m0,p0,label=\"Category 1\")\n", "plt.plot(m1,p1,label=\"Category 2\")\n", "plt.plot(m2,p2,label=\"Category 3\")\n", "plt.plot(m3,p3,label=\"Category 4\")\n", "plt.plot(m4,p4,label=\"Category 5\")\n", "plt.ylim((0.0001,1))\n", "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n", "plt.ylabel(\"p-value\")\n", "plt.yscale(\"log\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": 60, "id": "65544eb2", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "#answer\n", "def sigpol5(x,p0,p1,p2,p3,p4,p5,amp,mass,sigma):\n", "    bkg=pol5(x,p0,p1,p2,p3,p4,p5)\n", "    sig=amp*stats.norm.pdf(x,mass,sigma)\n", "    return sig+bkg\n", "\n", "NLL=fitSig(\"out.txt\",125,sigpol5,pol5,True)\n", "\n", "\n", "m03,p03 = pvaluePlot(\"out.txt\",sigpol5,pol5)\n", "plt.plot(m0,p0,label=\"Category 1 4th order\")\n", "plt.plot(m03,p03,label=\"Category 1 5th order\")\n", "plt.ylabel(\"p-value\")\n", "plt.yscale(\"log\")\n", "plt.legend()\n", "plt.show()\n", "\n", "#The 5th order polynomial is much less sensivitive since there are more degrees of freedom"]}, {"cell_type": "markdown", "id": "329a647d", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Combining p-values</h3>\n", "\n", "Now, if we have 5 experiments each giving a p-value at a specific mass point. How, do we combine these p-values. The strategy is to realize these are each independent experiments. Moreover, it can be shown that if you have a flat prior in probability, and if you take the log of this prior and multiply by 2 $2\\log(p)$ this distribution is approximately that of a $\\chi^{2}$ distribution of 2 degrees of freedom. \n", "\n", "\n", "We can see this analytically by noting that $\\chi^{2}$ distribution is given by \n", "\n", "\\begin{equation}\n", "\\chi^{2}(x,\\nu) = \\frac{1}{2^{\\nu/2}\\Gamma(\\nu/2)} x^{\\nu/2-1}e^{-x/2}\n", "\\end{equation}\n", "\n", "For $\\nu=2$, we have it is just a an exponential distribution given by:\n", "\n", "\\begin{equation}\n", "\\chi^{2}(x,\\nu=2) = \\frac{1}{2}e^{-x/2}\n", "\\end{equation}\n", "\n", "Now for a distribution that is flat is distribution from 0 to 1, such as the $p-value$ of a random measurement. Then if we take the log of that, we find\n", "\\begin{equation}\n", " y = -2\\log(x)\\rightarrow e^{-\\frac{y}{2}}=x \\\\\n", " dx = -\\frac{1}{2}e^{\\frac{-y}{2}} dy \\\\\n", "\\end{equation}\n", "To equate probability distributions, we want to solve for scenario where the probabilities over a range are equal, namely, to get it as a function of $y$, we can write, noting $p(x)=1$\n", "\\begin{equation}\n", " p(y) dy = p(x) dx \\\\\n", " p(y) = p(x) \\frac{dx}{dy}\\\\\n", " p(y) = p(f^{-1}(y)) \\frac{dx}{dy}\\\\\n", " p(y) = \\frac{dx}{dy} \\\\\n", " p(y)=e^{-\\frac{y}{2}} \\\\\n", "\\end{equation}\n", "or for $x$ a flat distribution, we have that $y$ has to be distributed such that $p(y)=e^{-\\frac{y}{2}}$ "]}, {"cell_type": "code", "execution_count": 60, "id": "55699f2c", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "samples = np.random.uniform(0,1,10000)\n", "plt.hist(-2.*np.log(samples),bins=30)\n", "plt.xlabel(\"y=log(x)\")\n", "plt.ylabel(\"N\")"]}, {"cell_type": "markdown", "id": "92692cb2", "metadata": {"tags": ["learner", "md"]}, "source": ["Now going back to our calculation, we can immediately relate \n", "\\begin{equation}\n", "\\chi^{2}_{\\nu=2} = -2 \\log(p_{i})\n", "\\end{equation}\n", "Now, lets say we have $n$ measurements each with probability $p_{i}$ for the i-th category. If we take the $2\\log(p_{i})$ and sum the distributions, we have a sum of $\\chi^{2}$ distributions of 2 degrees of freedom. This is just a $\\chi^{2}_{\\nu=2n}$ distribution. \n", "\n", "\\begin{equation}\n", "\\chi^{2}_{\\nu=2n} = -2 \\sum_{i=1}^{n} \\log(p_{i})\n", "\\end{equation}\n", "\n", "From this relation, we can immediately get the combined p-value by checking up the p-value of a $\\chi^{2}_{\\nu=2n}$ distribution.  Lets see this in action!"]}, {"cell_type": "code", "execution_count": 60, "id": "788dbc92", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "def pvalueCalc(iLabel,pMass,iSBFunc,iBFunc):\n", "    logp=0\n", "    for pLabel in iLabel:\n", "        NLL=fitSig(pLabel,pMass,iSBFunc,iBFunc,False)\n", "        NLLp = 1.-stats.chi2.cdf(NLL,1)\n", "        logp = logp - 2.*np.log(NLLp)\n", "    pPVal  = 1-stats.chi2.cdf(logp,2*len(iLabel))\n", "    return pPVal\n", "\n", "files=[\"out.txt\",\"out2.txt\",\"out3.txt\",\"out4.txt\",\"out5.txt\"]\n", "mC,pC = pvaluePlot(files,sigpol4,pol4)\n", "\n", "for pVal in range(4):\n", "    sigmas = 1-stats.norm.cdf(pVal+1)\n", "    plt.axhline(y=sigmas, color='r', linestyle='-')\n", "plt.plot(m0,p0,label=\"Category 1\")\n", "plt.plot(m1,p1,label=\"Category 2\")\n", "plt.plot(m2,p2,label=\"Category 3\")\n", "plt.plot(m3,p3,label=\"Category 4\")\n", "plt.plot(m4,p4,label=\"Category 5\")\n", "plt.plot(mC,pC,label=\"Combined Category\")\n", "plt.ylim((0.0001,1))\n", "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n", "plt.ylabel(\"p-value\")\n", "plt.yscale(\"log\")\n", "plt.legend()\n", "plt.show()\n"]}, {"cell_type": "code", "execution_count": 60, "id": "009e5e4b", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "plt.plot(mC,pC,label=\"Category 1\")\n", "plt.ylim((0.0001,1))\n", "plt.yscale(\"log\")"]}, {"cell_type": "markdown", "id": "a07e92c4", "metadata": {"tags": ["learner", "md"]}, "source": ["Ok, now that we have done that, lets compare our result for the 8TeV measurement in the original paper [here](https://arxiv.org/pdf/1407.0558.pdf). If you look at that plot, you will see that there is a significance of almost 4 standard deviations for the 8TeV data, whereas we only have 3 standard deviations. The reason is that the analysis in the paper is more complicated. To compare our result with something closer, look at the Higgs discovery [paper](https://arxiv.org/pdf/1207.7235.pdf). Here, we are more sensitive, but we also have more data. "]}, {"cell_type": "markdown", "id": "4712495d", "metadata": {"tags": ["learner", "md"]}, "source": ["### Challenging Challenge Question \n", "\n", "Compute the best fit mass for the Higgs boson? How does it compare to the true value"]}, {"cell_type": "code", "execution_count": 60, "id": "ee94281c", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#Answer, the strategy here is to realize that p-value plot is also 2*Log(L) of our best fit, thus, we just need to go 1 standard deviation from the minimum in likelihood\n", "logC = stats.chi2.ppf(1-pC,1)\n", "logC = -logC+np.max(logC)\n", "plt.plot(mC,logC,label=\"Combined\")\n", "plt.ylim(0,2)\n", "plt.xlim(124,126)\n", "plt.hlines(1,120,130)\n", "plt.show()\n", "#Its roughly 124.8-125.5 => 125.2+/-0.35 (Its almost spot on!)"]}, {"cell_type": "markdown", "id": "1bacb197", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_12_7'></a>     \n", "\n", "| [Top](#section_12_0) | [Restart Section](#section_12_7) | [Next Section](#section_12_8) |\n"]}, {"cell_type": "markdown", "id": "1c75bca6", "metadata": {"tags": ["learner", "md", "catsoop_07"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 12.7.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 73, "id": "a9b46779", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "98232553", "metadata": {"tags": ["learner", "catsoop_07", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "a47b3116", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_12_8'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L12.8 Building Interpolated Distributions</h2>  \n", "\n", "| [Top](#section_12_0) | [Previous Section](#section_12_7) | [Exercises](#exercises_12_8) | [Next Section](#section_12_9) |\n"]}, {"cell_type": "markdown", "id": "39c165da", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section."]}, {"cell_type": "code", "execution_count": 9, "id": "f28e9928", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"data": {"text/html": ["\n", "        <iframe\n", "            width=\"970\"\n", "            height=\"550\"\n", "            src=\"images/slides_L12_08.html\"\n", "            frameborder=\"0\"\n", "            allowfullscreen\n", "            \n", "        ></iframe>\n", "        "], "text/plain": ["<IPython.lib.display.IFrame at 0x10f582970>"]}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": ["#>>>RUN\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L12/slides_L12_08.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "4acb80f0", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "To find the right fit function, we used a library of functions to profile and find the minimum. Thanks to the f-statistic, we don't need to just throw one function at the problem, we can throw many. In fact, modern searches aiming for the most sensitivity will send a library of functions to fit a signal and not just one. For a more detailed analysis of how you would do this look at this [paper](https://arxiv.org/pdf/1408.6865.pdf). \n", "\n", "However, what we can also do is actually build functions by just throwing it to the data. To give you two ways to do this, let's first try a spline interpolated function. "]}, {"cell_type": "code", "execution_count": 60, "id": "8ddb27f4", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "#Now lets load some data and do gaussian kernals with it\n", "x,y,y_err,weights=load(\"out_2011.txt\")\n", "\n", "\n", "from scipy import interpolate\n", "tck = interpolate.splrep(x, y) #setup the spline\n", "x2 = np.linspace(110, 150) #range\n", "y2 = interpolate.splev(x2, tck)#apply the spline\n", "\n", "plt.plot(x, y, 'go',label='data')\n", "plt.plot(x2, y2, 'b',label='spline')\n", "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n", "plt.ylabel(\"$N_{events}$\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "a882678d", "metadata": {"tags": ["learner", "md"]}, "source": ["So what happened is, we just took the data and turned it into a smooth function that we can evaulate anywhere. This is done by chunking up the data into little pieces and fitting higher order polynomials to each. We end up with a function. \n", "\n", "Since its a function, we can do whatever we want with it. Let's smooth this function out by convolving it with a Gaussian distribution. "]}, {"cell_type": "code", "execution_count": 60, "id": "027ec626", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>RUN\n", "\n", "#spline convolve by hand\n", "def splineconvolve(tck,f2,x,iMin=-15,iMax=15,iN=500):\n", "    step=float((iMax-iMin))/float(iN)\n", "    pInt=0\n", "    for i0 in range(iN):\n", "            pX   = i0*step+iMin\n", "            pVal = interpolate.splev(x-pX,tck)*f2(pX)\n", "            pInt += pVal*step\n", "    return pInt\n", "\n", "def gaussian(x,mean=0,sigma=5):\n", "    return 1./(sigma * np.sqrt(2 * np.pi)) * np.exp( - (x - mean)**2 / (2 * sigma**2)) \n", "\n", "fig, ax = plt.subplots()\n", "x_in=np.linspace(122, 142, 10)\n", "conv_out=[]\n", "for val in x_in:\n", "    pConv_out=splineconvolve(tck,gaussian,val)\n", "    conv_out.append(pConv_out)\n", "\n", "#now we cna plot it\n", "plt.plot(x, y, 'go',label='data')\n", "plt.plot(x2, y2, 'b',label='spline')\n", "plt.plot(x_in,conv_out,c='orange',label='convolution')\n", "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n", "plt.ylabel(\"$N_{events}$\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "22f107df", "metadata": {"tags": ["learner", "md"]}, "source": ["In fact, in recent times interpolation with all sorts of functions, not just polynomials have become very popular. Just to show you another one that is used often, lets try Gaussian processes. Gaussian processes, aim to fit guassian distributiosn to describe the data. The strategy, conceptually, is like the f-test. Keep adding Gaussian's to fit the data until it is well described. See [here](https://en.wikipedia.org/wiki/Gaussian_process) for a much deeper explanation for how this would work. "]}, {"cell_type": "code", "execution_count": 60, "id": "2079543e", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["!pip install george\n", "import george\n", "from george import kernels\n", "\n", "kernel = np.var(y) * kernels.Matern52Kernel(5.0)\n", "#kernel = np.var(y) * kernels.Matern52Kernel(125.0)\n", "gp = george.GP(kernel)\n", "gp.compute(x, y_err)\n", "x_pred = np.linspace(110, 150, 100)\n", "pred, pred_var = gp.predict(y, x_pred, return_var=True)\n", "\n", "plt.fill_between(x_pred, pred - np.sqrt(pred_var), pred + np.sqrt(pred_var),color=\"k\", alpha=0.2,label=\"gaussian Process\")\n", "plt.plot(x_pred, pred, \"k\", lw=1.5, alpha=0.5)\n", "plt.errorbar(x, y, yerr=y_err, fmt=\".k\", capsize=0)\n", "plt.plot(x2, y2, 'b',label='spline')\n", "plt.plot(x_in,conv_out,c='orange',label='convolution')\n", "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n", "plt.ylabel(\"$N_{events}$\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "0d2d3243", "metadata": {"tags": ["learner", "md"]}, "source": ["As with the spline, there are parameters for how it averages over the points. Try the kernal 125 above, you will see that this is is effectively changing the size of the window of interpolation. This will allow us to either smooth or unsmooth our distribution.  "]}, {"cell_type": "markdown", "id": "83d9375f", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_12_8'></a>     \n", "\n", "| [Top](#section_12_0) | [Restart Section](#section_12_8) | [Next Section](#section_12_9) |\n"]}, {"cell_type": "markdown", "id": "04a3f8af", "metadata": {"tags": ["learner", "md", "catsoop_08"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 12.8.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 83, "id": "39bdf323", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "144bc5e5", "metadata": {"tags": ["learner", "catsoop_08", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "d0a69a1a", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_12_9'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L12.9 Dealing with non-Analytic Forms</h2>     \n", "\n", "| [Top](#section_12_0) | [Previous Section](#section_12_8) | [Exercises](#exercises_12_9) |\n"]}, {"cell_type": "markdown", "id": "a5197264", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section."]}, {"cell_type": "code", "execution_count": 10, "id": "c432c4e8", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"data": {"text/html": ["\n", "        <iframe\n", "            width=\"970\"\n", "            height=\"550\"\n", "            src=\"images/slides_L12_09.html\"\n", "            frameborder=\"0\"\n", "            allowfullscreen\n", "            \n", "        ></iframe>\n", "        "], "text/plain": ["<IPython.lib.display.IFrame at 0x10f582c70>"]}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": ["#>>>RUN\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L12/slides_L12_09.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "c1dece58", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "At this point, we have gone a bit away from just fitting functions. Its not always the case that we have a nice function that we can fit to describe our data. It is sometimes the case that we have a simulated shape. For High energy physics, simulated shapes often come from so called \"Monte-Carlo\" simulation. In this approach, we construct distributions by randomly sampling many millions of events. The resulting simulated events can then be treated like data. The data set below represents another Higgs boson channel, the Higgs decay to 4 leptons. However, in this instance, we want to fit the peak at 90 with a simulation of the peak. "]}, {"cell_type": "code", "execution_count": 60, "id": "bb1a5e9f", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["x,y_data,y_err,weights=load(\"data.txt\",True)\n", "x,y_mc,y_mc_err,_=load(\"zz_narrow.txt\",True)\n", "\n", "tck = interpolate.splrep(x, y_mc)\n", "x2 = np.linspace(50, 160)\n", "y2 = interpolate.splev(x2, tck)\n", "plt.errorbar(x,y_data,yerr=y_err,marker='.',linestyle = 'None', color = 'black')\n", "plt.plot(x2, y2, 'b')\n", "plt.plot(x,y_mc,drawstyle = 'steps-mid')\n", "plt.xlabel(\"$m_{4\\ell}$\")\n", "plt.ylabel(\"$N_{events}$\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "8276bbdf", "metadata": {"tags": ["learner", "md"]}, "source": ["There are clearly a few things off. First of all the number of events is off. Secondly, the shapes don't look exactly the same. \n", "\n", "Oftentimes, when we actually want to do a precision fit, we will rely on our simulated samples to extract the signal. What we will do is allow the shape to be modified by a number of different approaches. One appraoch is to apply a numerical convolution of the shape with a gaussian distribution, so that we can smear it out, making it wider. This what we will do here. "]}, {"cell_type": "code", "execution_count": 60, "id": "204630c1", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#spline convolve by hand\n", "def splineconvolvegaus(x,mean,sigma,iMin=-15,iMax=15,iN=500):\n", "    step=float((iMax-iMin))/float(iN)\n", "    pInt=0\n", "    for i0 in range(iN):\n", "            pX   = i0*step+iMin\n", "            pVal = interpolate.splev(x-pX,tck)*gaussian(pX,mean,sigma)\n", "            pInt += pVal*step\n", "    return pInt\n", "\n", "def gausconv(x,mean,sigma,amp,a,b):\n", "    val=splineconvolvegaus(x,mean,sigma)*amp\n", "    val=a + b*x + val\n", "    return val\n", "\n", "model  = lmfit.Model(gausconv)\n", "p = model.make_params(mean=0,sigma=1.0,amp=3.0,a=1,b=0)\n", "p[\"sigma\"].value=0.1\n", "#p[\"sigma\"].vary=False\n", "result = model.fit(data=y_data,params=p,x=x,weights=weights)\n", "lmfit.report_fit(result)\n", "result.plot()"]}, {"cell_type": "code", "execution_count": 60, "id": "e7b56476", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["def splineconvolvegaus(tck,f2,x,mean,sigma,iMin=-15,iMax=15,iN=500):\n", "    step=float((iMax-iMin))/float(iN)\n", "    pInt=0\n", "    for i0 in range(iN):\n", "            pX   = i0*step+iMin\n", "            pVal = interpolate.splev(x-pX,tck)*f2(pX,mean,sigma)\n", "            pInt += pVal*step\n", "    return pInt\n", "\n", "def gausconv(x,mean,sigma,sig,baseline,slope):\n", "    val=splineconvolvegaus(tck,gaussian,x,mean,sigma)\n", "    output = baseline+sig*val+slope*x\n", "    return output\n", "\n", "model  = lmfit.Model(gausconv)\n", "p = model.make_params(mean=0,sigma=1,sig=2,baseline=2,slope=0)\n", "p[\"mean\"].vary = False\n", "result = model.fit(data=y_data, params=p, x=x, weights=weights)\n", "lmfit.report_fit(result)\n", "result.plot()\n", "\n", "#Now lets not smear the data\n", "p[\"sigma\"].value = 0.01\n", "p[\"sigma\"].vary = False\n", "result = model.fit(data=y_data, params=p, x=x, weights=weights)\n", "lmfit.report_fit(result)\n", "result.plot()"]}, {"cell_type": "markdown", "id": "585a7360", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Going beyond</h3>\n", "\n", "This is just the start of construction fit functions. The next set of tools we can explore is how to build deep learning algorithms to model the data. "]}, {"cell_type": "markdown", "id": "21481a11", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_12_9'></a>   \n", "\n", "| [Top](#section_12_0) | [Restart Section](#section_12_9) |\n"]}, {"cell_type": "markdown", "id": "b340ecf0", "metadata": {"tags": ["learner", "md", "catsoop_09"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 12.9.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": 93, "id": "83c36c83", "metadata": {"tags": ["draft", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": []}], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "9b1d1c71", "metadata": {"tags": ["learner", "catsoop_09", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}