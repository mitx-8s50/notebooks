{"cells": [{"cell_type": "markdown", "id": "6af4c17a", "metadata": {"tags": ["learner", "md"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 4: Introduction to Data Fitting</h1>\n"]}, {"cell_type": "markdown", "id": "1655a9e6", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_4_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "208a0b68", "metadata": {"tags": ["learner", "md"]}, "source": ["<table style=\"width:100%\">\n", "    <colgroup>\n", "       <col span=\"1\" style=\"width: 40%;\">\n", "       <col span=\"1\" style=\"width: 15%;\">\n", "       <col span=\"1\" style=\"width: 45%;\">\n", "    </colgroup>\n", "    <tr>\n", "        <th style=\"text-align: left; font-size: 13pt;\">Section</th>\n", "        <th style=\"text-align: left; font-size: 13pt;\">Exercises</th>\n", "        <th style=\"text-align: left; font-size: 13pt;\">Summary</th>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_4_1\">L4.1 An Example: Hubble Constant</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_4_1\">L4.1 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_4_2\">L4.2 Derivation of Linear Regression</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_4_2\">L4.2 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_4_3\">L4.3 Linear Regression: Coding Example</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_4_3\">L4.3 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_4_4\">L4.4 Weighted Linear Regression</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_4_4\">L4.4 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_4_5\">L4.5 Minimization without the Math</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_4_5\">L4.5 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_4_6\">L4.6 Gradient Descent</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_4_6\">L4.6 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_4_7\">L4.7 Fitting the Full Range of Hubble Data</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_4_7\">L4.7 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_4_8\">L4.8 Fitting with lmfit</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_4_8\">L4.8 Exercises</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\">\n", "            <ul>\n", "                <li>text</li>\n", "                <li>text</li>\n", "            </ul>\n", "        </td>\n", "    </tr>\n", "</table>\n", "\n"]}, {"cell_type": "markdown", "id": "ca4caaa6", "metadata": {"tags": ["learner", "catsoop_00", "md"]}, "source": ["<h3>Learning Objectives</h3>\n", "\n", "In this lecture we will explore the following objectives:\n", "\n", "- Linear Regression\n", "- Weighted Linear Regression\u00b6\n", "- Minimizing without all of the math\n", "- Gradient Descent: Actually understanding how we minimize numerically\n", "- Optimized descent: The Newton Step\n", "\n", "In this class, we are going to take apart fit minizers and put them back together. The goal here is to understand, how a computer does a fast minmization. However, to do this, we will first look at how we would do this with a pen and paper, then we will proceed to add more a more numerical techniques, until we can almost fit everything."]}, {"cell_type": "markdown", "id": "d6935d92", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Importing Libraries</h3>\n", "\n", "Before beginning, run the cell below to import the relevant libraries for this notebook. \n", "Optionally, set the plot resolution and default figure size.\n"]}, {"cell_type": "code", "execution_count": null, "id": "a5e98fad", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#>>>RUN\n", "\n", "import numpy as np\n", "\n", "#set plot resolution\n", "#%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure size\n", "#plt.rcParams['figure.figsize'] = (9,6)\n"]}, {"cell_type": "markdown", "id": "26261305", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_4_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.1 An Example: Hubble Constant</h2>  \n", "\n", "| [Top](#section_4_0) | [Previous Section](#section_4_0) | [Exercises](#exercises_4_1) | [Next Section](#section_4_2) |\n"]}, {"cell_type": "markdown", "id": "da7d3da7", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section."]}, {"cell_type": "code", "execution_count": null, "id": "14a84965", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#>>>RUN\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L04/slides_L04_01.html', width=975, height=550)"]}, {"cell_type": "markdown", "id": "65f1b041", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_4_1'></a>     \n", "\n", "| [Top](#section_4_0) | [Restart Section](#section_4_1) | [Next Section](#section_4_2) |\n"]}, {"cell_type": "markdown", "id": "0c639f1c", "metadata": {"tags": ["learner", "md", "catsoop_01"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 4.1.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": null, "id": "08e20936", "metadata": {"tags": ["draft", "py"]}, "outputs": [], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "02093348", "metadata": {"tags": ["learner", "catsoop_01", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "88dfe05d", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_4_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.2 Derivation of Linear Regression</h2>  \n", "\n", "| [Top](#section_4_0) | [Previous Section](#section_4_1) | [Exercises](#exercises_4_2) | [Next Section](#section_4_3) |\n"]}, {"cell_type": "markdown", "id": "eb570008", "metadata": {"tags": ["learner", "catsoop_00", "md"]}, "source": ["<h3>Supernova Data</h3>\n", "\n", "For this class, we will use public data provided by the supernova galactic survey. This gives a list of supernova along with the distances, they were observed, their uncertainties, and their redshift. The way this is obtained is through galactic observations of supernovae, with the line shifts indicating their respective redshift.\n", "\n", "To load the data, we need to know that the data is stored in terms of the distance modulus, $\\mu$ defined [here](https://en.wikipedia.org/wiki/Distance_modulus). We can write this as \n", "\\begin{equation}\n", "d=10^{\\frac{\\mu}{5} + 1}\n", "\\end{equation}\n", "\n", "Furthermore, the uncertainty $\\sigma_{\\mu}$ is given in terms of the distance modulus. We can write this as\n", "\\begin{eqnarray}\n", "\\sigma_{d} & = &  \\frac{d d(\\mu)}{d\\mu} \\sigma_{\\mu} \\\\\n", "           & = &  \\frac{\\log(10)}{5} 10^{\\frac{\\mu}{5} + 1}   \\sigma_{\\mu}\n", "\\end{eqnarray}\n", "Lets go ahed and process that data. "]}, {"cell_type": "code", "execution_count": 12, "id": "3c1ce26d", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"data": {"text/plain": ["<ErrorbarContainer object of 3 artists>"]}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}, {"data": {"image/png": "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\n", "text/plain": ["<Figure size 432x288 with 1 Axes>"]}, "metadata": {"needs_background": "light"}, "output_type": "display_data"}], "source": ["#>>>RUN\n", "\n", "import math as math\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "import csv\n", "\n", "#Today we are going to start with astro data from here : \n", "#http://supernova.lbl.gov/Union/\n", "#Lets load the data\n", "label='data/sn_z_mu_dmu_plow_union2.1.txt'\n", "\n", "#Table stores name, redshift, distance modulus, distance modulus error\n", "#Lets convert from distance modulus to distance\n", "#See here https://en.wikipedia.org/wiki/Distance_modulus\n", "def distanceconv(iMu):\n", "    power=iMu/5+1\n", "    return 10**power\n", "\n", "def distanceconverr(iMu,iMuErr):\n", "    power=iMu/5+1\n", "    const=math.log(10)/5.\n", "    return const*(10**power)*iMuErr\n", "\n", "redshift=[]\n", "distance=[]\n", "distance_err=[]\n", "with open(label,'r') as csvfile:\n", "    plots = csv.reader(csvfile, delimiter='\\t')\n", "    for row in plots:\n", "        redshift.append(float(row[1]))\n", "        distance.append(distanceconv(float(row[2])))\n", "        distance_err.append(distanceconverr(float(row[2]),float(row[3])))\n", "        \n", "plt.xlabel('redshift(z)', fontsize=15) #Label x\n", "plt.ylabel('distances(parsec)', fontsize=15)#Label y\n", "plt.errorbar(redshift,distance,yerr=distance_err,marker='.',linestyle = 'None', color = 'black')"]}, {"cell_type": "code", "execution_count": 13, "id": "4f40ad94", "metadata": {"tags": ["learner", "py"]}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["<_csv.reader object at 0x111d23200>\n", "[]\n"]}, {"data": {"text/plain": ["<ErrorbarContainer object of 3 artists>"]}, "execution_count": 13, "metadata": {}, "output_type": "execute_result"}, {"data": {"image/png": "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\n", "text/plain": ["<Figure size 432x288 with 1 Axes>"]}, "metadata": {"needs_background": "light"}, "output_type": "display_data"}], "source": ["#>>>RUN\n", "\n", "import math as math\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "import csv\n", "import urllib.request\n", "\n", "#Today we are going to start with astro data from here : \n", "#http://supernova.lbl.gov/Union/\n", "#Lets load the data\n", "label='https://raw.githubusercontent.com/mitx-8s50/data/main/L04/sn_z_mu_dmu_plow_union2.1.txt'\n", "\n", "#Table stores name, redshift, distance modulus, distance modulus error\n", "#Lets convert from distance modulus to distance\n", "#See here https://en.wikipedia.org/wiki/Distance_modulus\n", "def distanceconv(iMu):\n", "    power=iMu/5+1\n", "    return 10**power\n", "\n", "def distanceconverr(iMu,iMuErr):\n", "    power=iMu/5+1\n", "    const=math.log(10)/5.\n", "    return const*(10**power)*iMuErr\n", "\n", "redshift=[]\n", "distance=[]\n", "distance_err=[]\n", "with urllib.request.urlopen(label) as csvfile:\n", "    lines = [l.decode('utf-8') for l in csvfile.readlines()]\n", "    plots = csv.reader(csvfile, delimiter='\\t')\n", "    print(plots)\n", "    for row in plots:\n", "        print(row)\n", "        redshift.append(float(row[1]))\n", "        distance.append(distanceconv(float(row[2])))\n", "        distance_err.append(distanceconverr(float(row[2]),float(row[3])))\n", "\n", "        \n", "print(redshift)\n", "        \n", "plt.xlabel('redshift(z)', fontsize=15) #Label x\n", "plt.ylabel('distances(parsec)', fontsize=15)#Label y\n", "plt.errorbar(redshift,distance,yerr=distance_err,marker='.',linestyle = 'None', color = 'black')"]}, {"cell_type": "markdown", "id": "d83aeb0e", "metadata": {"tags": ["learner", "catsoop_00", "md"]}, "source": ["This plot has an interesting shape. Instead of looking at the whole plot, what we can do is zoom in a specific region and look at the performance there. For this case, we will take redshift ($z$) with a value $ < 0.1$"]}, {"cell_type": "code", "execution_count": null, "id": "8801bb4a", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#>>>RUN\n", "\n", "#Now lets zoom in on the small redshift data\n", "def load(iLabel,iZMax):\n", "    redshift=np.array([])\n", "    distance=np.array([])\n", "    distance_err=np.array([])\n", "    with open(label,'r') as csvfile:\n", "        plots = csv.reader(csvfile, delimiter='\\t')\n", "        for row in plots:\n", "            if float(row[1]) > iZMax:\n", "                continue\n", "            redshift = np.append(redshift,float(row[1]))\n", "            distance = np.append(distance,distanceconv(float(row[2])))\n", "            distance_err = np.append(distance_err,distanceconverr(float(row[2]),float(row[3])))\n", "    return redshift,distance,distance_err\n", "\n", "redshift,distance,distance_err = load(label,0.1)\n", "plt.xlabel('redshift(z)', fontsize=15) #Label x\n", "plt.ylabel('distances(parsec)', fontsize=15)#Label y\n", "plt.errorbar(redshift,distance,yerr=distance_err,marker='.',linestyle = 'None', color = 'black')"]}, {"cell_type": "markdown", "id": "03080533", "metadata": {"tags": ["learner", "catsoop_00", "md"]}, "source": ["<h3>Linear Regression</h3>\n", "\n", "The data above has a linear trend. What we want to do in this class is to come up with the line that best fits the data. To understand this, we are going to do a derivation. Just to warn you, this will likely be the most involved derivation in this whole class. \n", "\n", "Since you see a trend in the data, we would like to fit a function to this trend, where we define \n", "\\begin{equation}\n", "y = Ax + b\n", "\\end{equation}\n", "The goal here is to extract $A$ and $b$. \n", "\n", "To extract this trend, we would like to create a best guess that we define as $\\hat{y}$, which is a prediction based on $x$. We define this for our existing data that we would like to fit, defining \n", "\\begin{equation}\n", "\\hat{y}_{i} = Ax_{i} + b\n", "\\end{equation}\n", "where now we vary $A$ and $b$ so as to get a best estimate of $\\hat{y}$. The minimization is then proceeded by minimizing the distance between the true value of $y$ in the data and the predicted value $\\hat{y}$. We define this metric as $Q$, given by\n", "\\begin{eqnarray} \n", "Q & = & \\sum_{i=1}^{N}\\left(y_{i}-\\hat{y}_{i}\\right)^2 \\\\\n", "Q & = & \\sum_{i=1}^{N}\\left(y_{i}-Ax_{i}-b\\right)^2 \\\\\n", "\\end{eqnarray}\n", "Now to define a minimum, we just need to take a derivative with respect to $A$ and to $b$ and set each to zero, and \\begin{eqnarray} \n", "\\frac{dQ}{dA} & = & \\sum_{i=1}^{N} -2 x_{i} \\left(y_{i}-Ax_{i}-b\\right) \\\\\n", "              & = & \\sum_{i=1}^{N} -2 \\left(x_{i} y_{i}-Ax^{2}_{i}-b x_{i}\\right) \\\\\n", "              & = & 0\n", "\\end{eqnarray}          \n", "and for $b$ we have, noting that the average value of $x$ is $\\bar{x}=\\frac{1}{N} \\sum_{i=1}^{N} x_{i}$, gives us\n", "\\begin{eqnarray} \n", "\\frac{dQ}{db} & = & \\sum_{i=1}^{N} -2  \\left(y_{i}-Ax_{i}-b\\right) \\\\\n", "              & = & 2Nb + 2A\\sum_{i=1}^{N}x_{i}-2\\sum_{i=1}^{N}y_{i} \\\\\n", "              & = & 0 \\\\\n", "{\\rm Simplifying} \\\\\n", "           b  & = & \\frac{1}{N} \\sum_{i=1}^{N}y_{i} - \\frac{A}{N} \\sum_{i=1}^{N}x_{i} \\\\\n", "              & = & \\bar{y} - A\\bar{x}      \n", "\\end{eqnarray}          \n", "Now, we can go back and solve for $A$ given the solution for $b$ in terms of $A$. This gives us\n", "\\begin{eqnarray} \n", "\\frac{dQ}{dA} & = & \\sum_{i=1}^{N} -2 \\left(x_{i} y_{i}-Ax^{2}_{i}-\\left(\\bar{y} - A\\bar{x}\\right) x_{i}\\right) \\\\\n", "              & = & \\sum_{i=1}^{N} -2 \\left(x_{i} y_{i}-x_{i}\\bar{y}-Ax^{2}_{i}+ A\\bar{x}x_{i}\\right) \\\\                           & = & -2 \\sum_{i=1}^{N} x_{i}\\left( y_{i}-\\bar{y}\\right)-2A\\sum_{i=1}^{N} x_{i}\\left(x_{i}-\\bar{x}\\right) \\\\\n", "              & = & 0 \\\\\n", "{\\rm Solving} \\\\\n", "A & = & \\frac{\\sum_{i=1}^{N} x_{i}\\left( y_{i}-\\bar{y}\\right)}{\\sum_{i=1}^{N} x_{i}\\left(x_{i}-\\bar{x}\\right)} \\\\\n", "  & = & \\frac{\\sum_{i=1}^{N} x_{i}\\left( y_{i}-\\bar{y}\\right) +\\sum_{i=1}^{N}\\left(\\bar{x}\\bar{y}-y_{i}\\bar{x}\\right)}      \n", "  {\\sum_{i=1}^{N} x_{i}\\left(x_{i}-\\bar{x}\\right) + \\sum_{i=1}^{N}\\left(\\bar{x}^2-x_{i}\\bar{x}\\right)} \\\\\n", "  & = & \\frac{\\sum_{i=1}^{N} \\left( x_{i} y_{i}-x_{i}\\bar{y}+\\bar{x}\\bar{y}-y_{i}\\bar{x}\\right)}      \n", "  {\\sum_{i=1}^{N} \\left(x^2_{i}-x_{i}\\bar{x} + \\bar{x}^2-x_{i}\\bar{x}\\right)} \\\\\n", "  & = & \\frac{\\frac{1}{N}\\sum_{i=1}^{N} \\left(x_{i} - \\bar{x} \\right) \\left(y_{i}-\\bar{y}\\right)}      \n", "             {\\frac{1}{N}\\sum_{i=1}^{N} \\left(x_{i}-\\bar{x}\\right)^2}\\\\\n", "\\end{eqnarray} \n", "Ok that is a lot of math, but in the end we get variable that we can calculate it. Lets code this up! To do this I am going to break the above into a few functions. Let's define the covariance and variance as : \n", "\n", "\\begin{eqnarray} \n", "\\rm{VAR(x)}   & = & \\frac{1}{N}\\sum_{i=1}^{N} \\left(x_{i}-\\bar{x}\\right)^2 \\\\\n", "\\rm{COV(x,y)} & = & \\frac{1}{N}\\sum_{i=1}^{N} \\left(x_{i} - \\bar{x} \\right) \\left(y_{i}-\\bar{y}\\right) \\\\\n", "            A & = & \\frac{\\rm{COV(x,y)}}{\\rm{VAR(x)}}\n", "\\end{eqnarray}\n", "Note that this variance is just the variance we defined on the first lecture, but now in discrete form. Alright, now let's code this guy up!"]}, {"cell_type": "markdown", "id": "2d0be785", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_4_2'></a>     \n", "\n", "| [Top](#section_4_0) | [Restart Section](#section_4_2) | [Next Section](#section_4_3) |\n"]}, {"cell_type": "markdown", "id": "ff7ee31a", "metadata": {"tags": ["learner", "md", "catsoop_02"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 4.2.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": null, "id": "f7865bb3", "metadata": {"tags": ["draft", "py"]}, "outputs": [], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "8c76ed60", "metadata": {"tags": ["learner", "catsoop_02", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "a83fc089", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_4_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.3 Linear Regression: Coding Example</h2>  \n", "\n", "| [Top](#section_4_0) | [Previous Section](#section_4_2) | [Exercises](#exercises_4_3) | [Next Section](#section_4_4) |\n"]}, {"cell_type": "markdown", "id": "fc19df9e", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "TEXT\n"]}, {"cell_type": "code", "execution_count": null, "id": "8aa7abb2", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#>>>RUN\n", "\n", "#Lets run the regression again\n", "def variance(isamples):\n", "    mean=isamples.mean()\n", "    n=len(isamples)\n", "    tot=0\n", "    for pVal in isamples:\n", "        tot+=(pVal-mean)**2\n", "    return tot/n\n", "\n", "def covariance(ixs,iys):\n", "    meanx=ixs.mean()\n", "    meany=iys.mean()\n", "    n=len(ixs)\n", "    tot=0\n", "    for i0 in range(len(ixs)):\n", "        tot+=(ixs[i0]-meanx)*(iys[i0]-meany)\n", "    return tot/n\n", "\n", "def linear(ix,ia,ib):\n", "    return ia*ix+ib\n", "\n", "def regress(redshift,distance):\n", "    #Lets regress\n", "    var=variance(redshift)\n", "    cov=covariance(redshift,distance)\n", "    A=cov/var\n", "    b=distance.mean()-A*redshift.mean()\n", "    #Done!\n", "    return A,b\n", "\n", "def plotAll(redshift,distance,distance_err,A,b):\n", "    #now lets plot it\n", "    xvals = np.linspace(0,0.1,100)\n", "    yvals = []\n", "    for pX in xvals:\n", "        yvals.append(linear(pX,A,b))\n", "\n", "    #Plot the line\n", "    plt.plot(xvals,yvals)\n", "    plt.errorbar(redshift,distance,yerr=distance_err,marker='.',linestyle = 'None', color = 'black')\n", "    plt.xlabel('redshift(z)', fontsize=15) #Label x\n", "    plt.ylabel('distances(parsec)', fontsize=15)#Label y\n", "    plt.show()\n", "    #Print it out\n", "    print(\"Hubbles Constant:\",1e6*3e5/A,\"intercept\",b)#Note 1e6 is from pc to Mpc and 3e5 is c in km/s\n", "\n", "A,b=regress(redshift,distance)\n", "plotAll(redshift,distance,distance_err,A,b)"]}, {"cell_type": "markdown", "id": "85f643aa", "metadata": {"tags": ["learner", "md"]}, "source": ["Above we have for the units of $A$ \n", "\n", "$$A = \\frac{\\rm distance(pc) }{\\rm  Redshift~(z)}$$\n", "\n", "Hubble's constant is defined as\n", "\n", "$$h_{0} = \\frac{\\rm Redshift~(km/s)}{\\rm distance(Mpc)}$$ \n", "\n", "So \n", "\n", "$$h_{0} = \\frac{1}{A}\\frac{\\rm c~(km/s)}{10^{-6}}=\\frac{1}{A}\\frac{\\rm 3\\times10^{5}~(km/s)}{10^{-6}} $$"]}, {"cell_type": "markdown", "id": "331749c3", "metadata": {"tags": ["learner", "md"]}, "source": ["### Challenge Question\n", "\n", "if you know the uncertainty on $\\sigma_{A}$, what is the uncertainty on $h_{0}$?"]}, {"cell_type": "code", "execution_count": null, "id": "0b72f64a", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#Propagation of uncertainty\n", "#f(A)=1/A=> df/dA=1/A^2\n", "#sigma_{f} = df/dA \\sigma_{A} = 1/A^2 \\sigma_{A}"]}, {"cell_type": "markdown", "id": "fabaa6bc", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_4_3'></a>     \n", "\n", "| [Top](#section_4_0) | [Restart Section](#section_4_3) | [Next Section](#section_4_4) |\n"]}, {"cell_type": "markdown", "id": "65fcdf40", "metadata": {"tags": ["learner", "md", "catsoop_03"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 4.3.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": null, "id": "c5ea9e85", "metadata": {"tags": ["draft", "py"]}, "outputs": [], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "4199000c", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "27b5ddd9", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_4_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.4 Weighted Linear Regression</h2>  \n", "\n", "| [Top](#section_4_0) | [Previous Section](#section_4_3) | [Exercises](#exercises_4_4) | [Next Section](#section_4_5) |\n"]}, {"cell_type": "markdown", "id": "862b2121", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "Now for each measurement, we also have an uncertainty. This uncertainty, we can define as $\\sigma_{y}$. How actually the uncertainty is computed depends on the experiment. However, the uncertainty is **supposed** to correspond to the RMS of the measured value, or in other words, we perform the same measurement $N$ times, and we look at tha Variance of the measured value, and the variance over all measurements $y_{i}$ will be $V[y]=\\sigma^2_{y}$.\n", "\n", "Now for every measurement $y_{i}$ we have an uncertainty associated with it $\\sigma_{y_{i}}$. We can thus define a metric to mimize that accounts for the fact that some measurements have large and small uncertainties. \n", "\n", "\\begin{eqnarray} \n", "Q & = & \\sum_{i=1}^{N}\\frac{\\left(y_{i}-\\hat{y}_{i}\\right)^2}{\\sigma^{2}_{y_{i}}} \\\\\n", "Q & = & \\sum_{i=1}^{N}\\frac{\\left(y_{i}-Ax_{i}-b\\right)^2}{\\sigma^{2}_{y_{i}}} \\\\\n", "\\end{eqnarray}\n", "\n", "From this setup, we can then follow the same procedure as before and minimize with respect to $A$ and $b$.  Yielding the equations below. \n", "\n", "\\begin{eqnarray} \n", "\\frac{dQ}{dA} & = & \\sum_{i=1}^{N} -2 \\frac{x_{i}}{\\sigma_{i}^2} \\left(y_{i}-Ax_{i}-b\\right) \\\\\n", "              & = & \\sum_{i=1}^{N} -2 \\frac{1}{\\sigma_{i}^2} \\left(x_{i} y_{i}-Ax^{2}_{i}-b x_{i}\\right) \\\\\n", "              & = & 0\n", "\\end{eqnarray}          \n", "\n", "and \n", "\\begin{eqnarray} \n", "\\frac{dQ}{db} & = & \\sum_{i=1}^{N} -2  \\frac{1}{\\sigma_{i}^2} \\left(y_{i}-Ax_{i}-b\\right) \\\\\n", "              & = & 2Nb\\sum_{i=1}^{N}\\frac{1}{\\sigma_{i}^2} + 2A\\sum_{i=1}^{N} \\frac{x_{i}}{\\sigma_{i}^2} -2\\sum_{i=1}^{N}\\frac{y_{i}}{\\sigma_{i}^2} \\\\\n", "              & = & 0 \\\\\n", "\\end{eqnarray}          \n", "\n", "From this, we can draw an analogy to the previous derivation, and write the answer given by reparametrizing in terms of the weighted mean defined as \n", "\\begin{eqnarray} \n", " \\bar{y}_{w} & = & \\frac{\\sum_{i=1}^{N} \\frac{y_{i}}{\\sigma_{i}^2} }{\\sum_{i=1}^{N} \\frac{1}{\\sigma_{i}^2} }\n", "\\end{eqnarray}    \n", ", which also applies to $\\bar{x}_{w}$. Given this we have the values for $A$ and $b$ with weights are. \n", "\\begin{eqnarray} \n", "A  & = & \\frac{\\frac{1}{N}\\sum_{i=1}^{N} \\frac{1}{\\sigma_{i}^2}\\left(x_{i} - \\bar{x}_{w} \\right) \\left(y_{i}-\\bar{y}_{w}\\right)}      \n", "              {\\frac{1}{N}\\sum_{i=1}^{N} \\frac{1}{\\sigma_{i}^2}\\left(x_{i}-\\bar{x}_{w}\\right)^2}\\\\\n", "b  & = & \\bar{y}_{w} - A\\bar{x}_{w}      \n", "\\end{eqnarray}   \n", "\n", "Let's again update our plot with this result.  "]}, {"cell_type": "code", "execution_count": null, "id": "529ffc1a", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#>>>RUN\n", "\n", "weights=np.array([])\n", "for pVal in distance_err:\n", "    weights = np.append(weights,1./pVal**2)\n", "\n", "#Now lets do it with weights\n", "def variance_w(isamples,iweights):\n", "    mean=np.average(isamples,weights=iweights)\n", "    sumw=np.sum(iweights)\n", "    tot=0\n", "    for i0 in range(len(isamples)):\n", "        tot+=iweights[i0]*(isamples[i0]-mean)**2\n", "    return tot/sumw\n", "\n", "def covariance_w(ixs,iys,iweights):\n", "    meanx=np.average(ixs,weights=iweights)\n", "    meany=np.average(iys,weights=iweights)\n", "    sumw=np.sum(iweights)\n", "    tot=0\n", "    for i0 in range(len(ixs)):\n", "        tot+=iweights[i0]*(ixs[i0]-meanx)*(iys[i0]-meany)\n", "    return tot/sumw\n", "\n", "def regress_w(redshift,weights,distance):\n", "    varw=variance_w(redshift,weights)\n", "    covw=covariance_w(redshift,distance,weights)\n", "    Aw=covw/varw\n", "    bw=np.average(distance,weights=weights)-Aw*np.average(redshift,weights=weights)\n", "    return Aw,bw\n", "\n", "Aw,bw=regress_w(redshift,weights,distance)\n", "plotAll(redshift,distance,distance_err,Aw,bw)"]}, {"cell_type": "markdown", "id": "3f0182c8", "metadata": {"tags": ["learner", "md"]}, "source": ["*Brief aside on general form*\n", "One last quick technical point is that people often write the linear regression in terms of a matrix over a vector $\\vec{x}$ where each element $x_{i}$ in the vector is one of our measurements. The linear equation is then written as \n", "\\begin{eqnarray} \n", "  \\vec{\\hat{y}} & = & A\\vec{x}+b \\\\\n", "  \\vec{\\hat{y}} & = & \\beta\\vec{x^{\\prime}}\\\\\n", "\\end{eqnarray}\n", "where $\\vec{x^\\prime} = (\\vec{x},1)$ The resulting solution for best fit is written as\n", "\\begin{eqnarray} \n", "  \\hat{\\beta} & = & (\\vec{x}^{T}\\vec{x})^{-1}\\vec{x}^{T}\\vec{y}\n", "\\end{eqnarray} \n", "This is often how it is written in more advanced classes. In reality it is no different than what we showed above. We haven't included weights here, but they can be added. "]}, {"cell_type": "markdown", "id": "29e4bcf6", "metadata": {"tags": ["learner", "md"]}, "source": ["### Challenge Question\n", "\n", "Let's imagine that our uncertainty is larger by a factor of its redshift value $\\sigma_{i}^{new} = z \\sigma_{i}^{old}$. How does the hubble constant change? Why this direction?"]}, {"cell_type": "code", "execution_count": null, "id": "245ffd9a", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#Answer\n", "weights=np.array([])\n", "#pC = 0\n", "#for pVal in distance_err:\n", "#    weights = np.append(weights,1./pVal/pVal/redshift[pC]/redshift[pC])\n", "#    pC = pC + 1\n", "weights=1/((redshift*distance_err)**2)\n", "\n", "Aw,bw=regress_w(redshift,weights,distance)\n", "plotAll(redshift,distance,distance_err,Aw,bw)"]}, {"cell_type": "markdown", "id": "109a9654", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_4_4'></a>     \n", "\n", "| [Top](#section_4_0) | [Restart Section](#section_4_4) | [Next Section](#section_4_5) |\n"]}, {"cell_type": "markdown", "id": "2d741bd5", "metadata": {"tags": ["learner", "md", "catsoop_04"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 4.4.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": null, "id": "d2d7da5f", "metadata": {"tags": ["draft", "py"]}, "outputs": [], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "dcef3459", "metadata": {"tags": ["learner", "catsoop_04", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "8b2e662e", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_4_5'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.5 Minimization without the Math</h2>  \n", "\n", "| [Top](#section_4_0) | [Previous Section](#section_4_4) | [Exercises](#exercises_4_5) | [Next Section](#section_4_6) |\n"]}, {"cell_type": "markdown", "id": "193cf4bd", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "Now what if we just want to minimize without all of the math. Well first, lets just generalize the form of what we would like to minimize. We can write the form of this minimization in terms of an arbitrary function $f(x|\\theta_{i})$ where $\\theta_{i}$ denotes any parameter that we would want to minimize. For the linear regression $\\vec{\\theta}=(A,b)$. The parameter we would want to minimize is\n", "\\begin{equation}\n", "Q=\\sum_{i=1}^N \\left(y-f(x\\mid\\theta_j)\\right)^2,\n", "\\end{equation}\n", "which in it's most general form minimization gives us\n", "\\begin{equation}\n", " \\frac{\\partial Q}{\\partial\\theta_{i}} = \\frac{\\partial}{\\partial \\theta_{i}}\\sum_{i=1}^{N}\\left(y_{i}-f(x|\\theta_{i}\\right)^2=\\sum_{i=1}^{N} 2 \\left(y_{i}-f(x|\\theta_{i}\\right)\\frac{\\partial f(x|\\theta_{i})}{\\partial \\theta_{i}}\n", "\\end{equation}\n", "\n", "Which is effectively a tool kit that we can compute numrically. Let's start by using the scipy stats toolkit to perform the linear regression. Note, this code is done just by going to the scipy stats page and following their examples."]}, {"cell_type": "code", "execution_count": null, "id": "7b9ccffb", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#>>>RUN\n", "\n", "from scipy import stats\n", "\n", "#Now lets do the same thing with standard python tools\n", "slope, intercept, r_value, p_value, std_err = stats.linregress(redshift,distance)\n", "print(\"UnWeighted Fit:\",\"Hubbles Constant:\",1e6*3e5/slope,\"intercept\",intercept)\n", "\n", "#now with weights\n", "from sklearn.linear_model import LinearRegression\n", "model = LinearRegression()\n", "redshifthack=np.reshape(redshift,(len(redshift),1))#line to get the fit code to work\n", "model.fit(redshifthack,distance,weights)\n", "slope=model.coef_\n", "const=model.intercept_\n", "print(\"Weighted Fit:\",\"Hubbles Constant:\",1e6*3e5/slope,\"intercept\",const)"]}, {"cell_type": "markdown", "id": "c45fbde8", "metadata": {"tags": ["learner", "md"]}, "source": ["What exactly is this doing? Its finding a mimum to get the slope. Lets understand the toolkit to find a minimum. "]}, {"cell_type": "code", "execution_count": null, "id": "699ae0b5", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#>>>RUN\n", "\n", "#Now lets do this with a general tool that optimizes\n", "from scipy import optimize as opt\n", "#First lets see what it does\n", "def f(x):\n", "    return x**4 + 3*(x-2)**3 - 15*(x)**2 + 1\n", "sol=opt.minimize_scalar(f, method='Brent')\n", "x = np.linspace(-8, 5, 100)\n", "\n", "plt.xlabel('x', fontsize=15) #Label x\n", "plt.ylabel('f(x)', fontsize=15)#Label y\n", "plt.plot(x, f(x));\n", "plt.axvline(sol.x, c='red')"]}, {"cell_type": "markdown", "id": "3dd940e2", "metadata": {"tags": ["learner", "md"]}, "source": ["This algorithm steps along the function and finds a minimum. The way it does this is through some minimum finding algorithm. Its not essential, just yet, how it finds the minimum, but suffice it to say that its basically just evaluating the function many times and finding a minimum.  Before we discuss how the minimizer works, lets go ahead and see how we can use this same minimizer to perform a fit.  **Note the uncertainties will be discussed later**"]}, {"cell_type": "code", "execution_count": null, "id": "2bdce3a0", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#>>>RUN\n", "\n", "#great it finds the minimum of a function lets take a fancier tool based on this\n", "from scipy.optimize import curve_fit\n", "\n", "def f(x,a,b):\n", "    return a*x+b\n", "\n", "popt, pcov = curve_fit(f, redshift,distance)\n", "perr = np.sqrt(np.diag(pcov))\n", "print(\"Unweighted Hubbles Constant:\",1e6*3e5/popt[0],\"+/-\",(1e6*3e5/popt[0]/popt[0])*perr[0],\"intercept\",popt[1],\"+/-\",perr[1])\n", "\n", "#Now lets do it with weights\n", "popt, pcov = curve_fit(f, redshift,distance,sigma=distance_err)\n", "perr = np.sqrt(np.diag(pcov))\n", "print(\"Weighted Hubbles Constant:\",1e6*3e5/popt[0],\"+/-\",(1e6*3e5/popt[0]/popt[0])*perr[0],\"intercept\",popt[1],\"+/-\",perr[1])"]}, {"cell_type": "markdown", "id": "0bf62d26", "metadata": {"tags": ["learner", "md"]}, "source": ["### Challenge Question\n", "\n", "Compute the minimum of a 2D mexican hat potential function given by \n", "\\begin{equation}\n", "f(x) = (x-1)^{2}(x+1)^2-50x^2\n", "\\end{equation}\n", "What is wrong with this minimizer? "]}, {"cell_type": "code", "execution_count": null, "id": "21cf4fa6", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#answer\n", "def f(x):\n", "    return ((x-1)**2)*((x+1)**2)-50*x**2\n", "sol=opt.minimize_scalar(f, method='Brent')\n", "x = np.linspace(-8, 8, 100)\n", "\n", "plt.xlabel('x', fontsize=15) #Label x\n", "plt.ylabel('f(x)', fontsize=15)#Label y\n", "plt.plot(x, f(x));\n", "plt.axvline(sol.x, c='red')\n", "hide_toggle()"]}, {"cell_type": "markdown", "id": "5d1a395e", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_4_5'></a>     \n", "\n", "| [Top](#section_4_0) | [Restart Section](#section_4_5) | [Next Section](#section_4_6) |\n"]}, {"cell_type": "markdown", "id": "7ef51d36", "metadata": {"tags": ["learner", "md", "catsoop_05"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 4.5.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": null, "id": "7e0ff971", "metadata": {"tags": ["draft", "py"]}, "outputs": [], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "dfb1fe39", "metadata": {"tags": ["learner", "catsoop_05", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "82adc12d", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_4_6'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.6 Gradient Descent</h2>  \n", "\n", "| [Top](#section_4_0) | [Previous Section](#section_4_5) | [Exercises](#exercises_4_6) | [Next Section](#section_4_7) |\n"]}, {"cell_type": "markdown", "id": "306d239b", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section."]}, {"cell_type": "code", "execution_count": null, "id": "de48c1b4", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#>>>RUN\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L04/slides_L04_06.html', width=975, height=550)"]}, {"cell_type": "markdown", "id": "90ef93aa", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "![Screen%20Shot%202021-12-02%20at%2011.31.53%20AM.png](attachment:Screen%20Shot%202021-12-02%20at%2011.31.53%20AM.png)\n", "\n", "To minimize something, what we need to is follow along a function until we reach the minimium. Another way to think about this is if we have a potential function $U(x)$, how do we get an object to reach the bottom of the potential? \n", "\n", "Well, in this case, all we need to do is rely on our understanding of physics and motion in a potential.  To understand gradient descent, we need to use some physics. Recall that for a potential given by $U$ the force on that potential is \n", "\n", "$F=-\\nabla U$ \n", "\n", "and this force will give us some momentum as if we are moving down a hill. Likewise from this we can write the acceleration over time is given by\n", "\n", "$\\vec{a}=-\\frac{1}{m}\\nabla U$\n", "\n", "and further we can write, integrating,\n", "\n", "$\\vec{v}=\\int \\vec{a} dt =-\\int \\frac{1}{m}\\nabla U dt$,\n", "\n", "and for position we can write for displacement \n", "\n", "$\\vec{s}=\\int \\vec{v} dt =\\int \\int \\vec{a} dt dt =-\\int \\int \\frac{1}{m}\\nabla U dt dt$\n", "\n", "\n", "Now to find a minimum what we can do is envision a ball rolling down our function towards the minimum. We can compute its motion and acceleration through the usual kinematics equation. We can do this with a stepwise iterator\n", "\\begin{eqnarray}\n", "\\vec{v} = \\vec{v_0} - \\delta t \\nabla f \\\\\n", "\\vec{x} = \\vec{x_0} + \\delta t \\vec{v} - (\\delta t)^2\\nabla f \n", "\\end{eqnarray}\n", "Let's code this up by hand first to understand gradient descent, keep in mind that this is not a physics problem, it's just a function in the end. \n"]}, {"cell_type": "code", "execution_count": null, "id": "36df49c8", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#>>>RUN\n", "\n", "#Lets find the minimum of x^2\n", "def f(x):\n", "    return x**2\n", "def grad(x):\n", "    return 2*x\n", "\n", "<h3>Simple gradient descent</h3>\n", "#now we are going to move from x in timestesps alpha\n", "def gd(x, grad, alpha, max_iter=10):\n", "    xs = np.zeros(1 + max_iter)\n", "    xs[0] = x #start at x\n", "    for i in range(max_iter):\n", "        x = x - alpha * grad(x) #alpha is going to be our timestep\n", "        xs[i+1] = x\n", "    return xs\n", "\n", "<h3>Physics based gradient descent</h3>\n", "#Now will update both momentum and velocity in timesteps alpha but with a dilution factor of beta\n", "def gd_momentum_true(x, grad, alpha,max_iter=10):\n", "    xs = np.zeros(1 + max_iter)\n", "    xs[0] = x\n", "    v = 0\n", "    for i in range(max_iter):\n", "        v = v - alpha*grad(x)\n", "        x = x + alpha * v  - 0.5 * alpha * alpha*grad(x)\n", "        xs[i+1] = x\n", "    return xs\n", "\n", "def plotGDAlgo(iGDAlgo,alpha=0.1,beta=-1,x0=1):\n", "    if beta != -1:\n", "        xs = iGDAlgo(x0, grad, alpha,beta)\n", "    else:\n", "        xs = iGDAlgo(x0, grad, alpha)\n", "    xp = np.linspace(-1.2, 1.2, 100)\n", "\n", "    #Now just plotting code\n", "    plt.xlabel('x', fontsize=15) #Label x\n", "    plt.ylabel('f(x)', fontsize=15)#Label y\n", "    plt.plot(xp, f(xp)) #function\n", "    plt.plot(xs, f(xs), 'o-', c='red') #varied function\n", "    for i, (x, y) in enumerate(zip(xs, f(xs)), 1):\n", "        plt.text(x, y+0.2, i,bbox=dict(facecolor='yellow', alpha=0.5), fontsize=14)\n", "    plt.show()    \n", "\n", "plotGDAlgo(gd)\n", "plotGDAlgo(gd,0.95) #large timestep alpha\n", "plotGDAlgo(gd_momentum_true,0.95) #Large timestep"]}, {"cell_type": "markdown", "id": "e6972a71", "metadata": {"tags": ["learner", "md"]}, "source": ["So why isn't it finding the minimium? \n", "Energy is conserved! We need to add a friction term, which we do by defining a variable we call damp. \n", "\n", "We can define the damp variable as some number damp$<1$. In this case, we will define it as \n", "\\begin{equation}\n", " \\rm{damp} = \\frac{1}{1+\\beta^{\\rm{iter}+1}}\n", "\\end{equation}\n", "for $\\beta$ some parameter. Damp is, thus, a value less than 1 that rapidly damps down towards motion at the minimum. Its effectively a friction term. Lets add it. "]}, {"cell_type": "code", "execution_count": null, "id": "5100997b", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#>>>RUN\n", "\n", "#now we are going to move from x in timestesps alpha\n", "def gd_momentum_true(x, grad, alpha,beta=1.0, max_iter=10):\n", "    xs = np.zeros(1 + max_iter)\n", "    xs[0] = x\n", "    v = 0\n", "    for i in range(max_iter):\n", "        v = v - alpha*grad(x)\n", "        damp = 1/(1+beta**(i+1)) #now damped\n", "        x = x + damp*(alpha * v  - 0.5 * alpha * alpha*grad(x))\n", "        xs[i+1] = x\n", "    return xs\n", "\n", "#Now will update both momentum and velocity in timesteps alpha but with a dilution factor of beta\n", "def gd_momentum(x, grad, alpha, beta=0.9, max_iter=10):\n", "    xs = np.zeros(1 + max_iter)\n", "    xs[0] = x\n", "    v = 0\n", "    for i in range(max_iter):\n", "        v = beta*v + (1-beta)*grad(x)#beta is the samping term here\n", "        vc = v/(1+beta**(i+1))#and damp\n", "        x = x - alpha * vc\n", "        xs[i+1] = x\n", "    return xs\n", "\n", "plotGDAlgo(gd)\n", "plotGDAlgo(gd,0.95) #large timestep alpha\n", "plotGDAlgo(gd_momentum_true,0.95) #Large timestep\n", "plotGDAlgo(gd_momentum,0.95) #Large timestep\n", "plotGDAlgo(gd_momentum,0.95,beta=0.7) #Large timestep\n", "plotGDAlgo(gd_momentum,0.95,beta=0.9) #Large timestep"]}, {"cell_type": "markdown", "id": "a4dbd365", "metadata": {"tags": ["learner", "md"]}, "source": ["So all of these guys approach the minimum and you can be quite flexible. We can in fact use any minimization function we want in `scipy.optimize`. It will do the iterative stepping for us. Let's write our own minimizer and throw it in. We can do it for our previous quartic equation. "]}, {"cell_type": "code", "execution_count": null, "id": "4a46a96f", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#Now lets do this with a general tool that optimizes\n", "import scipy.linalg as la\n", "def f(x):\n", "    return x**4 + 3*(x-2)**3 - 15*(x)**2 + 1\n", "\n", "def fprime(x):\n", "    return 4*x**3 + 9*(x-2)**2 - 30*(x)\n", "\n", "#Where is our minimizer\n", "def custmin(fun, x0, args=(), maxfev=None, alpha=0.0002,\n", "        maxiter=100000, tol=1e-10, callback=None, **options):\n", "    \"\"\"Implements simple gradient descent for the function above.\"\"\"\n", "    bestx = x0\n", "    bestf = fun(x0)\n", "    funcalls = 1\n", "    niter = 0\n", "    improved = True\n", "    stop = False\n", "\n", "    while improved and not stop and niter < maxiter:\n", "        niter += 1\n", "        # the next 2 lines are gradient descent\n", "        step = alpha * fprime(bestx)\n", "        bestx = bestx - step\n", "        \n", "        bestf = fun(bestx)\n", "        funcalls += 1\n", "\n", "        if la.norm(step) < tol:\n", "            improved = False\n", "        if callback is not None:\n", "            callback(bestx)\n", "        if maxfev is not None and funcalls >= maxfev:\n", "            stop = True\n", "            break\n", "\n", "    return opt.OptimizeResult(fun=bestf, x=bestx, nit=niter,nfev=funcalls, success=(niter > 1))\n", "\n", "def reporter(p):\n", "    \"\"\"Reporter function to capture intermediate states of optimization.\"\"\"\n", "    global ps\n", "    ps.append(p)\n", "\n", "x0=-7\n", "x0 = np.array([x0])\n", "ps = [x0]\n", "sol=opt.minimize(f, x0, method=custmin, callback=reporter)\n", "x = np.linspace(-8, 5, 100)\n", "plt.xlabel('x', fontsize=15) #Label x\n", "plt.ylabel('f(x)', fontsize=15)#Label y\n", "plt.plot(x, f(x));\n", "plt.axvline(sol.x, c='red')\n", "plt.show()\n", "\n", "#Now lets trick it\n", "x0=5\n", "x0 = np.array([x0])\n", "ps = [x0]\n", "sol=opt.minimize(f, x0, method=custmin, callback=reporter)\n", "x = np.linspace(-8, 5, 100)\n", "plt.xlabel('x', fontsize=15) #Label x\n", "plt.ylabel('f(x)', fontsize=15)#Label y\n", "plt.plot(x, f(x));\n", "plt.axvline(sol.x, c='red')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "2871b414", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_4_6'></a>     \n", "\n", "| [Top](#section_4_0) | [Restart Section](#section_4_6) | [Next Section](#section_4_7) |\n"]}, {"cell_type": "markdown", "id": "e6b052d3", "metadata": {"tags": ["learner", "md", "catsoop_06"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 4.6.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": null, "id": "2f7d4cc8", "metadata": {"tags": ["draft", "py"]}, "outputs": [], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "e654e17a", "metadata": {"tags": ["learner", "catsoop_06", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "4b310c0b", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_4_7'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.7 Fitting the Full Range of Hubble Data</h2>  \n", "\n", "| [Top](#section_4_0) | [Previous Section](#section_4_6) | [Exercises](#exercises_4_7) | [Next Section](#section_4_8) |\n"]}, {"cell_type": "markdown", "id": "659cb0fb", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "Now, what if we want to minimize a function a fit function in two dimensions.  To do that, we are going to go back to our supernova data, and fit the line with a modified slope expansion given by:\n", "\n", "$$ f(x) = \\frac{x}{h_{0}}\\left(1+\\frac{1-q}{2}x\\right) $$\n", "\n", "Additionally, we have removed $b$, since this form is a closer approximation of the true form of the expansion of the universe, and in that instance $f(0)=0$\n", "\n", "Recall that we needed to minimize: \n", "\n", "\\begin{equation}\n", " \\frac{\\partial Q}{\\partial\\theta_{i}} = \\frac{\\partial}{\\partial \\theta_{i}}\\sum_{i=1}^{N}\\left(y_{i}-f(x|\\theta_{i}\\right)^2=\\sum_{i=1}^{N} 2 \\left(y_{i}-f(x|\\theta_{i}\\right)\\frac{\\partial f(x|\\theta_{i})}{\\partial \\theta_{i}}\n", "\\end{equation}\n", "\n", ", which means that we need compute $\\frac{\\partial f}{\\partial h_{0}}$ and $\\frac{\\partial f}{\\partial q}$. We do it belowin `fprime` and output a vector with two outputs for $h_{0}$ and $q$ respectively. \n"]}, {"cell_type": "code", "execution_count": null, "id": "48535426", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#Ok lets do a custom minimization of our fit function with gradient descent\n", "#Note that since we are fitting two parameters we need to do this in 2D\n", "redshift,distance,distance_err = load(label,10)\n", "weights=np.array([])\n", "for pVal in distance_err:\n", "    weights = np.append(weights,1./pVal/pVal)\n", "\n", "def f(x,h0,q):\n", "    val=x*(1e6*3e5/h0)*(1 + ((1-q)*0.5)*x)\n", "    return val\n", "\n", "def fprime(x,h0,q):\n", "    der=np.zeros(2)\n", "    der[0]=-1*x*(1e6*3e5/h0/h0)*(1 + ((1-q)*0.5)*x)\n", "    der[1]=x*(1e6*3e5/h0)*(-0.5*x)\n", "    return der\n", "\n", "def algof(inputs):\n", "    d=0\n", "    for i0 in range(len(redshift)):\n", "        yhat=f(redshift[i0],inputs[0],inputs[1])\n", "        pD=(distance[i0]-yhat)**2\n", "        #pD=pD/dmean\n", "        d+=pD*weights[i0]\n", "    return d\n", "\n", "def algofprime(inputs):\n", "    d=np.zeros(2)\n", "    for i0 in range(len(redshift)):\n", "        yhat=f(redshift[i0],inputs[0],inputs[1])\n", "        pD=2*(distance[i0]-yhat)\n", "        yhatprime=fprime(redshift[i0],inputs[0],inputs[1])\n", "        #print(yhatprime,pD,d)\n", "        #pD=pD/dmean\n", "        d=d+(yhatprime*pD)*weights[i0]\n", "    return d\n", "\n", "def custmin(fun, x0, args=(), maxfev=None, alpha=0.0001,maxiter=10000, tol=1e-10, callback=None, **options):\n", "    \"\"\"Implements simple gradient descent for the function above.\"\"\"\n", "    bestx = x0\n", "    bestf = fun(x0)\n", "    funcalls = 1\n", "    niter = 0\n", "    improved = True\n", "    stop = False\n", "\n", "    while improved and not stop and niter < maxiter:\n", "        niter += 1\n", "        if niter % 1000 == 0: \n", "            print(niter,bestx)\n", "        # the next 2 lines are gradient descent\n", "        step = alpha * algofprime(bestx)\n", "        #print(bestx,step)\n", "        bestx = bestx + step\n", "\n", "        bestf = fun(bestx)\n", "        funcalls += 1\n", "\n", "        if la.norm(step) < tol:\n", "            improved = False\n", "        if callback is not None:\n", "            callback(bestx)\n", "        if maxfev is not None and funcalls >= maxfev:\n", "            stop = True\n", "            break\n", "\n", "    return opt.OptimizeResult(fun=bestf, x=bestx, nit=niter,nfev=funcalls, success=(niter > 1))\n", "\n", "def reporter(p):\n", "    \"\"\"Reporter function to capture intermediate states of optimization.\"\"\"\n", "    global ps\n", "    ps.append(p)\n", "\n", "#tmp=algofprime([70,0.03])\n", "x0 = np.array([40,0])\n", "ps = [x0]\n", "sol0=opt.minimize(algof, x0, method=custmin, callback=reporter)\n", "print(sol0)\n", "\n", "sol1=opt.minimize(algof, x0)#, method=custmin, callback=reporter)\n", "print(sol1)\n", "    "]}, {"cell_type": "markdown", "id": "85dd7b10", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section."]}, {"cell_type": "code", "execution_count": null, "id": "1e53a6c3", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#>>>RUN\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L04/slides_L04_07.html', width=975, height=550)"]}, {"cell_type": "markdown", "id": "c183ba97", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Optimized minimizers Newton Step</h3>\n", "\n", "Now typically more advanced optimization methods are used beyond gradient descent. Hence the fact that if we don't use our custom minimizer we get the same answer but in a fraction of time. These minimizers are based on something called the Newton step, which you can think of as a dance move that Newton perfomed. It follows like this. Recall from a Taylor expansion we have\n", "\\begin{equation}\n", "f(x+h)=f(x)+h\\frac{df}{dx}(x)+\\frac{h^2}{2}\\frac{d^2f}{dx^2} \\\\\n", "\\end{equation}\n", "Now we know that at the function's minimum the derivative is zero and so we have\n", "\\begin{equation}\n", "\\frac{f(x+h)-f(x)}{h}=0=\\frac{df}{dx}(x)+\\frac{h}{2}\\frac{d^2f}{dx^2} \\\\\n", "\\end{equation}\n", "and so for $\\frac{h}{2}=\\Delta x$ we get the Newton step: \n", "\\begin{equation}\n", "0=\\frac{df}{dx}(x)+\\Delta x \\frac{d^2f}{dx^2} \\\\\n", "\\Delta x=-\\frac{\\frac{df}{dx}}{\\frac{d^2f}{dx^2}}(x)\\\\\n", "\\Delta x=-\\frac{f^{\\prime}(x)}{f^{\\prime\\prime}(x)}\\\\\n", "\\end{equation}\n", "This defines a stepwise iteration that we can do step by step. Namely, we have $x\\rightarrow x+\\Delta x$. You might recall this same procedure from Newtons' method of finding roots $x_{k+1}=x_{k}-\\frac{f(x)}{f^{\\prime}(x)}$.  We can generalize this into N dimensions. This gives us: \n", "\\begin{equation}\n", "f(\\vec{x}+\\vec{h})=f(\\vec{x})+\\vec{h}^{T}\\nabla f(x)+ \\frac{1}{2}\\vec{h}^{T}\\frac{\\partial^2 f}{dx_{i}dx_{j}}\\vec{h} \\\\\n", "\\vec{h}=-\\left(\\frac{\\partial^2 f}{dx_{i}dx_{j}} \\right)^{-1}\\nabla f(\\vec{x})\n", "\\end{equation}\n", "where we have the Hessian given by $\\frac{\\partial^2 f}{dx_{i}dx_{j}}$. This gives us a way to step through the optimization very efficiently. There are many variations on a theme, Adam, RMSProp, ... All of these are minimization algorithms that get us along the same way. \n", "\n", "Finally, we should note that above, we analytically computed the derivative. However, its often the case the function is not analytic. Then we have to compute this on the fly. Let's take our example above and do a numerical derivative again. \n", "\n", "We can define the numerical derivative as \n", "\\begin{equation}\n", "\\frac{\\partial}{\\partial \\theta_{i}} \\vec{f} = \\frac{f\\left(\\vec{x} + \\Delta \\hat{\\theta}_{i}\\right)-f\\left(\\vec{x} - \\Delta \\hat{\\theta}_{i}\\right)}{2\\Delta\\theta}\n", "\\end{equation}"]}, {"cell_type": "code", "execution_count": null, "id": "68b385a8", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#Ok lets do a custom minimization of our fit function with gradient descent\n", "#Note that since we are fitting two parameters we need to do this in 2D\n", "def f(x,h0,q):\n", "    val=x*(1e6*3e5/h0)*(1 + ((1-q)*0.5)*x)\n", "    return val\n", "\n", "def algof(inputs):\n", "    d=0\n", "    for i0 in range(len(redshift)):\n", "        yhat=f(redshift[i0],inputs[0],inputs[1])\n", "        pD=(distance[i0]-yhat)**2\n", "        #pD=pD/dmean\n", "        d+=pD*weights[i0]\n", "    return d\n", "\n", "def algofprime(inputs):\n", "    delta1=np.array([inputs[0]*0.001,0])\n", "    delta2=np.array([0,inputs[1]*0.001])\n", "    dp11=algof(inputs-delta1)\n", "    dp21=algof(inputs-delta2)\n", "    dp12=algof(inputs+delta1)\n", "    dp22=algof(inputs+delta2)\n", "    deriv=np.array([0,0])\n", "    deriv[0]=(dp11-dp12)/(2*delta1[0])\n", "    deriv[1]=(dp21-dp22)/(2*delta2[1])\n", "    return deriv\n", "\n", "def algofprime2(inputs):\n", "    d=np.zeros(2)\n", "    for i0 in range(len(redshift)):\n", "        yhat=f(redshift[i0],inputs[0],inputs[1])\n", "        pD=2*(distance[i0]-yhat)\n", "        yhatprime=fprime(redshift[i0],inputs[0],inputs[1])\n", "        #print(yhatprime,pD,d)\n", "        #pD=pD/dmean\n", "        d=d+(yhatprime*pD)*weights[i0]\n", "    return d\n", "\n", "def custmin(fun, x0, args=(), maxfev=None, alpha=0.01,maxiter=10000, tol=1e-10, callback=None, **options):\n", "    \"\"\"Implements simple gradient descent for the function above.\"\"\"\n", "    bestx = x0\n", "    bestf = fun(x0)\n", "    funcalls = 1\n", "    niter = 0\n", "    improved = True\n", "    stop = False\n", "\n", "    while improved and not stop and niter < maxiter:\n", "        niter += 1\n", "        if niter % 1000 == 0: \n", "            print(niter,bestx)\n", "        # the next 2 lines are gradient descent\n", "        step = alpha * algofprime(bestx)\n", "        #print(bestx,step)\n", "        bestx = bestx + step\n", "\n", "        bestf = fun(bestx)\n", "        funcalls += 1\n", "\n", "        if la.norm(step) < tol:\n", "            improved = False\n", "        if callback is not None:\n", "            callback(bestx)\n", "        if maxfev is not None and funcalls >= maxfev:\n", "            stop = True\n", "            break\n", "\n", "    return opt.OptimizeResult(fun=bestf, x=bestx, nit=niter,nfev=funcalls, success=(niter > 1))\n", "\n", "def reporter(p):\n", "    \"\"\"Reporter function to capture intermediate states of optimization.\"\"\"\n", "    global ps\n", "    ps.append(p)\n", "\n", "x0 = np.array([40,0.1])\n", "ps = [x0]\n", "sol0=opt.minimize(algof, x0, method=custmin, callback=reporter)\n", "print(sol0)\n", "print(\"\\n\\n\\n\")\n", "sol1=opt.minimize(algof, x0)#, method=custmin, callback=reporter)\n", "print(sol1)"]}, {"cell_type": "markdown", "id": "8bfc48ee", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_4_7'></a>     \n", "\n", "| [Top](#section_4_0) | [Restart Section](#section_4_7) | [Next Section](#section_4_8) |\n"]}, {"cell_type": "markdown", "id": "992c8538", "metadata": {"tags": ["learner", "md", "catsoop_07"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 4.7.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": null, "id": "fe641a84", "metadata": {"tags": ["draft", "py"]}, "outputs": [], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "54c94470", "metadata": {"tags": ["learner", "catsoop_07", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}, {"cell_type": "markdown", "id": "8d3bf599", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='section_4_8'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.8 Fitting with lmfit</h2>     \n", "\n", "| [Top](#section_4_0) | [Previous Section](#section_4_7) | [Exercises](#exercises_4_8) |\n"]}, {"cell_type": "markdown", "id": "533712cb", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "Now finally, let's write this with `lmfit`, which is the way I would actually do the problem. "]}, {"cell_type": "code", "execution_count": null, "id": "9cb0cae0", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#>>>RUN\n", "\n", "import lmfit\n", "\n", "weights=np.array([])\n", "for pVal in distance_err:\n", "    #weighted fits in lmfit require you to ass in 1/sigma (not 1/sigma^2\n", "    weights = np.append(weights,1./pVal)\n", "\n", "#Clearly thats not working so lets use an approximation to this\n", "def f(x,h0,q):\n", "    val=x*(1e6*3e5/h0)*(1 + ((1-q)*0.5)*x)\n", "    return val\n", "\n", "model  = lmfit.Model(f)\n", "p = model.make_params(h0=50,q=0)\n", "result = model.fit(data=distance, params=p, x=redshift, weights=weights)\n", "lmfit.report_fit(result)\n", "plt.figure()\n", "result.plot()"]}, {"cell_type": "markdown", "id": "250e1593", "metadata": {"tags": ["learner", "md"]}, "source": ["### Challenge Question\n", "Add a constant term to the fit, and see if the fitted parameter is consistent with zero. Your fit function, shoul d have the form: \n", "\n", "$$ f(x) = c + \\frac{x}{h_{0}}\\left(1+\\frac{1-q}{2}x\\right) $$\n", "\n", "Run the fit with this modified form. "]}, {"cell_type": "code", "execution_count": null, "id": "bad9cd64", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#Clearly thats not working so lets use an approximation to this\n", "def f(x,h0,q,c):\n", "    val=c+x*(1e6*3e5/h0)*(1 + ((1-q)*0.5)*x)\n", "    return val\n", "\n", "model  = lmfit.Model(f)\n", "p = model.make_params(h0=50,q=0,c=0)\n", "result = model.fit(data=distance, params=p, x=redshift, weights=weights)\n", "lmfit.report_fit(result)\n", "plt.figure()\n", "result.plot()\n", "\n", "hide_toggle()"]}, {"cell_type": "markdown", "id": "50381a37", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_4_8'></a>   \n", "\n", "| [Top](#section_4_0) | [Restart Section](#section_4_8) |\n"]}, {"cell_type": "markdown", "id": "14d9240f", "metadata": {"tags": ["learner", "md", "catsoop_08"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 4.8.1</span>\n", "\n", "text\n"]}, {"cell_type": "code", "execution_count": null, "id": "adfcd4fe", "metadata": {"tags": ["draft", "py"]}, "outputs": [], "source": ["#>>>EXERCISE\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def exp_func(x):\n", "    return 0\n"]}, {"cell_type": "markdown", "id": "7b3ea155", "metadata": {"tags": ["learner", "catsoop_08", "md"]}, "source": ["### <span style=\"color: #90409C;\">*>>> Follow-up (ungraded)*</span>\n", "\n", "    \n", "><span style=\"color: #90409C;\">*TEXT*</span>\n"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}